{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome Percona Monitoring and Management (PMM) is a free, open-source monitoring tool for MySQL, PostgreSQL, MongoDB, and ProxySQL, and the servers they run on. PMM helps you improve the performance of databases, simplify their management, and strengthen their security. This documentation is for the latest release: PMM 2.15.0 With PMM, you can: Visualize a wide range of out-of-the-box system performance metrics Collect and analyze data across complex multi-vendor system topologies Drill-down and discover the cause of inefficiencies, anticipate performance issues, or troubleshoot existing ones Watch for potential security issues and remedy them PMM is efficient, quick to set up and easy to use. It runs in cloud, on-prem, or across hybrid platforms. It is supported by Percona\u2019s legendary expertise in open source databases, and by a vibrant developer and user community . Try the live demo: pmmdemo.percona.com Setting up Quickstart installation PMM Server communicates with clients, receives metrics data and presents it in a web-based user interface. PMM Server can run as: a Docker container , an virtual machine , or as an Amazon AWS EC2 instance . (Learn more about setting up PMM Server .) PMM Client runs on all hosts you want to monitor according to the type of system, be they databases ( MySQL, Percona Server, MariaDB , MongoDB , PostgreSQL , Amazon RDS , Microsoft Azure ) or services ( ProxySQL , Linux , External services , HAProxy ). (Learn more about setting up PMM Client .) How it works PMM is a client/server application built by Percona with our own and third-party open-source tools. (Read more in Architecture .) PMM Server PMM Server is the heart of PMM. It receives data from clients, collates it and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface . This is the home dashboard from pmmdemo : PMM Client PMM Client runs on every database host or node you want to monitor. The client collects server metrics, general system metrics, and query analytics data, and sends it to the server. Percona Enterprise Platform Percona Enterprise Platform (in development) provides value-added services for PMM. Documentation site map","title":"Welcome"},{"location":"index.html#setting-up","text":"Quickstart installation PMM Server communicates with clients, receives metrics data and presents it in a web-based user interface. PMM Server can run as: a Docker container , an virtual machine , or as an Amazon AWS EC2 instance . (Learn more about setting up PMM Server .) PMM Client runs on all hosts you want to monitor according to the type of system, be they databases ( MySQL, Percona Server, MariaDB , MongoDB , PostgreSQL , Amazon RDS , Microsoft Azure ) or services ( ProxySQL , Linux , External services , HAProxy ). (Learn more about setting up PMM Client .)","title":"Setting up"},{"location":"index.html#how-it-works","text":"PMM is a client/server application built by Percona with our own and third-party open-source tools. (Read more in Architecture .) PMM Server PMM Server is the heart of PMM. It receives data from clients, collates it and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface . This is the home dashboard from pmmdemo : PMM Client PMM Client runs on every database host or node you want to monitor. The client collects server metrics, general system metrics, and query analytics data, and sends it to the server. Percona Enterprise Platform Percona Enterprise Platform (in development) provides value-added services for PMM.","title":"How it works"},{"location":"index.html#documentation-site-map","text":"","title":"Documentation site map"},{"location":"faq.html","text":"FAQ How can I contact the developers? How can I contact the technical writers? What are the minimum system requirements for PMM? How can I upgrade from PMM version 1? How to control data retention for PMM? How often are NGINX logs in PMM Server rotated? What privileges are required to monitor a MySQL instance? Can I monitor multiple service instances? Can I rename instances? Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition? How do I troubleshoot communication issues between PMM Client and PMM Server? What resolution is used for metrics? How do I set up Alerting in PMM? How do I use a custom Prometheus configuration file inside PMM Server? How to troubleshoot an Update? What are my login credentials when I try to connect to a Prometheus Exporter? How to provision PMM Server with non-default admin password? How can I contact the developers? The best place to discuss PMM with developers and other community members is the community forum . To report a bug, visit the PMM project in JIRA . How can I contact the technical writers? Open a Jira ticket Open a GitHub issue What are the minimum system requirements for PMM? PMM Server Any system which can run Docker version 1.12.6 or later. It needs roughly 1 GB of storage for each monitored database node with data retention set to one week. Note By default, retention is set to 30 days for Metrics Monitor and for Query Analytics. You can consider disabling table statistics to decrease the VictoriaMetrics database size. You need at least 2 GB for one monitored database node. Note The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. Note Your CPU must support the SSE4.2 instruction set, a requirement of ClickHouse, a third-party column-oriented database used by Query Analytics. If your CPU is lacking this instruction set you won\u2019t be able to use QAN. PMM Client Any modern 64-bit Linux distribution. It is tested on the latest versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.) How can I upgrade from PMM version 1? Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age out during the course of the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Note Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. However, we are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules. How to control data retention for PMM? By default, PMM stores time-series data for 30 days. Depending on your available disk space and requirements, you may need to adjust the data retention time: Go to PMM > PMM Settings > Advanced Settings . Change the data retention value. Click Apply changes . How often are NGINX logs in PMM Server rotated? PMM Server runs logrotate on a daily basis to rotate NGINX logs and keeps up to ten of the most recent log files. What privileges are required to monitor a MySQL instance? GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ; Can I monitor multiple service instances? You can add multiple instances of MySQL or some other service to be monitored from one PMM Client. In this case, you must provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add complete MySQL monitoring for two local MySQL servers, the commands would be: sudo pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 sudo pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 For more information, run: pmm-admin add mysql --help Can I rename instances? You can remove any monitoring instance and then add it back with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name. Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition? By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] . How do I troubleshoot communication issues between PMM Client and PMM Server? Broken network connectivity may be due to many reasons. Particularly, when using Docker , the container is constrained by the host-level routing and firewall rules. For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM is also able to generate diagnostics data which can be examined and/or shared with Percona Support to help quickly solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way includes PMM Client logs and logs which were received from the PMM Server, stored separately in the client and server folders. The server folder also contains its own client subfolder with the self-monitoring client information collected on the PMM Server. Note Beginning with PMM version 2.4.0, there is an additional flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs in two ways: In a browser, visit https://<address-of-your-pmm-server>/logs.zip . Go to PMM > PMM Settings and click Download server diagnostics . (See Diagnostics in PMM Settings .) What resolution is used for metrics? The default values are: Low: 60 seconds Medium: 10 seconds High: 5 seconds (See Metrics resolution .) How do I set up Alerting in PMM? When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it either using the Grafana Alerting feature or by using an external alert manager. With these methods you must configure alerting rules that define conditions under which an alert should be triggered, and the channel used to send the alert (e.g. email). Alerting in Grafana allows attaching rules to your dashboard panels. Grafana Alerts are already integrated into PMM Server and may be simpler to get set up. Alertmanager allows the creation of more sophisticated alerting rules and can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. Note We can only offer support for creating custom rules to Percona customers, so you should already have a working Alertmanager instance prior to using this feature. See also PMM Alerting with Grafana: Working with Templated Dashboards How do I use a custom Prometheus configuration file inside PMM Server? Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration How to troubleshoot an Update? If PMM server wasn\u2019t updated properly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click with the Alt key on the reload icon in the Update panel (IMG needed) to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. Note You will not see the logs using this method. Refresh The Home page in 2-5 min and you should see that PMM was updated. What are my login credentials when I try to connect to a Prometheus Exporter? PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use \u201c pmm \u201d as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list . How to provision PMM Server with non-default admin password? Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMMPASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm2-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm2-server bash -c \"ln -s /srv/grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMMPASSWORD \"","title":"FAQ"},{"location":"faq.html#how-can-i-contact-the-developers","text":"The best place to discuss PMM with developers and other community members is the community forum . To report a bug, visit the PMM project in JIRA .","title":"How can I contact the developers?"},{"location":"faq.html#how-can-i-contact-the-technical-writers","text":"Open a Jira ticket Open a GitHub issue","title":"How can I contact the technical writers?"},{"location":"faq.html#what-are-the-minimum-system-requirements-for-pmm","text":"PMM Server Any system which can run Docker version 1.12.6 or later. It needs roughly 1 GB of storage for each monitored database node with data retention set to one week. Note By default, retention is set to 30 days for Metrics Monitor and for Query Analytics. You can consider disabling table statistics to decrease the VictoriaMetrics database size. You need at least 2 GB for one monitored database node. Note The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. Note Your CPU must support the SSE4.2 instruction set, a requirement of ClickHouse, a third-party column-oriented database used by Query Analytics. If your CPU is lacking this instruction set you won\u2019t be able to use QAN. PMM Client Any modern 64-bit Linux distribution. It is tested on the latest versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.)","title":"What are the minimum system requirements for PMM?"},{"location":"faq.html#how-can-i-upgrade-from-pmm-version-1","text":"Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age out during the course of the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Note Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. However, we are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules.","title":"How can I upgrade from PMM version 1?"},{"location":"faq.html#how-to-control-data-retention-for-pmm","text":"By default, PMM stores time-series data for 30 days. Depending on your available disk space and requirements, you may need to adjust the data retention time: Go to PMM > PMM Settings > Advanced Settings . Change the data retention value. Click Apply changes .","title":"How to control data retention for PMM?"},{"location":"faq.html#how-often-are-nginx-logs-in-pmm-server-rotated","text":"PMM Server runs logrotate on a daily basis to rotate NGINX logs and keeps up to ten of the most recent log files.","title":"How often are NGINX logs in PMM Server rotated?"},{"location":"faq.html#what-privileges-are-required-to-monitor-a-mysql-instance","text":"GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ;","title":"What privileges are required to monitor a MySQL instance?"},{"location":"faq.html#can-i-monitor-multiple-service-instances","text":"You can add multiple instances of MySQL or some other service to be monitored from one PMM Client. In this case, you must provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add complete MySQL monitoring for two local MySQL servers, the commands would be: sudo pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 sudo pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 For more information, run: pmm-admin add mysql --help","title":"Can I monitor multiple service instances?"},{"location":"faq.html#can-i-rename-instances","text":"You can remove any monitoring instance and then add it back with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name.","title":"Can I rename instances?"},{"location":"faq.html#can-i-add-an-aws-rds-mysql-or-aurora-mysql-instance-from-a-non-default-aws-partition","text":"By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] .","title":"Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition?"},{"location":"faq.html#how-do-i-troubleshoot-communication-issues-between-pmm-client-and-pmm-server","text":"Broken network connectivity may be due to many reasons. Particularly, when using Docker , the container is constrained by the host-level routing and firewall rules. For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM is also able to generate diagnostics data which can be examined and/or shared with Percona Support to help quickly solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way includes PMM Client logs and logs which were received from the PMM Server, stored separately in the client and server folders. The server folder also contains its own client subfolder with the self-monitoring client information collected on the PMM Server. Note Beginning with PMM version 2.4.0, there is an additional flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs in two ways: In a browser, visit https://<address-of-your-pmm-server>/logs.zip . Go to PMM > PMM Settings and click Download server diagnostics . (See Diagnostics in PMM Settings .)","title":"How do I troubleshoot communication issues between PMM Client and PMM Server?"},{"location":"faq.html#what-resolution-is-used-for-metrics","text":"The default values are: Low: 60 seconds Medium: 10 seconds High: 5 seconds (See Metrics resolution .)","title":"What resolution is used for metrics?"},{"location":"faq.html#how-do-i-set-up-alerting-in-pmm","text":"When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it either using the Grafana Alerting feature or by using an external alert manager. With these methods you must configure alerting rules that define conditions under which an alert should be triggered, and the channel used to send the alert (e.g. email). Alerting in Grafana allows attaching rules to your dashboard panels. Grafana Alerts are already integrated into PMM Server and may be simpler to get set up. Alertmanager allows the creation of more sophisticated alerting rules and can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. Note We can only offer support for creating custom rules to Percona customers, so you should already have a working Alertmanager instance prior to using this feature. See also PMM Alerting with Grafana: Working with Templated Dashboards","title":"How do I set up Alerting in PMM?"},{"location":"faq.html#how-do-i-use-a-custom-prometheus-configuration-file-inside-pmm-server","text":"Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration","title":"How do I use a custom Prometheus configuration file inside PMM Server?"},{"location":"faq.html#how-to-troubleshoot-an-update","text":"If PMM server wasn\u2019t updated properly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click with the Alt key on the reload icon in the Update panel (IMG needed) to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. Note You will not see the logs using this method. Refresh The Home page in 2-5 min and you should see that PMM was updated.","title":"How to troubleshoot an Update?"},{"location":"faq.html#what-are-my-login-credentials-when-i-try-to-connect-to-a-prometheus-exporter","text":"PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use \u201c pmm \u201d as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list .","title":"What are my login credentials when I try to connect to a Prometheus Exporter?"},{"location":"faq.html#how-to-provision-pmm-server-with-non-default-admin-password","text":"Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMMPASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm2-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm2-server bash -c \"ln -s /srv/grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMMPASSWORD \"","title":"How to provision PMM Server with non-default admin password?"},{"location":"details/index.html","text":"Details Architecture : high-level architecture and main components. User interface components : Descriptions of the main menus and icons Dashboards reference : A complete list of dashboards by category, with screenshots Commands: pmm-admin : The manual page for the PMM administration tool pmm-agent : The manual page for the PMM Client agent program API : How to access the Swagger API VictoriaMetrics : the third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 Glossary : A list of obscure terms and definitions","title":"Details"},{"location":"details/api.html","text":"API PMM Server lets you visually interact with API resources representing all objects within PMM. You can browse the API using the Swagger UI, accessible at the /swagger/ endpoint URL: Clicking an object lets you examine objects and execute requests on them: The objects visible are nodes, services, and agents: A Node represents a bare metal server, a virtual machine, a Docker container, or a more specific type such as an Amazon RDS Node. A node runs zero or more Services and Agents, and has zero or more Agents providing insights for it. A Service represents something useful running on the Node: Amazon Aurora MySQL, MySQL, MongoDB, etc. It runs on zero (Amazon Aurora Serverless), single (MySQL), or several (Percona XtraDB Cluster) Nodes. It also has zero or more Agents providing insights for it. An Agent represents something that runs on the Node which is not useful in itself but instead provides insights (metrics, query performance data, etc) about Nodes and/or Services. An agent always runs on the single Node (except External Exporters), and provides insights for zero or more Services and Nodes. Nodes, Services, and Agents have Types which define specific their properties, and the specific logic they implement. Nodes and Services are external by nature \u2013 we do not manage them (create, destroy), but merely maintain a list of them (add to inventory, remove from inventory) in pmm-managed . Most Agents, however, are started and stopped by pmm-agent . The only exception is the External Exporter Type which is started externally.","title":"API"},{"location":"details/architecture.html","text":"Architecture PMM works on the client/server principle, where a single server instance communicates with one or more clients. Except when monitoring AWS RDS instances, a PMM Client must be running on the host to be monitored. PMM context The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent : Run as a daemon process, it starts and stops exporters when instructed. vmagent : A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics PMM Server PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze MySQL query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the back-end for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN Web App is a web application for visualizing collected Query Analytics data. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona. PMM Client The PMM Client package consist of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more. ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"Architecture"},{"location":"details/architecture.html#pmm-context","text":"The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent : Run as a daemon process, it starts and stops exporters when instructed. vmagent : A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics","title":"PMM context"},{"location":"details/architecture.html#pmm-server","text":"PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze MySQL query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the back-end for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN Web App is a web application for visualizing collected Query Analytics data. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona.","title":"PMM Server"},{"location":"details/architecture.html#pmm-client","text":"The PMM Client package consist of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more. ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"PMM Client"},{"location":"details/glossary.html","text":"div.section dl.glossary dt {font-weight: bold; font-size: 1.3em;} div.section dd {margin-top: 10px; margin-bottom: 10px; margin-left: 30px;} Glossary Annotation A way of showing a mark on dashboards signifying an important point in time. Dimension In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host EBS Amazon\u2019s Elastic Block Store. Fingerprint A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query. IAM Identity and Access Management (for Amazon AWS). MM Metrics Monitor. NUMA Non-Uniform Memory Access. PEM Privacy Enhanced Mail. QPS Queries Per Second. A measure of the rate of queries being monitored. Query Analytics Component of PMM Server that enables you to analyze MySQL query performance over periods of time. STT Security Threat Tool. Technical Preview Releases intended for public preview and feedback but with no support or SLAs. Should not be used on production or business-critical systems. May contain breaking changes to UI, API, CLI. ( Read more .) VG Volume Group.","title":"Glossary"},{"location":"details/glossary.html#annotation","text":"A way of showing a mark on dashboards signifying an important point in time.","title":"Annotation"},{"location":"details/glossary.html#dimension","text":"In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host","title":"Dimension"},{"location":"details/glossary.html#ebs","text":"Amazon\u2019s Elastic Block Store.","title":"EBS"},{"location":"details/glossary.html#fingerprint","text":"A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query.","title":"Fingerprint"},{"location":"details/glossary.html#iam","text":"Identity and Access Management (for Amazon AWS).","title":"IAM"},{"location":"details/glossary.html#mm","text":"Metrics Monitor.","title":"MM"},{"location":"details/glossary.html#numa","text":"Non-Uniform Memory Access.","title":"NUMA"},{"location":"details/glossary.html#pem","text":"Privacy Enhanced Mail.","title":"PEM"},{"location":"details/glossary.html#qps","text":"Queries Per Second. A measure of the rate of queries being monitored.","title":"QPS"},{"location":"details/glossary.html#query-analytics","text":"Component of PMM Server that enables you to analyze MySQL query performance over periods of time.","title":"Query Analytics"},{"location":"details/glossary.html#stt","text":"Security Threat Tool.","title":"STT"},{"location":"details/glossary.html#technical-preview","text":"Releases intended for public preview and feedback but with no support or SLAs. Should not be used on production or business-critical systems. May contain breaking changes to UI, API, CLI. ( Read more .)","title":"Technical Preview"},{"location":"details/glossary.html#vg","text":"Volume Group.","title":"VG"},{"location":"details/interface.html","text":"User Interface components Main menu (also Grafana menu , side menu ) Navigation bar View controls View selectors (dynamic contents) Shortcut menu (dynamic contents) Main menu The main menu is part of the Grafana framework and is visible on every page. Item (Top) Name Description Home Link to home dashboard Search Search dashboards by name Create Create dashboards or folders , import dashboards Dashboards Manage dashboards, create playlists, manage snapshots Explore Run queries with PromQL Alerting Alerting, Integrated Alerting , Alert Rules, Notification Channels Configuration Server Admin DBaaS Note The DBaaS icon appears only if a server feature flag has been set. Icon (Bottom) Description (Profile icon) User menu Help Navigation bar Item (left) Description (Display only) (Name) / (Optional) Folder name (Name) Dashboard name Mark as favorite Share dashboard View controls Item (right) Description Dashboard settings Cycle view mode (time range) Time range selector Time range zoom out Refresh dashboard (Time interval) Refresh period View selectors This menu bar is context sensitive; it changes according to the page you are on. (With wide menus on small screens, items may wrap to the next row.) Item Description Interval Data interval Region Filter by region Environment Filter by environment Cluster Filter by cluster Replication Set Filter by replication set Node Name Filter by node name Service Name Filter by service name PMM Annotations View annotations Shortcut menu This menu contains shortcuts to other dashboards. The list changes according to the page you\u2019re on. Item Description Home Home dashboard Query Analytics Query Analytics Compare Nodes compare (Service Type) Service type menu (see below) HA HA dashboards Services Services menu PMM PMM menu Note The Compare menu links to the Instances Overview dashboard for the current service type. Services menu The Services menu choice determines the Service Type menu. Menu Item Service type menu Description Services MongoDB Instances Overview MongoDB MongoDB dashboards MySQL Instances Overview MySQL MySQL dashboards Nodes Overview OS OS dashboards PostgreSQL Instances Overview PostgreSQL PostgreSQL dashboards PMM menu This item lists shortcuts to utility pages. Menu Item PMM PMM Add Instance PMM Database Checks PMM Inventory PMM Settings","title":"User Interface components"},{"location":"details/interface.html#main-menu","text":"The main menu is part of the Grafana framework and is visible on every page. Item (Top) Name Description Home Link to home dashboard Search Search dashboards by name Create Create dashboards or folders , import dashboards Dashboards Manage dashboards, create playlists, manage snapshots Explore Run queries with PromQL Alerting Alerting, Integrated Alerting , Alert Rules, Notification Channels Configuration Server Admin DBaaS Note The DBaaS icon appears only if a server feature flag has been set. Icon (Bottom) Description (Profile icon) User menu Help","title":"Main menu"},{"location":"details/interface.html#navigation-bar","text":"Item (left) Description (Display only) (Name) / (Optional) Folder name (Name) Dashboard name Mark as favorite Share dashboard","title":"Navigation bar"},{"location":"details/interface.html#view-controls","text":"Item (right) Description Dashboard settings Cycle view mode (time range) Time range selector Time range zoom out Refresh dashboard (Time interval) Refresh period","title":"View controls"},{"location":"details/interface.html#view-selectors","text":"This menu bar is context sensitive; it changes according to the page you are on. (With wide menus on small screens, items may wrap to the next row.) Item Description Interval Data interval Region Filter by region Environment Filter by environment Cluster Filter by cluster Replication Set Filter by replication set Node Name Filter by node name Service Name Filter by service name PMM Annotations View annotations","title":"View selectors"},{"location":"details/interface.html#shortcut-menu","text":"This menu contains shortcuts to other dashboards. The list changes according to the page you\u2019re on. Item Description Home Home dashboard Query Analytics Query Analytics Compare Nodes compare (Service Type) Service type menu (see below) HA HA dashboards Services Services menu PMM PMM menu Note The Compare menu links to the Instances Overview dashboard for the current service type.","title":"Shortcut menu"},{"location":"details/victoria-metrics.html","text":"VictoriaMetrics VictoriaMetrics is a third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 . Push/Pull modes VictoriaMetrics allows metrics data to be \u2018pushed\u2019 to the server in addition to it being \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. Note For PMM 2.12.0 the default mode is \u2018pull\u2019. Later releases will use the \u2018push\u2019 mode by default for newly-added services. The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (There is currently no ability to \u201cupdate\u201d a service.) Remapped targets for direct Prometheus paths Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Here are their equivalents. /prometheus/alerts \u2192 No change. /prometheus/config \u2192 No equivalent. However, some information is at /prometheus/targets . /prometheus/flags \u2192 The flag metrics at /prometheus/metrics . /prometheus/graph \u2192 /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules \u2192 No change. /prometheus/service-discovery \u2192 No equivalent. /prometheus/status \u2192 Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets \u2192 /victoriametrics/targets . Troubleshooting To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"VictoriaMetrics"},{"location":"details/victoria-metrics.html#pushpull-modes","text":"VictoriaMetrics allows metrics data to be \u2018pushed\u2019 to the server in addition to it being \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. Note For PMM 2.12.0 the default mode is \u2018pull\u2019. Later releases will use the \u2018push\u2019 mode by default for newly-added services. The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (There is currently no ability to \u201cupdate\u201d a service.)","title":"Push/Pull modes"},{"location":"details/victoria-metrics.html#remapped-targets-for-direct-prometheus-paths","text":"Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Here are their equivalents. /prometheus/alerts \u2192 No change. /prometheus/config \u2192 No equivalent. However, some information is at /prometheus/targets . /prometheus/flags \u2192 The flag metrics at /prometheus/metrics . /prometheus/graph \u2192 /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules \u2192 No change. /prometheus/service-discovery \u2192 No equivalent. /prometheus/status \u2192 Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets \u2192 /victoriametrics/targets .","title":"Remapped targets for direct Prometheus paths"},{"location":"details/victoria-metrics.html#troubleshooting","text":"To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"Troubleshooting"},{"location":"details/commands/index.html","text":"Commands pmm-admin \u2013 Command line tool for configuring and administering PMM pmm-agent \u2013 Daemon process, communicating between PMM Client and PMM Server","title":"Commands"},{"location":"details/commands/pmm-admin.html","text":"pmm-admin - Administration Tool NAME SYNOPSIS DESCRIPTION COMMON FLAGS COMMANDS GENERAL COMMANDS INFORMATION COMMANDS CONFIGURATION COMMANDS DATABASE COMMANDS OTHER COMMANDS EXAMPLES Disable collectors NAME pmm-admin - Administer PMM SYNOPSIS pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add haproxy [FLAGS] [NAME] pmm-admin add external [FLAGS] [NAME] [ADDRESS] pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND] DESCRIPTION pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process. COMMON FLAGS -h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external COMMANDS GENERAL COMMANDS pmm-admin help [COMMAND] Show help for COMMAND . INFORMATION COMMANDS pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default file name summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof Include performance profiling data in the summary. CONFIGURATION COMMANDS pmm-admin config pmm-admin config [FLAGS] [node-address] [node-type] [node-name] Configure a local pmm-agent . FLAGS: --node-id=node-id Node ID (default is auto-detected). --node-model=node-model Node model --region=region Node region --az=availability-zone Node availability zone --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent pmm-admin register pmm-admin register [FLAGS] [node-address] [node-type] [node-name] Register the current Node with the PMM Server. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --machine-id=\"/machine_id/9812826a1c45454a98ba45c56cc4f5b0\" Node machine-id (default is auto-detected). --distro=\"linux\" Node OS distribution (default is auto-detected). --container-id=container-id Container ID. --container-name=container-name Container name. --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --custom-labels=labels Custom user-assigned labels. pmm-admin remove pmm-admin remove [FLAGS] service-type [service-name] Remove Service from monitoring. --service-id=service-id Service ID. --force Remove service with that name or ID and all dependent services and agents. When you remove a service, collected data remains on PMM Server for the specified retention period . pmm-admin annotate pmm-admin annotate [--node|--service] <annotation> [--tags <tags>] [--node-name=<node>] [--service-name=<service>] Annotate an event. ( Read more ) <annotation> The annotation string. If it contains spaces, it should be quoted. --node Annotate the current node or that specified by --node-name . --service Annotate all services running on the current node, or that specified by --service-name . --tags A quoted string that defines one or more comma-separated tags for the annotation. Example: \"tag 1,tag 2\" . --node-name The node name being annotated. --service-name The service name being annotated. Combining flags Flags may be combined as shown in the following examples. --node current node --node-name node with name --node --node-name=NODE_NAME node with name --node --service-name current node and service with name --node --node-name --service-name node with name and service with name --node --service current node and all services of current node -node --node-name --service --service-name service with name and node with name --service all services of the current node --service-name service with name --service --service-name service with name --service --node-name all services of current node and node with name --service-name --node-name service with name and node with name --service --service-name -node-name service with name and node with name Note If node or service name is specified, they are used instead of other parameters. DATABASE COMMANDS MongoDB pmm-admin add mongodb [FLAGS] [node-name] [node-address] Add MongoDB to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MongoDB username. --password=password MongoDB password. --query-source=profiler Source of queries, one of: profiler , none (default: profiler ). --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file-password=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent MySQL pmm-admin add mysql [FLAGS] node-name node-address | [--name=service-name] --address=address[:port] | --socket Add MySQL to monitoring. FLAGS: --address MySQL address and port (default: 127.0.0.1:3306). --socket=socket Path to MySQL socket. (Find the socket path with mysql -u root -p -e \"select @@socket\" .) --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MySQL username. --password=password MySQL password. --query-source=slowlog Source of SQL queries, one of: slowlog , perfschema , none (default: slowlog ). --size-slow-logs=N Rotate slow log file at this size. If 0 , use server-defined default. Negative values disable log rotation. A unit suffix must be appended to the number and can be one of: KB , kB , MB , mB , GB , gB , TB , tB for base 10 units (1000, 1000000, etc); KiB , MiB , GiB , TiB for base 2 units (1024, 1048576, etc). --disable-queryexamples Disable collection of query examples. --disable-tablestats Disable table statistics collection. Excluded collectors for low-resolution time intervals: --collect.auto_increment.columns --collect.info_schema.tables --collect.info_schema.tablestats --collect.perf_schema.indexiowaits --collect.perf_schema.tableiowaits --collect.perf_schema.file_instances Excluded collectors for medium-resolution time intervals: --collect.perf_schema.tablelocks --disable-tablestats-limit=disable-tablestats-limit Table statistics collection will be disabled if there are more than specified number of tables (default: server-defined). 0=no limit. Negative value disables collection. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent PostgreSQL pmm-admin add postgresql [FLAGS] [node-name] [node-address] Add PostgreSQL to monitoring. FLAGS: --node-id=<node id> Node ID (default is auto-detected). --pmm-agent-id=<pmm agent id> The pmm-agent identifier which runs this instance (default is auto-detected). --username=<username> PostgreSQL username. --password=<password> PostgreSQL password. --query-source=<query source> Source of SQL queries, one of: pgstatements , pgstatmonitor , none (default: pgstatements ). --environment=<environment> Environment name. --cluster=<cluster> Cluster name. --replication-set=<replication set> Replication set name --custom-labels=<custom labels> Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent ProxySQL pmm-admin add proxysql [FLAGS] [node-name] [node-address] Add ProxySQL to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username ProxySQL username. --password=password ProxySQL password. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent --disable-collectors Comma-separated list of collector names to exclude from exporter HAProxy pmm-admin add haproxy [FLAGS] [NAME] Add HAProxy to monitoring. FLAGS: --server-url=SERVER-URL PMM Server URL in https://username:password@pmm-server-host/ format --server-insecure-tls Skip PMM Server TLS certificate validation. --username=USERNAME HAProxy username. --password=PASSWORD HAProxy password. --scheme=SCHEME Scheme to generate URI to exporter metrics endpoints (http or https). --metrics-path=METRICS-PATH Path under which metrics are exposed, used to generate URI (default: /metrics). --listen-port=LISTEN-PORT Listen port of haproxy exposing the metrics for scraping metrics (Required). --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is auto-detected). --environment=ENVIRONMENT Environment name like \u2018production\u2019 or \u2018qa\u2019. --cluster=CLUSTER Cluster name. --replication-set=REPLICATION-SET Replication set name. --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1. --metrics-mode=MODE Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent --skip-connection-check Skip connection check. OTHER COMMANDS pmm-admin add external [FLAGS] Add External source of data (like a custom exporter running on a port) to the monitoring FLAGS: --service-name=\"current-hostname\" Service name (autodetected defaults to the hostname where pmm-admin is running) --agent-node-id=AGENT-NODE-ID Node ID where agent runs (default is autodetected) --username=USERNAME External username --password=PASSWORD External password --scheme=http or https Scheme to generate URI to exporter metrics endpoints --metrics-path=/metrics Path under which metrics are exposed, used to generate URI. --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics. (Required) --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is autodetected) --environment=prod Environment name like \u2018production\u2019 or \u2018qa\u2019 --cluster=east-cluster Cluster name --replication-set=rs1 Replication set name --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1 --metrics-mode=auto Metrics flow mode, can be push : agent will push metrics, pull : server scrape metrics from agent or auto : chosen by server. --group=\"external\" Group name of external service (default: external) pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring. Usage example: sudo pmm-admin add external-serverless --url=http://1.2.3.4:9093/metrics Also, individual parameters can be set instead of --url like: sudo pmm-admin add external-serverless --scheme=http --host=1.2.3.4 --listen-port=9093 --metrics-path=/metrics --container-name=ddd --external-name=e125 Notice that some parameters are mandatory depending on the context. For example, if you specify --url , --schema and other related parameters are not mandatory but, if you specify --host you must provide all other parameters needed to build the destination URL or even you can specify --address instead of host and port as individual parameters. FLAGS: --url=URL Full URL to exporter metrics endpoints --scheme=https Scheme to generate URL to exporter metrics endpoints --username=USERNAME External username --password=PASSWORD External password --address=1.2.3.4:9000 External exporter address and port --host=1.2.3.4 External exporters hostname or IP address --listen-port=9999 Listen port of external exporter for scraping metrics. --metrics-path=/metrics Path under which metrics are exposed, used to generate URL. --environment=testing Environment name --cluster=CLUSTER Cluster name --replication-set=rs1 Replication set name --custom-labels='app=myapp,region=s1' Custom user-assigned labels --group=\"external\" Group name of external service (default: external) --machine-id=MACHINE-ID Node machine-id --distro=DISTRO Node OS distribution --container-id=CONTAINER-ID Container ID --container-name=CONTAINER-NAME Container name --node-model=NODE-MODEL Node model --region=REGION Node region --az=AZ Node availability zone EXAMPLES pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running Disable collectors pmm-admin add mysql --disable-collectors = 'heartbeat,global_status,info_schema.innodb_cmp' --username = pmm --password = pmm --service-name = db1-mysql --host = 127 .0.0.1 --port = 3306 For other collectors that you can disable with the --disable-collectors option, please visit the official repositories for each exporter: node_exporter mysqld_exporter mongodb_exporter postgres_exporter proxysql_exporter","title":"pmm-admin - Administration Tool"},{"location":"details/commands/pmm-admin.html#name","text":"pmm-admin - Administer PMM","title":"NAME"},{"location":"details/commands/pmm-admin.html#synopsis","text":"pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] DATABASE:= [ MongoDB | MySQL | PostgreSQL | ProxySQL ] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add haproxy [FLAGS] [NAME] pmm-admin add external [FLAGS] [NAME] [ADDRESS] pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND]","title":"SYNOPSIS"},{"location":"details/commands/pmm-admin.html#description","text":"pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process.","title":"DESCRIPTION"},{"location":"details/commands/pmm-admin.html#common-flags","text":"-h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external","title":"COMMON FLAGS"},{"location":"details/commands/pmm-admin.html#commands","text":"","title":"COMMANDS"},{"location":"details/commands/pmm-admin.html#general-commands","text":"pmm-admin help [COMMAND] Show help for COMMAND .","title":"GENERAL COMMANDS"},{"location":"details/commands/pmm-admin.html#information-commands","text":"pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default file name summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof Include performance profiling data in the summary.","title":"INFORMATION COMMANDS"},{"location":"details/commands/pmm-admin.html#configuration-commands","text":"","title":"CONFIGURATION COMMANDS"},{"location":"details/commands/pmm-admin.html#database-commands","text":"","title":"DATABASE COMMANDS"},{"location":"details/commands/pmm-admin.html#other-commands","text":"pmm-admin add external [FLAGS] Add External source of data (like a custom exporter running on a port) to the monitoring FLAGS: --service-name=\"current-hostname\" Service name (autodetected defaults to the hostname where pmm-admin is running) --agent-node-id=AGENT-NODE-ID Node ID where agent runs (default is autodetected) --username=USERNAME External username --password=PASSWORD External password --scheme=http or https Scheme to generate URI to exporter metrics endpoints --metrics-path=/metrics Path under which metrics are exposed, used to generate URI. --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics. (Required) --service-node-id=SERVICE-NODE-ID Node ID where service runs (default is autodetected) --environment=prod Environment name like \u2018production\u2019 or \u2018qa\u2019 --cluster=east-cluster Cluster name --replication-set=rs1 Replication set name --custom-labels=CUSTOM-LABELS Custom user-assigned labels. Example: region=east,app=app1 --metrics-mode=auto Metrics flow mode, can be push : agent will push metrics, pull : server scrape metrics from agent or auto : chosen by server. --group=\"external\" Group name of external service (default: external) pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring. Usage example: sudo pmm-admin add external-serverless --url=http://1.2.3.4:9093/metrics Also, individual parameters can be set instead of --url like: sudo pmm-admin add external-serverless --scheme=http --host=1.2.3.4 --listen-port=9093 --metrics-path=/metrics --container-name=ddd --external-name=e125 Notice that some parameters are mandatory depending on the context. For example, if you specify --url , --schema and other related parameters are not mandatory but, if you specify --host you must provide all other parameters needed to build the destination URL or even you can specify --address instead of host and port as individual parameters. FLAGS: --url=URL Full URL to exporter metrics endpoints --scheme=https Scheme to generate URL to exporter metrics endpoints --username=USERNAME External username --password=PASSWORD External password --address=1.2.3.4:9000 External exporter address and port --host=1.2.3.4 External exporters hostname or IP address --listen-port=9999 Listen port of external exporter for scraping metrics. --metrics-path=/metrics Path under which metrics are exposed, used to generate URL. --environment=testing Environment name --cluster=CLUSTER Cluster name --replication-set=rs1 Replication set name --custom-labels='app=myapp,region=s1' Custom user-assigned labels --group=\"external\" Group name of external service (default: external) --machine-id=MACHINE-ID Node machine-id --distro=DISTRO Node OS distribution --container-id=CONTAINER-ID Container ID --container-name=CONTAINER-NAME Container name --node-model=NODE-MODEL Node model --region=REGION Node region --az=AZ Node availability zone","title":"OTHER COMMANDS"},{"location":"details/commands/pmm-admin.html#examples","text":"pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running","title":"EXAMPLES"},{"location":"details/commands/pmm-admin.html#disable-collectors","text":"pmm-admin add mysql --disable-collectors = 'heartbeat,global_status,info_schema.innodb_cmp' --username = pmm --password = pmm --service-name = db1-mysql --host = 127 .0.0.1 --port = 3306 For other collectors that you can disable with the --disable-collectors option, please visit the official repositories for each exporter: node_exporter mysqld_exporter mongodb_exporter postgres_exporter proxysql_exporter","title":"Disable collectors"},{"location":"details/commands/pmm-agent.html","text":"pmm-agent - PMM Client agent NAME pmm-agent - The PMM Client daemon program SYNOPSIS pmm-agent [command] [options] DESCRIPTION pmm-agent, part of the PMM Client package, runs as a daemon process on all monitored hosts. COMMANDS pmm-agent run Run pmm-agent (default) pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent pmm-agent help [command] Show help (for command) and exit OPTIONS AND ENVIRONMENT Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-pt-mysql-summary=PATH PMM_AGENT_PATHS_PT_MYSQL_SUMMARY Path to pt-mysql-summary . --paths-pt-pg-summary=PATH PMM_AGENT_PATHS_PT_PG_SUMMARY Path to pt-pg-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, time-stamp, git commit hash and branch. LOGGING By default, pmm-agent sends messages to stderr and to the system log ( syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd -based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError=file:/var/log/pmm-agent.log initd -based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log=\"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"pmm-agent - PMM Client agent"},{"location":"details/commands/pmm-agent.html#name","text":"pmm-agent - The PMM Client daemon program","title":"NAME"},{"location":"details/commands/pmm-agent.html#synopsis","text":"pmm-agent [command] [options]","title":"SYNOPSIS"},{"location":"details/commands/pmm-agent.html#description","text":"pmm-agent, part of the PMM Client package, runs as a daemon process on all monitored hosts.","title":"DESCRIPTION"},{"location":"details/commands/pmm-agent.html#commands","text":"pmm-agent run Run pmm-agent (default) pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent pmm-agent help [command] Show help (for command) and exit","title":"COMMANDS"},{"location":"details/commands/pmm-agent.html#options-and-environment","text":"Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-pt-mysql-summary=PATH PMM_AGENT_PATHS_PT_MYSQL_SUMMARY Path to pt-mysql-summary . --paths-pt-pg-summary=PATH PMM_AGENT_PATHS_PT_PG_SUMMARY Path to pt-pg-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, time-stamp, git commit hash and branch.","title":"OPTIONS AND ENVIRONMENT"},{"location":"details/commands/pmm-agent.html#logging","text":"By default, pmm-agent sends messages to stderr and to the system log ( syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd -based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError=file:/var/log/pmm-agent.log initd -based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log=\"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"LOGGING"},{"location":"details/dashboards/index.html","text":"Dashboards Insight Advanced Data Exploration Home Dashboard Prometheus Exporter Status Prometheus Exporters Overview VictoriaMetrics VictoriaMetrics Agents Overview PMM PMM Inventory OS Dashboards CPU Utilization Details Disk Details Network Details Memory Details Node Temperature Details Nodes Compare Nodes Overview Node Summary NUMA Details Processes Details Prometheus Dashboards Prometheus Exporter Status Prometheus Exporters Overview MySQL Dashboards MySQL Amazon Aurora Details MySQL Command/Handler Counters Compare MySQL InnoDB Compression Details MySQL InnoDB Details MySQL MyISAM/Aria Details MySQL MyRocks Details MySQL Instance Summary MySQL Instances Compare MySQL Instances Overview MySQL Wait Event Analyses Details MySQL Performance Schema Details MySQL Query Response Time Details MySQL Replication Summary MySQL Group Replication Summary MySQL Table Details MySQL User Details MySQL TokuDB Details MongoDB Dashboards MongoDB Cluster Summary MongoDB Instance Summary MongoDB Instances Overview MongoDB Instances Compare MongoDB ReplSet Summary MongoDB InMemory Details MongoDB MMAPv1 Details MongoDB WiredTiger Details PostgreSQL Dashboards PostgreSQL Instances Overview PostgreSQL Instance Summary PostgreSQL Instances Compare ProxySQL Dashboards ProxySQL Instance Summary HA Dashboards PXC/Galera Node Summary PXC/Galera Cluster Summary PXC/Galera Nodes Compare HAProxy Instance Summary","title":"Dashboards"},{"location":"details/dashboards/index.html#insight","text":"Advanced Data Exploration Home Dashboard Prometheus Exporter Status Prometheus Exporters Overview VictoriaMetrics VictoriaMetrics Agents Overview","title":"Insight"},{"location":"details/dashboards/index.html#pmm","text":"PMM Inventory","title":"PMM"},{"location":"details/dashboards/index.html#os-dashboards","text":"CPU Utilization Details Disk Details Network Details Memory Details Node Temperature Details Nodes Compare Nodes Overview Node Summary NUMA Details Processes Details","title":"OS Dashboards"},{"location":"details/dashboards/index.html#prometheus-dashboards","text":"Prometheus Exporter Status Prometheus Exporters Overview","title":"Prometheus Dashboards"},{"location":"details/dashboards/index.html#mysql-dashboards","text":"MySQL Amazon Aurora Details MySQL Command/Handler Counters Compare MySQL InnoDB Compression Details MySQL InnoDB Details MySQL MyISAM/Aria Details MySQL MyRocks Details MySQL Instance Summary MySQL Instances Compare MySQL Instances Overview MySQL Wait Event Analyses Details MySQL Performance Schema Details MySQL Query Response Time Details MySQL Replication Summary MySQL Group Replication Summary MySQL Table Details MySQL User Details MySQL TokuDB Details","title":"MySQL Dashboards"},{"location":"details/dashboards/index.html#mongodb-dashboards","text":"MongoDB Cluster Summary MongoDB Instance Summary MongoDB Instances Overview MongoDB Instances Compare MongoDB ReplSet Summary MongoDB InMemory Details MongoDB MMAPv1 Details MongoDB WiredTiger Details","title":"MongoDB Dashboards"},{"location":"details/dashboards/index.html#postgresql-dashboards","text":"PostgreSQL Instances Overview PostgreSQL Instance Summary PostgreSQL Instances Compare","title":"PostgreSQL Dashboards"},{"location":"details/dashboards/index.html#proxysql-dashboards","text":"ProxySQL Instance Summary","title":"ProxySQL Dashboards"},{"location":"details/dashboards/index.html#ha-dashboards","text":"PXC/Galera Node Summary PXC/Galera Cluster Summary PXC/Galera Nodes Compare HAProxy Instance Summary","title":"HA Dashboards"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html","text":"Advanced Data Exploration The Advanced Data Exploration dashboard provides detailed information about the progress of a single Prometheus metric across one or more hosts. View actual metric values (Gauge) A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines. View Metric Rate of Change (Counter) A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case. Metric Rates Shows the number of samples Per second stored for a given interval in the time series. Note This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Advanced Data Exploration"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-actual-metric-values-gauge","text":"A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines.","title":"View actual metric values (Gauge)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-metric-rate-of-change-counter","text":"A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case.","title":"View Metric Rate of Change (Counter)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#metric-rates","text":"Shows the number of samples Per second stored for a given interval in the time series. Note This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Metric Rates"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html","text":"CPU Utilization Details Overall CPU Utilization The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. Note This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%. Current CPU Threads Utilization This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause. CPU Threads Frequency No description Current CPU Cores Temperature No description Overall CPU Threads Utilization Details No description","title":"OS Dashboards"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-utilization","text":"The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. Note This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%.","title":"Overall CPU Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-threads-utilization","text":"This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause.","title":"Current CPU Threads Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#cpu-threads-frequency","text":"No description","title":"CPU Threads Frequency"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-cores-temperature","text":"No description","title":"Current CPU Cores Temperature"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-threads-utilization-details","text":"No description","title":"Overall CPU Threads Utilization Details"},{"location":"details/dashboards/dashboard-disk-details.html","text":"Disk Details mount point Usage Shows the percentage of disk space utilization for every mount point defined on the system. Having some of the mount points close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mount point is close to 100% consider removing unused files or expanding the space allocated to the mount point. mount point Shows information about the disk space usage of the specified mount point. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mount point. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mount point. Disk Latency Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk Operations Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload. Disk Bandwidth Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time. Disk Load Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Disk IO Utilization Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead. Avg Disks Operations Merge Ratio Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization. Disk IO Size Shows average size of a single disk operation.","title":"Disk Details"},{"location":"details/dashboards/dashboard-disk-details.html#mount-point-usage","text":"Shows the percentage of disk space utilization for every mount point defined on the system. Having some of the mount points close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mount point is close to 100% consider removing unused files or expanding the space allocated to the mount point.","title":"mount point Usage"},{"location":"details/dashboards/dashboard-disk-details.html#mount-point","text":"Shows information about the disk space usage of the specified mount point. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mount point. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mount point.","title":"mount point"},{"location":"details/dashboards/dashboard-disk-details.html#disk-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk Latency"},{"location":"details/dashboards/dashboard-disk-details.html#disk-operations","text":"Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload.","title":"Disk Operations"},{"location":"details/dashboards/dashboard-disk-details.html#disk-bandwidth","text":"Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time.","title":"Disk Bandwidth"},{"location":"details/dashboards/dashboard-disk-details.html#disk-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk Load"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-utilization","text":"Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead.","title":"Disk IO Utilization"},{"location":"details/dashboards/dashboard-disk-details.html#avg-disks-operations-merge-ratio","text":"Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization.","title":"Avg Disks Operations Merge Ratio"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-size","text":"Shows average size of a single disk operation.","title":"Disk IO Size"},{"location":"details/dashboards/dashboard-haproxy-instance-summary.html","text":"HAProxy Instance Summary","title":"HAProxy Instance Summary"},{"location":"details/dashboards/dashboard-home.html","text":"Home Dashboard The Home Dashboard is a high-level overview of your environment, the starting page of the PMM portal from which you can open the tools of PMM, and browse to online resources. On the PMM home page, you can also find the version number and a button to update your PMM Server. General Information This section contains links to online resources, such as PMM documentation, releases notes, and blogs. Shared and Recently Used Dashboards This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked. Statistics This section shows the total number of hosts added to PMM and the total number of database instanced being monitored. This section also current the version number. Use the Check for Updates Manually button to see if you are using the most recent version of PMM. Environment Overview This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Insight"},{"location":"details/dashboards/dashboard-home.html#general-information","text":"This section contains links to online resources, such as PMM documentation, releases notes, and blogs.","title":"General Information"},{"location":"details/dashboards/dashboard-home.html#shared-and-recently-used-dashboards","text":"This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked.","title":"Shared and Recently Used Dashboards"},{"location":"details/dashboards/dashboard-home.html#statistics","text":"This section shows the total number of hosts added to PMM and the total number of database instanced being monitored. This section also current the version number. Use the Check for Updates Manually button to see if you are using the most recent version of PMM.","title":"Statistics"},{"location":"details/dashboards/dashboard-home.html#environment-overview","text":"This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Environment Overview"},{"location":"details/dashboards/dashboard-inventory.html","text":"PMM Inventory The Inventory dashboard is a high level overview of all objects PMM \u201cknows\u201d about. It contains three tabs ( services , agents , and nodes ) with lists of the correspondent objects and details about them, so that users are better able to understand which objects are registered against PMM Server. These objects are composing a hierarchy with Node at the top, then Service and Agents assigned to a Node. Nodes \u2013 Where the service and agents will run. Assigned a node_id , associated with a machine_id (from /etc/machine-id ). Few examples are bare metal, virtualized, container. Services \u2013 Individual service names and where they run, against which agents will be assigned. Each instance of a service gets a service_id value that is related to a node_id . Examples are MySQL, Amazon Aurora MySQL. This feature also allows to support multiple mysqld instances on a single node, with different service names, e.g. mysql1-3306, and mysql1-3307. Agents \u2013 Each binary (exporter, agent) running on a client will get an agent_id value. pmm-agent one is the top of the tree, assigned to a node_id node_exporter is assigned to pmm-agent agent_id mysqld_exporter & QAN MySQL Perfschema are assigned to a service_id . Examples are pmm-agent , node_exporter , mysqld_exporter , QAN MySQL Perfschema. Removing items from the inventory You can remove items from the inventory. Open Home Dashboard > PMM Inventory In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"PMM"},{"location":"details/dashboards/dashboard-inventory.html#removing-items-from-the-inventory","text":"You can remove items from the inventory. Open Home Dashboard > PMM Inventory In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"Removing items from the inventory"},{"location":"details/dashboards/dashboard-memory-details.html","text":"Memory Details Memory Usage No description","title":"Memory Details"},{"location":"details/dashboards/dashboard-memory-details.html#memory-usage","text":"No description","title":"Memory Usage"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html","text":"MongoDB Cluster Summary Current Connections Per Shard TCP connections (Incoming) in mongod processes. Total Connections Incoming connections to mongos nodes. Cursors Per Shard The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Mongos Cursors The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Operations Per Shard Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Total Mongos Operations Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Change Log Events Count, over last 10 minutes, of all types of configuration db changelog events. Oplog Range by Set Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"MongoDB Dashboards"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#current-connections-per-shard","text":"TCP connections (Incoming) in mongod processes.","title":"Current Connections Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-connections","text":"Incoming connections to mongos nodes.","title":"Total Connections"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#cursors-per-shard","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Cursors Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#mongos-cursors","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Mongos Cursors"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#operations-per-shard","text":"Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Operations Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-mongos-operations","text":"Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Total Mongos Operations"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#change-log-events","text":"Count, over last 10 minutes, of all types of configuration db changelog events.","title":"Change Log Events"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#oplog-range-by-set","text":"Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"Oplog Range by Set"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html","text":"MongoDB InMemory Details InMemory Transactions WiredTiger internal transactions InMemory Capacity Configured max and current size of the WiredTiger cache. InMemory Sessions Internal WiredTiger storage engine cursors and sessions currently open. InMemory Pages Pages in the WiredTiger cache InMemory Concurrency Tickets A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hard coded high value - \u201cTickets Out\u201d. Queued Operations Operations queued due to a lock Document Changes Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. InMemory Cache Eviction This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages. Scanned and Moved Objects This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB InMemory Details"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-transactions","text":"WiredTiger internal transactions","title":"InMemory Transactions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-capacity","text":"Configured max and current size of the WiredTiger cache.","title":"InMemory Capacity"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-sessions","text":"Internal WiredTiger storage engine cursors and sessions currently open.","title":"InMemory Sessions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-pages","text":"Pages in the WiredTiger cache","title":"InMemory Pages"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-concurrency-tickets","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hard coded high value - \u201cTickets Out\u201d.","title":"InMemory Concurrency Tickets"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#queued-operations","text":"Operations queued due to a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-cache-eviction","text":"This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages.","title":"InMemory Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html","text":"MongoDB Instance Summary Command Operations Ops or Replicated Ops/sec classified by legacy wire protocol type ( query , insert , update , delete , getmore ). And (from the internal TTL threads) the docs deletes/sec by TTL indexes. Latency Detail Average latency of operations (classified by read, write, or (other) command) Connections TCP connections (Incoming) Cursors Open cursors. Includes idle cursors. Document Operations Docs per second inserted, updated, deleted or returned. (not 1-to-1 with operation counts.) Queued Operations Operations queued due to a lock. Query Efficiency Ratio of Documents returned or Index entries scanned / full documents scanned Scanned and Moved Objects This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. getLastError Write Time Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern. getLastError Write Operations Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern. Assert Events This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high. Page Faults Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB Instance Summary"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#command-operations","text":"Ops or Replicated Ops/sec classified by legacy wire protocol type ( query , insert , update , delete , getmore ). And (from the internal TTL threads) the docs deletes/sec by TTL indexes.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#latency-detail","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency Detail"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#connections","text":"TCP connections (Incoming)","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#cursors","text":"Open cursors. Includes idle cursors.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#document-operations","text":"Docs per second inserted, updated, deleted or returned. (not 1-to-1 with operation counts.)","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#query-efficiency","text":"Ratio of Documents returned or Index entries scanned / full documents scanned","title":"Query Efficiency"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-time","text":"Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern.","title":"getLastError Write Time"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-operations","text":"Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern.","title":"getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#assert-events","text":"This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high.","title":"Assert Events"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html","text":"MongoDB Instances Compare Connections No description Cursors No description Latency Average latency of operations (classified by read, write, or (other) command) Scan Ratios Ratio of index entries scanned or whole docs scanned / number of documents returned Index Filtering Effectiveness No description Requests Ops/sec (classified by (legacy) wire protocol request type) Document Operations Documents inserted/updated/deleted or returned per sec Queued Operations The number of operations that are currently queued and waiting for a lock Used Memory No description","title":"MongoDB Instances Compare"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#connections","text":"No description","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#cursors","text":"No description","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#latency","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#scan-ratios","text":"Ratio of index entries scanned or whole docs scanned / number of documents returned","title":"Scan Ratios"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#index-filtering-effectiveness","text":"No description","title":"Index Filtering Effectiveness"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#requests","text":"Ops/sec (classified by (legacy) wire protocol request type)","title":"Requests"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#document-operations","text":"Documents inserted/updated/deleted or returned per sec","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#queued-operations","text":"The number of operations that are currently queued and waiting for a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#used-memory","text":"No description","title":"Used Memory"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html","text":"MongoDB Instances Overview This dashboard provides basic information about MongoDB instances. Command Operations Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs. Connections Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly. Cursors Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection. Document Operations When used in combination with Command Operations , this graph can help identify write amplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents. Queued Operations Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue. getLastError Write Time, getLastError Write Operations This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring. Asserts Asserts are not important by themselves, but you can correlate spikes with other graphs. Memory Faults Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"MongoDB Instances Overview"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#command-operations","text":"Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#connections","text":"Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly.","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#cursors","text":"Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#document-operations","text":"When used in combination with Command Operations , this graph can help identify write amplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents.","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#queued-operations","text":"Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#getlasterror-write-time-getlasterror-write-operations","text":"This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring.","title":"getLastError Write Time, getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#asserts","text":"Asserts are not important by themselves, but you can correlate spikes with other graphs.","title":"Asserts"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#memory-faults","text":"Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"Memory Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html","text":"MongoDB MMAPv1 Details Document Activity Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes. MMAPv1 Lock Wait Time Time spent per second waiting to acquire locks. MMAPv1 Page Faults Unix or Window memory page faults. Not necessarily from MongoDB. MMAPv1 Journal Write Activity MB processed through the journal in memory. MMAPv1 Journal Commit Activity MB committed to disk for the journal. MMAPv1 Background Flushing Time Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken. Queued Operations Queue size of ops waiting to be submitted to storage engine layer. (see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.) Client Operations Ops and Replicated Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ). Scanned and Moved Objects This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"MongoDB MMAPv1 Details"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#document-activity","text":"Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes.","title":"Document Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-lock-wait-time","text":"Time spent per second waiting to acquire locks.","title":"MMAPv1 Lock Wait Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MMAPv1 Page Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-write-activity","text":"MB processed through the journal in memory.","title":"MMAPv1 Journal Write Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-commit-activity","text":"MB committed to disk for the journal.","title":"MMAPv1 Journal Commit Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-background-flushing-time","text":"Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken.","title":"MMAPv1 Background Flushing Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#queued-operations","text":"Queue size of ops waiting to be submitted to storage engine layer. (see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.)","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#client-operations","text":"Ops and Replicated Ops/sec, classified by legacy wire protocol type ( query , insert , update , delete , getmore ).","title":"Client Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html","text":"MongoDB ReplSet Summary Replication Lag MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue. Operations - by service name Operations are classified by legacy wire protocol type (insert, update, and delete only). Max Member Ping Time - by service name This metric can show a correlation with the replication lag value. Max Heartbeat Time Time span between now and last heartbeat from replicaset members. Elections Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events. Oplog Recovery Window - by service name Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"MongoDB ReplSet Summary"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#replication-lag","text":"MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue.","title":"Replication Lag"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#operations-by-service-name","text":"Operations are classified by legacy wire protocol type (insert, update, and delete only).","title":"Operations - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-member-ping-time-by-service-name","text":"This metric can show a correlation with the replication lag value.","title":"Max Member Ping Time - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-heartbeat-time","text":"Time span between now and last heartbeat from replicaset members.","title":"Max Heartbeat Time"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#elections","text":"Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events.","title":"Elections"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#oplog-recovery-window-by-service-name","text":"Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"Oplog Recovery Window - by service name"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html","text":"MongoDB WiredTiger Details WiredTiger Transactions WiredTiger internal transactions WiredTiger Cache Activity Data volume transferred per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS file buffer cache already in RAM, but disk if not. WiredTiger Block Activity Data volume handled by the WT block manager per second WiredTiger Sessions Internal WT storage engine cursors and sessions currently open WiredTiger Concurrency Tickets Available A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hard-coded high value - \u201cOut\u201d. Queued Operations Operations queued due to a lock. WiredTiger Checkpoint Time The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value. WiredTiger Cache Eviction Least-recently used pages being evicted due to WT cache becoming full. WiredTiger Cache Capacity Configured max and current size of the WT cache. WiredTiger Cache Pages WiredTiger Log Operations WT internal write-ahead log operations. WiredTiger Log Activity Data volume moved per second in WT internal write-ahead log. WiredTiger Log Records Number of records appended per second in WT internal log. Document Changes Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. Scanned and Moved Objects This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults Unix or Window memory page faults. Not necessarily from MongoDB.","title":"MongoDB WiredTiger Details"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-transactions","text":"WiredTiger internal transactions","title":"WiredTiger Transactions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-activity","text":"Data volume transferred per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS file buffer cache already in RAM, but disk if not.","title":"WiredTiger Cache Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-block-activity","text":"Data volume handled by the WT block manager per second","title":"WiredTiger Block Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-sessions","text":"Internal WT storage engine cursors and sessions currently open","title":"WiredTiger Sessions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-concurrency-tickets-available","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hard-coded high value - \u201cOut\u201d.","title":"WiredTiger Concurrency Tickets Available"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-checkpoint-time","text":"The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value.","title":"WiredTiger Checkpoint Time"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-eviction","text":"Least-recently used pages being evicted due to WT cache becoming full.","title":"WiredTiger Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-capacity","text":"Configured max and current size of the WT cache.","title":"WiredTiger Cache Capacity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-pages","text":"","title":"WiredTiger Cache Pages"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-operations","text":"WT internal write-ahead log operations.","title":"WiredTiger Log Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-activity","text":"Data volume moved per second in WT internal write-ahead log.","title":"WiredTiger Log Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-records","text":"Number of records appended per second in WT internal log.","title":"WiredTiger Log Records"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data ( scanned_objects ) and index ( scanned )) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from MongoDB.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html","text":"MySQL Amazon Aurora Details Amazon Aurora Transaction Commits This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations Amazon Aurora Load This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query. Aurora Memory Used This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes. Amazon Aurora Statement Latency This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries Amazon Aurora Special Command Counters Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular unit_test calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system. Amazon Aurora Problems This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"MySQL Dashboards"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-transaction-commits","text":"This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations","title":"Amazon Aurora Transaction Commits"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-load","text":"This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query.","title":"Amazon Aurora Load"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#aurora-memory-used","text":"This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes.","title":"Aurora Memory Used"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-statement-latency","text":"This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries","title":"Amazon Aurora Statement Latency"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-special-command-counters","text":"Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular unit_test calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system.","title":"Amazon Aurora Special Command Counters"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-problems","text":"This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"Amazon Aurora Problems"},{"location":"details/dashboards/dashboard-mysql-command-handler-counters-compare.html","text":"MySQL Command/Handler Counters Compare This dashboard shows server status variables. On this dashboard, you may select multiple servers and compare their counters simultaneously. Server status variables appear in two sections: Commands and Handlers . Choose one or more variables in the Command and Handler fields in the top menu to select the variables which will appear in the COMMANDS or HANDLERS section for each host. Your comparison may include from one up to three hosts. By default or if no item is selected in the menu, PMM displays each command or handler respectively.","title":"MySQL Command/Handler Counters Compare"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html","text":"MySQL Group Replication Summary Overview PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time Transactions Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue Conflicts Detected Conflicts","title":"MySQL Group Replication Summary"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#overview","text":"PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time","title":"Overview"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#transactions","text":"Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue","title":"Transactions"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#conflicts","text":"Detected Conflicts","title":"Conflicts"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html","text":"MySQL InnoDB Compression Details This dashboard helps you analyze the efficiency of InnoDB compression. Compression level and failure rate threshold InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data. Statistic of compression operations Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist. CPU Core Usage CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations. Buffer Pool Total Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"MySQL InnoDB Compression Details"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#compression-level-and-failure-rate-threshold","text":"InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data.","title":"Compression level and failure rate threshold"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#statistic-of-compression-operations","text":"Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist.","title":"Statistic of compression operations"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#cpu-core-usage","text":"CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations.","title":"CPU Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#buffer-pool-total","text":"Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"Buffer Pool Total"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html","text":"MySQL InnoDB Details InnoDB Activity Writes (Rows) Writes (Transactions) Row Writes per Trx Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rows Read Per Trx Log Space per Trx Rollbacks Percent of Transaction Rollbacks (as portion of read-write transactions). BP Reqs Per Row Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page. Log Fsync Per Trx Log Fsync Per Transaction. InnoDB Row Reads InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Row Writes InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Read-Only Transactions InnoDB Read-Write Transactions InnoDB Transactions Information (RW) The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Misc InnoDB Transactions Information Additional InnoDB Transaction Information InnoDB Storage Summary InnoDB Tables Current Number of InnoDB Tables in database Data Buffer Pool Fit Buffer Pool Size as Portion of the Data Avg Row Size Amount of Data Per Row Index Size Per Row Index Size Per Row shows how much space we\u2019re using for indexes on per row basics InnoDB Data Summary Space Allocated Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance. Space Used Space used in All InnoDB Tables. Reported Allocated Space Less Free Space. Data Length Space Used by Data (Including Primary Key). Index Length Space Used by Secondary Indexes. Estimated Rows Estimated number of Rows in InnoDB Storage Engine. It is not exact value and it can change abruptly as information is updated. Indexing Overhead How Much Indexes Take Compared to Data. Free Space Percent How Much Space is Free. Too high value wastes space on disk. Free Allocated Space not currently used by Data or Indexes. InnoDB File Per Table If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace. InnoDB Disk IO InnoDB Page Size Avg Data Read Rq Size Avg Data Write Rq Size Avg Log Write Rq Size Data Written Per Fsync Log Written Per Fsync Data Read Per Row Read Data Written Per Row Written Note: Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals. InnoDB Data I/O InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB Data Bandwidth InnoDB Log IO InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB FSyncs InnoDB Pending IO InnoDB Pending Fsyncs InnoDB Auto Extend Increment When Growing InnoDB System Tablespace extend it by this size at the time. InnoDB Double Write Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems. InnoDB Fast Shutdown Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations. InnoDB Open Files Maximum Number of Files InnoDB is Allowed to use. InnoDB File Use Portion of Allowed InnoDB Open Files Use. InnoDB IO Objects InnoDB IO Targets Write Load Write Load Includes both Write and fsync (referred as misc). InnoDB Buffer Pool Buffer Pool Size InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. Buffer Pool Size of Total RAM InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. NUMA Interleave Interleave Buffer Pool between NUMA zones to better support NUMA systems. Buffer Pool Activity Combined value of Buffer Pool Read and Write requests. BP Data Percent of Buffer Pool Occupied by Cached Data. BP Data Dirty Percent of Data which is Dirty. BP Miss Ratio How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance. BP Write Buffering Number of Logical Writes to Buffer Pool Per logical Write. InnoDB Buffer Pool LRU Sub-Chain Churn Buffer Pool Chunk Size Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize. Buffer Pool Instances Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead. Read Ahead IO Percent Percent of Reads Caused by InnoDB Read Ahead. Read Ahead Wasted Percent of Pages Fetched by Read Ahead Evicted Without Access. Dump Buffer Pool on Shutdown Load Buffer Pool at Startup Portion of Buffer Pool To Dump/Load Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time. Include Buffer Pool in Core Dump Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled. InnoDB Old Blocks Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time. InnoDB Old Blocks Time The Time which has to pass between multiple touches for the block for it to qualify as old block. InnoDB Random Read Ahead Is InnoDB Random ReadAhead Enabled. InnoDB Random Read Ahead The Threshold (in Pages) to trigger Linear Read Ahead. InnoDB Read IO Threads Number of Threads used to Schedule Reads. InnoDB Write IO Threads Number of Threads used to Schedule Writes. InnoDB Native AIO Enabled Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance. InnoDB Buffer Pool - Replacement Management LRU Scan Depth InnoDB LRU Scan Depth This variable defines InnoDB Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed. LRU Clean Page Searches When Page is being read (or created) the Page need to be allocated in Buffer Pool. Free List Miss Rate The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed. LRU Get Free Loops If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page. LRU Scans If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient. Pages Scanned in LRU Scans Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted. Pages scanned per LRU Scan Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries. LRU Get Free Waits If InnoDB could not find a free page in LRU list and had to sleep. Should be zero. InnoDB Checkpointing and Flushing Pages Flushed from Flush List Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush. Page Flush Batches Executed InnoDB Flush Cycle typically Runs on 1 second intervals. If too far off from this number it can indicate an issue. Pages Flushed Per Batch How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen. Neighbor Flushing Enabled Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it. InnoDB Checkpoint Age InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed synchronously. The rules of the thumb is to keep one hour of traffic in those logs and let the check-pointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. Pages Flushed (Adaptive) Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit. Adaptive Flush Batches Executed Pages Per Batch (Adaptive) Pages Flushed Per Adaptive Batch. Neighbor Flushing To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting. Pages Flushed (LRU) Flushing from the tail of the LRU list is needed to keep readily-available free pages for new data to be read when data does not fit in the buffer pool. LRU Flush Batches Executed Pages Per Batch (LRU) Pages Flushed Per Neighbor. LSN Age Flush Batch Target Target for Pages to Flush due to LSN Age. Pages Flushed (Neighbor) Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined. Neighbor Flush Batches Executed Pages Per Batch (Neighbor) Pages Flushed Per Neighbor. Sync Flush Waits If InnoDB could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen. Pages Flushed (Background) Pages Flushed by Background Flush which is activated when server is considered to be idle. Background Flush Batches Executed Pages Per Batch (Background) Pages Flushed Per Background Batch. Redo Generation Rate Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. InnoDB Flushing by Type Pages Evicted (LRU) This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer. Page Eviction Batches Pages Evicted per Batch Max Log Space Used Single Page Flushes Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads. Single Page Flush Pages Scanned Pages Scanned Per Single Page Flush InnoDB IO Capacity Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity. InnoDB IO Capacity Max InnoDB IO Capacity to use when falling behind and need to catch up with Flushing. InnoDB Logging Total Log Space Number of InnoDB Log Files Multiplied by Their Size. Log Buffer Size InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files. At Transaction Commit What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only. Flush Transaction Log Every Every Specified Number of Seconds Flush Transaction Log. InnoDB Write Ahead Block Size This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient. Log Write Amplification How much Writes to Log Are Amplified compared to how much Redo is Generated. Log Fsync Rate Redo Generated per Trx Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size. InnoDB Log File Usage Hourly InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size . Log Padding Written Amount of Log Padding Written. InnoDB Log File Size InnoDB Log Files Number of InnoDB Redo Log Files. Log Bandwidth Redo Generation Rate Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. InnoDB Group Commit Batch Size The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size. InnoDB Locking Lock Wait Timeout InnoDB Lock Wait Timeout How long to wait for row lock before timing out. InnoDB Deadlock Detection If Disabled InnoDB Will not detect deadlocks but rely on timeouts. InnoDB Auto Increment Lock Mode Will Define How much locking will come from working with Auto Increment Columns. Rollback on Timeout Whenever to rollback all transaction on timeout or just last statement. Row Lock Blocking Percent of Active Sections which are blocked due to waiting on InnoDB Row Locks. Row Writes per Trx Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rollbacks Percent of Transaction Rollbacks (as portion of read-write transactions). InnoDB Row Lock Wait Activity InnoDB Row Lock Wait Time InnoDB Row Lock Wait Load Average Number of Sessions blocked from proceeding due to waiting on row level lock. InnoDB Row Locks Activity InnoDB Table Lock Activity Current Locks InnoDB Undo Space and Purging Undo Tablespaces Max Undo Log Size InnoDB Undo Log Truncate Purge Threads Max Purge Lag Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements. Max Purge Lag Delay Current Purge Delay The Delay Injected due to Purge Thread(s) unable to keep up with purge progress. Rollback Segments InnoDB Purge Activity The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Transactions and Undo Records InnoDB Undo Space Usage The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Transaction History InnoDB Purge Throttling Records Per Undo Log Page How Many Undo Operations Are Handled Per Each Undo Log Page. Purge Invoked How Frequently Purge Operation is Invoked. Ops Per Purge Home Many Purge Actions are done Per invocation. Undo Slots Used Number of Undo Slots Used. Max Transaction History Length Purge Batch Size Rseg Truncate Frequency InnoDB Page Operations InnoDB Page Splits and Merges The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+). Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Page Merge Success Ratio InnoDB Page Reorg Attempts The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Page Reorgs Failures The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Fill Factor The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index. InnoDB Adaptive Hash Index Adaptive Hash Index Enabled Adaptive Hash Index helps to optimize index Look-ups but can be severe hotspot for some workloads. Adaptive Hash Index Partitions How many Partitions Used for Adaptive Hash Index (to reduce contention). Percent of Pages Hashed Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool. AHI Miss Ratio Percent of Searches which could not be resolved through AHI. Rows Added Per Page Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI. AHI ROI How Many Successful Searches using AHI are performed per each row maintenance operation. InnoDB AHI Usage The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB AHI Miss Ratio InnoDB AHI Churn - Rows InnoDB AHI Churn - Pages InnoDB Change Buffer Change Buffer Max Size The Maximum Size of Change Buffer (as Percent of Buffer Pool Size). Change Buffer Max Size The Maximum Size of Change Buffer (Bytes). InnoDB Change Buffer Merge Load Number of Average of Active Merge Buffer Operations in Process. InnoDB Contention InnoDB Thread Concurrency If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time. InnoDB Commit Concurrency If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage. InnoDB Thread Sleep Delay The Time the thread will Sleep before Re-Entering InnoDB Kernel if high contention. InnoDB Adaptive Max Sleep Delay If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable. InnoDB Concurrency Tickets Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting. InnoDB Spin Wait Delay InnoDB Spin Wait Pause Multiplier InnoDB Sync Spin Loops InnoDB Contention - OS Waits The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Contention - Spin Rounds The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed to get a lock. A spin round is a fast retry to get the lock in a loop. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Misc InnoDB Main Thread Utilization The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Activity The InnoDB Activity graph shows a measure of the activity of the InnoDB threads. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Dedicated Server InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables). InnoDB Sort Buffer Size This Buffer is used for Building InnoDB Indexes using Sort algorithm. InnoDB Stats Auto Recalc Update Stats when Metadata Queried Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues. Index Condition Pushdown (ICP) Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine. InnoDB Persistent Statistics InnoDB Persistent Sample Pages Number of Pages To Sample if Persistent Statistics are Enabled. InnoDB Transient Sample Pages Number of Pages To Sample if Persistent Statistics are Disabled. InnoDB Online Operations (MariaDB) InnoDB Defragmentation The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Note: Currently available only on a MariaDB server. InnoDB Online DDL The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Note: Currently available only on a MariaDB server. MySQL Summary MySQL Uptime MySQL Uptime The amount of time since the last restart of the MySQL server process. Current QPS Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands. File Handlers Used Table Open Cache Miss Ratio Table Open Cache Size Table Definition Cache Size MySQL Connections Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Client Thread Activity MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Handlers MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. Top Command Counters Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Network Traffic MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. Node Summary System Uptime The parameter shows how long a system has been up and running without a shut down or restart. Load Average The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load. RAM RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor. Memory Available Percent of Memory Available Note: on Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers. Virtual Memory RAM + SWAP Disk Space Sum of disk space on all partitions. Note it can be significantly over-reported in some installations. Min Space Available Lowest percent of the disk space available. CPU Usage The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Disk I/O and Swap Activity Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage. Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time.","title":"MySQL InnoDB Details"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity","text":"","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-rows","text":"","title":"Writes (Rows)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-transactions","text":"","title":"Writes (Transactions)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-read-per-trx","text":"","title":"Rows Read Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-space-per-trx","text":"","title":"Log Space per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-reqs-per-row","text":"Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page.","title":"BP Reqs Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-per-trx","text":"Log Fsync Per Transaction.","title":"Log Fsync Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-reads","text":"InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Reads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-writes","text":"InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Writes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-only-transactions","text":"","title":"InnoDB Read-Only Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-write-transactions","text":"","title":"InnoDB Read-Write Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transactions-information-rw","text":"The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Transactions Information (RW)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#misc-innodb-transactions-information","text":"Additional InnoDB Transaction Information","title":"Misc InnoDB Transactions Information"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-storage-summary","text":"","title":"InnoDB Storage Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-tables","text":"Current Number of InnoDB Tables in database","title":"InnoDB Tables"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-buffer-pool-fit","text":"Buffer Pool Size as Portion of the Data","title":"Data Buffer Pool Fit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-row-size","text":"Amount of Data Per Row","title":"Avg Row Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-size-per-row","text":"Index Size Per Row shows how much space we\u2019re using for indexes on per row basics","title":"Index Size Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-summary","text":"","title":"InnoDB Data Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-allocated","text":"Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance.","title":"Space Allocated"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-used","text":"Space used in All InnoDB Tables. Reported Allocated Space Less Free Space.","title":"Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-length","text":"Space Used by Data (Including Primary Key).","title":"Data Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-length","text":"Space Used by Secondary Indexes.","title":"Index Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#estimated-rows","text":"Estimated number of Rows in InnoDB Storage Engine. It is not exact value and it can change abruptly as information is updated.","title":"Estimated Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#indexing-overhead","text":"How Much Indexes Take Compared to Data.","title":"Indexing Overhead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-space-percent","text":"How Much Space is Free. Too high value wastes space on disk.","title":"Free Space Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free","text":"Allocated Space not currently used by Data or Indexes.","title":"Free"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-per-table","text":"If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace.","title":"InnoDB File Per Table"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-disk-io","text":"","title":"InnoDB Disk IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-size","text":"","title":"InnoDB Page Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-read-rq-size","text":"","title":"Avg Data Read Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-write-rq-size","text":"","title":"Avg Data Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-log-write-rq-size","text":"","title":"Avg Log Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-fsync","text":"","title":"Data Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-written-per-fsync","text":"","title":"Log Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-read-per-row-read","text":"","title":"Data Read Per Row Read"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-row-written","text":"Note: Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals.","title":"Data Written Per Row Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Data I/O"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-bandwidth","text":"","title":"InnoDB Data Bandwidth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Log IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fsyncs","text":"","title":"InnoDB FSyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-io","text":"","title":"InnoDB Pending IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-fsyncs","text":"","title":"InnoDB Pending Fsyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-extend-increment","text":"When Growing InnoDB System Tablespace extend it by this size at the time.","title":"InnoDB Auto Extend Increment"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-double-write","text":"Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems.","title":"InnoDB Double Write"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fast-shutdown","text":"Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations.","title":"InnoDB Fast Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-open-files","text":"Maximum Number of Files InnoDB is Allowed to use.","title":"InnoDB Open Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-use","text":"Portion of Allowed InnoDB Open Files Use.","title":"InnoDB File Use"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-objects","text":"","title":"InnoDB IO Objects"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-targets-write-load","text":"Write Load Includes both Write and fsync (referred as misc).","title":"InnoDB IO Targets Write Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool","text":"","title":"InnoDB Buffer Pool"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size","text":"InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size-of-total-ram","text":"InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size of Total RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#numa-interleave","text":"Interleave Buffer Pool between NUMA zones to better support NUMA systems.","title":"NUMA Interleave"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-activity","text":"Combined value of Buffer Pool Read and Write requests.","title":"Buffer Pool Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data","text":"Percent of Buffer Pool Occupied by Cached Data.","title":"BP Data"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data-dirty","text":"Percent of Data which is Dirty.","title":"BP Data Dirty"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-miss-ratio","text":"How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance.","title":"BP Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-write-buffering","text":"Number of Logical Writes to Buffer Pool Per logical Write.","title":"BP Write Buffering"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-lru-sub-chain-churn","text":"","title":"InnoDB Buffer Pool LRU Sub-Chain Churn"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-chunk-size","text":"Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize.","title":"Buffer Pool Chunk Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-instances","text":"Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead.","title":"Buffer Pool Instances"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-io-percent","text":"Percent of Reads Caused by InnoDB Read Ahead.","title":"Read Ahead IO Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-wasted","text":"Percent of Pages Fetched by Read Ahead Evicted Without Access.","title":"Read Ahead Wasted"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#dump-buffer-pool-on-shutdown","text":"","title":"Dump Buffer Pool on Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-buffer-pool-at-startup","text":"","title":"Load Buffer Pool at Startup"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#portion-of-buffer-pool-to-dumpload","text":"Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time.","title":"Portion of Buffer Pool To Dump/Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#include-buffer-pool-in-core-dump","text":"Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled.","title":"Include Buffer Pool in Core Dump"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks","text":"Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time.","title":"InnoDB Old Blocks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks-time","text":"The Time which has to pass between multiple touches for the block for it to qualify as old block.","title":"InnoDB Old Blocks Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead","text":"Is InnoDB Random ReadAhead Enabled.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead_1","text":"The Threshold (in Pages) to trigger Linear Read Ahead.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-io-threads","text":"Number of Threads used to Schedule Reads.","title":"InnoDB Read IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-io-threads","text":"Number of Threads used to Schedule Writes.","title":"InnoDB Write IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-native-aio-enabled","text":"Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance.","title":"InnoDB Native AIO Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-replacement-management","text":"","title":"InnoDB Buffer Pool - Replacement Management"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scan-depth","text":"InnoDB LRU Scan Depth This variable defines InnoDB Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed.","title":"LRU Scan Depth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-clean-page-searches","text":"When Page is being read (or created) the Page need to be allocated in Buffer Pool.","title":"LRU Clean Page Searches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-list-miss-rate","text":"The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed.","title":"Free List Miss Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-loops","text":"If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page.","title":"LRU Get Free Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scans","text":"If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient.","title":"LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-in-lru-scans","text":"Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted.","title":"Pages Scanned in LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-lru-scan","text":"Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries.","title":"Pages scanned per LRU Scan"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-waits","text":"If InnoDB could not find a free page in LRU list and had to sleep. Should be zero.","title":"LRU Get Free Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpointing-and-flushing","text":"","title":"InnoDB Checkpointing and Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-from-flush-list","text":"Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush.","title":"Pages Flushed from Flush List"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-flush-batches-executed","text":"InnoDB Flush Cycle typically Runs on 1 second intervals. If too far off from this number it can indicate an issue.","title":"Page Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-per-batch","text":"How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen.","title":"Pages Flushed Per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing-enabled","text":"Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it.","title":"Neighbor Flushing Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpoint-age","text":"InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed synchronously. The rules of the thumb is to keep one hour of traffic in those logs and let the check-pointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest.","title":"InnoDB Checkpoint Age"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-adaptive","text":"Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit.","title":"Pages Flushed (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-flush-batches-executed","text":"","title":"Adaptive Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-adaptive","text":"Pages Flushed Per Adaptive Batch.","title":"Pages Per Batch (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing","text":"To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting.","title":"Neighbor Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-lru","text":"Flushing from the tail of the LRU list is needed to keep readily-available free pages for new data to be read when data does not fit in the buffer pool.","title":"Pages Flushed (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-flush-batches-executed","text":"","title":"LRU Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-lru","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lsn-age-flush-batch-target","text":"Target for Pages to Flush due to LSN Age.","title":"LSN Age Flush Batch Target"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-neighbor","text":"Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined.","title":"Pages Flushed (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flush-batches-executed","text":"","title":"Neighbor Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-neighbor","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#sync-flush-waits","text":"If InnoDB could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen.","title":"Sync Flush Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-background","text":"Pages Flushed by Background Flush which is activated when server is considered to be idle.","title":"Pages Flushed (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#background-flush-batches-executed","text":"","title":"Background Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-background","text":"Pages Flushed Per Background Batch.","title":"Pages Per Batch (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-flushing-by-type","text":"","title":"InnoDB Flushing by Type"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-lru","text":"This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer.","title":"Pages Evicted (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-eviction-batches","text":"","title":"Page Eviction Batches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-per-batch","text":"","title":"Pages Evicted per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-log-space-used","text":"","title":"Max Log Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flushes","text":"Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads.","title":"Single Page Flushes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flush-pages-scanned","text":"","title":"Single Page Flush Pages Scanned"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-single-page-flush","text":"","title":"Pages Scanned Per Single Page Flush"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity","text":"Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity.","title":"InnoDB IO Capacity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity-max","text":"InnoDB IO Capacity to use when falling behind and need to catch up with Flushing.","title":"InnoDB IO Capacity Max"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-logging","text":"","title":"InnoDB Logging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#total-log-space","text":"Number of InnoDB Log Files Multiplied by Their Size.","title":"Total Log Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-buffer-size","text":"InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files.","title":"Log Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#at-transaction-commit","text":"What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only.","title":"At Transaction Commit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#flush-transaction-log-every","text":"Every Specified Number of Seconds Flush Transaction Log.","title":"Flush Transaction Log Every"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-ahead-block-size","text":"This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient.","title":"InnoDB Write Ahead Block Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-write-amplification","text":"How much Writes to Log Are Amplified compared to how much Redo is Generated.","title":"Log Write Amplification"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-rate","text":"","title":"Log Fsync Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generated-per-trx","text":"Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size.","title":"Redo Generated per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-usage-hourly","text":"InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, i.e., when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size .","title":"InnoDB Log File Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-padding-written","text":"Amount of Log Padding Written.","title":"Log Padding Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-size","text":"","title":"InnoDB Log File Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-files","text":"Number of InnoDB Redo Log Files.","title":"InnoDB Log Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-bandwidth","text":"","title":"Log Bandwidth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate_1","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-group-commit-batch-size","text":"The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size.","title":"InnoDB Group Commit Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-locking","text":"","title":"InnoDB Locking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lock-wait-timeout","text":"InnoDB Lock Wait Timeout How long to wait for row lock before timing out.","title":"Lock Wait Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-deadlock-detection","text":"If Disabled InnoDB Will not detect deadlocks but rely on timeouts.","title":"InnoDB Deadlock Detection"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-increment-lock-mode","text":"Will Define How much locking will come from working with Auto Increment Columns.","title":"InnoDB Auto Increment Lock Mode"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-on-timeout","text":"Whenever to rollback all transaction on timeout or just last statement.","title":"Rollback on Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-lock-blocking","text":"Percent of Active Sections which are blocked due to waiting on InnoDB Row Locks.","title":"Row Lock Blocking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx_1","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks_1","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-activity","text":"","title":"InnoDB Row Lock Wait Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-time","text":"","title":"InnoDB Row Lock Wait Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-load","text":"Average Number of Sessions blocked from proceeding due to waiting on row level lock.","title":"InnoDB Row Lock Wait Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-locks-activity","text":"","title":"InnoDB Row Locks Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-table-lock-activity","text":"","title":"InnoDB Table Lock Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-locks","text":"","title":"Current Locks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-and-purging","text":"","title":"InnoDB Undo Space and Purging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-tablespaces","text":"","title":"Undo Tablespaces"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-undo-log-size","text":"","title":"Max Undo Log Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-log-truncate","text":"","title":"InnoDB Undo Log Truncate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-threads","text":"","title":"Purge Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag","text":"Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements.","title":"Max Purge Lag"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag-delay","text":"","title":"Max Purge Lag Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-purge-delay","text":"The Delay Injected due to Purge Thread(s) unable to keep up with purge progress.","title":"Current Purge Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-segments","text":"","title":"Rollback Segments"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-activity","text":"The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Purge Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transactions-and-undo-records","text":"","title":"Transactions and Undo Records"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-usage","text":"The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Undo Space Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transaction-history","text":"","title":"Transaction History"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-throttling","text":"","title":"InnoDB Purge Throttling"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#records-per-undo-log-page","text":"How Many Undo Operations Are Handled Per Each Undo Log Page.","title":"Records Per Undo Log Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-invoked","text":"How Frequently Purge Operation is Invoked.","title":"Purge Invoked"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ops-per-purge","text":"Home Many Purge Actions are done Per invocation.","title":"Ops Per Purge"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-slots-used","text":"Number of Undo Slots Used.","title":"Undo Slots Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-transaction-history-length","text":"","title":"Max Transaction History Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-batch-size","text":"","title":"Purge Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rseg-truncate-frequency","text":"","title":"Rseg Truncate Frequency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-operations","text":"","title":"InnoDB Page Operations"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-splits-and-merges","text":"The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+). Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Splits and Merges"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-merge-success-ratio","text":"","title":"Page Merge Success Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorg-attempts","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Reorg Attempts"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorgs-failures","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Reorgs Failures"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fill-factor","text":"The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index.","title":"InnoDB Fill Factor"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-hash-index","text":"","title":"InnoDB Adaptive Hash Index"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-enabled","text":"Adaptive Hash Index helps to optimize index Look-ups but can be severe hotspot for some workloads.","title":"Adaptive Hash Index Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-partitions","text":"How many Partitions Used for Adaptive Hash Index (to reduce contention).","title":"Adaptive Hash Index Partitions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#percent-of-pages-hashed","text":"Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool.","title":"Percent of Pages Hashed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-miss-ratio","text":"Percent of Searches which could not be resolved through AHI.","title":"AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-added-per-page","text":"Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI.","title":"Rows Added Per Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-roi","text":"How Many Successful Searches using AHI are performed per each row maintenance operation.","title":"AHI ROI"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-usage","text":"The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB AHI Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-miss-ratio","text":"","title":"InnoDB AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-rows","text":"","title":"InnoDB AHI Churn - Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-pages","text":"","title":"InnoDB AHI Churn - Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer","text":"","title":"InnoDB Change Buffer"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size","text":"The Maximum Size of Change Buffer (as Percent of Buffer Pool Size).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size_1","text":"The Maximum Size of Change Buffer (Bytes).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer-merge-load","text":"Number of Average of Active Merge Buffer Operations in Process.","title":"InnoDB Change Buffer Merge Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention","text":"","title":"InnoDB Contention"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-concurrency","text":"If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time.","title":"InnoDB Thread Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-commit-concurrency","text":"If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage.","title":"InnoDB Commit Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-sleep-delay","text":"The Time the thread will Sleep before Re-Entering InnoDB Kernel if high contention.","title":"InnoDB Thread Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-max-sleep-delay","text":"If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable.","title":"InnoDB Adaptive Max Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-concurrency-tickets","text":"Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting.","title":"InnoDB Concurrency Tickets"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-delay","text":"","title":"InnoDB Spin Wait Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-pause-multiplier","text":"","title":"InnoDB Spin Wait Pause Multiplier"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sync-spin-loops","text":"","title":"InnoDB Sync Spin Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-os-waits","text":"The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Contention - OS Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-spin-rounds","text":"The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed to get a lock. A spin round is a fast retry to get the lock in a loop. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Contention - Spin Rounds"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-misc","text":"","title":"InnoDB Misc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-main-thread-utilization","text":"The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Main Thread Utilization"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity_1","text":"The InnoDB Activity graph shows a measure of the activity of the InnoDB threads. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-dedicated-server","text":"InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables).","title":"InnoDB Dedicated Server"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sort-buffer-size","text":"This Buffer is used for Building InnoDB Indexes using Sort algorithm.","title":"InnoDB Sort Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-stats-auto-recalc","text":"","title":"InnoDB Stats Auto Recalc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#update-stats-when-metadata-queried","text":"Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues.","title":"Update Stats when Metadata Queried"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-condition-pushdown-icp","text":"Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine.","title":"Index Condition Pushdown (ICP)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-statistics","text":"","title":"InnoDB Persistent Statistics"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Enabled.","title":"InnoDB Persistent Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transient-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Disabled.","title":"InnoDB Transient Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-operations-mariadb","text":"","title":"InnoDB Online Operations (MariaDB)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-defragmentation","text":"The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Note: Currently available only on a MariaDB server.","title":"InnoDB Defragmentation"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-ddl","text":"The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Note: Currently available only on a MariaDB server.","title":"InnoDB Online DDL"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-summary","text":"","title":"MySQL Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-uptime","text":"MySQL Uptime The amount of time since the last restart of the MySQL server process.","title":"MySQL Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-qps","text":"Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands.","title":"Current QPS"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#file-handlers-used","text":"","title":"File Handlers Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-miss-ratio","text":"","title":"Table Open Cache Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-size","text":"","title":"Table Open Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-definition-cache-size","text":"","title":"Table Definition Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-connections","text":"Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-client-thread-activity","text":"MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-handlers","text":"MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#top-command-counters","text":"Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-network-traffic","text":"MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#node-summary","text":"","title":"Node Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#system-uptime","text":"The parameter shows how long a system has been up and running without a shut down or restart.","title":"System Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-average","text":"The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load.","title":"Load Average"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ram","text":"RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor.","title":"RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#memory-available","text":"Percent of Memory Available Note: on Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers.","title":"Memory Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#virtual-memory","text":"RAM + SWAP","title":"Virtual Memory"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-space","text":"Sum of disk space on all partitions. Note it can be significantly over-reported in some installations.","title":"Disk Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#min-space-available","text":"Lowest percent of the disk space available.","title":"Min Space Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-io-and-swap-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Disk I/O and Swap Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html","text":"MySQL Instance Summary MySQL Connections Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Aborted Connections Aborted Connections When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema ). If the amount of failed requests without a successful connection reaches the value of max_connect_errors , mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement. MySQL Client Thread Activity MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Thread Cache MySQL Thread Cache The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is auto-sized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. threads_created : The number of threads created to handle connections. threads_cached : The number of threads in the thread cache. MySQL Slow Queries MySQL Slow Queries Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph. MySQL Select Types MySQL Select Types As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit. MySQL Sorts MySQL Sorts Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table to return the results and which could not have been sorted via an index. MySQL Table Locks Table Locks MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity. MySQL Questions MySQL Questions The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET MySQL Network Traffic MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. MySQL Network Usage Hourly MySQL Network Usage Hourly Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL. MySQL Internal Memory Overview System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MyISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit. Top Command Counters Top Command Counters The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. Top Command Counters Hourly Top Command Counters Hourly The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Handlers MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. MySQL Query Cache Memory MySQL Query Cache Memory The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Query Cache Activity MySQL Query Cache Activity The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Table Open Cache Status MySQL Table Open Cache Status The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value). MySQL Open Tables MySQL Open Tables The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value). MySQL Table Definition Cache MySQL Table Definition Cache The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Instance Summary"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-connections","text":"Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-aborted-connections","text":"Aborted Connections When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema ). If the amount of failed requests without a successful connection reaches the value of max_connect_errors , mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement.","title":"MySQL Aborted Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-client-thread-activity","text":"MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-thread-cache","text":"MySQL Thread Cache The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is auto-sized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. threads_created : The number of threads created to handle connections. threads_cached : The number of threads in the thread cache.","title":"MySQL Thread Cache"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-slow-queries","text":"MySQL Slow Queries Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph.","title":"MySQL Slow Queries"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-select-types","text":"MySQL Select Types As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit.","title":"MySQL Select Types"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-sorts","text":"MySQL Sorts Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table to return the results and which could not have been sorted via an index.","title":"MySQL Sorts"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-locks","text":"Table Locks MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity.","title":"MySQL Table Locks"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-questions","text":"MySQL Questions The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET","title":"MySQL Questions"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-traffic","text":"MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-usage-hourly","text":"MySQL Network Usage Hourly Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL.","title":"MySQL Network Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-internal-memory-overview","text":"System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MyISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit.","title":"MySQL Internal Memory Overview"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters","text":"Top Command Counters The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters-hourly","text":"Top Command Counters Hourly The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-handlers","text":"MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-memory","text":"MySQL Query Cache Memory The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Memory"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-activity","text":"MySQL Query Cache Activity The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-open-cache-status","text":"MySQL Table Open Cache Status The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Table Open Cache Status"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-open-tables","text":"MySQL Open Tables The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Open Tables"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-definition-cache","text":"MySQL Table Definition Cache The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (i.e., do not set them to any value).","title":"MySQL Table Definition Cache"},{"location":"details/dashboards/dashboard-mysql-instances-compare.html","text":"MySQL Instances Compare No description","title":"MySQL Instances Compare"},{"location":"details/dashboards/dashboard-mysql-instances-overview.html","text":"MySQL Instances Overview No description","title":"MySQL Instances Overview"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html","text":"MySQL MyISAM/Aria Details MyISAM Key Buffer Performance The Key Read Ratio ( Key_reads / Key_read_requests ) ratio should normally be less than 0.01. The Key Write Ratio ( Key_writes / Key_write_requests ) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option. Aria Pagecache Reads/Writes This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.) Aria Transaction Log Syncs This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well. Aria Pagecache Blocks This graph shows the utilization for the Aria pagecache. This is similar to InnoDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"MySQL MyISAM/Aria Details"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#myisam-key-buffer-performance","text":"The Key Read Ratio ( Key_reads / Key_read_requests ) ratio should normally be less than 0.01. The Key Write Ratio ( Key_writes / Key_write_requests ) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option.","title":"MyISAM Key Buffer Performance"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-readswrites","text":"This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Reads/Writes"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-transaction-log-syncs","text":"This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well.","title":"Aria Transaction Log Syncs"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-blocks","text":"This graph shows the utilization for the Aria pagecache. This is similar to InnoDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Blocks"},{"location":"details/dashboards/dashboard-mysql-myrocks-details.html","text":"MySQL MyRocks Details The MyRocks storage engine developed by Facebook based on the RocksDB storage engine is applicable to systems which primarily interact with the database by writing data to it rather than reading from it. RocksDB also features a good level of compression, higher than that of the InnoDB storage engine, which makes it especially valuable when optimizing the usage of hard drives. PMM collects statistics on the MyRocks storage engine for MySQL in the Metrics Monitor information for this dashboard comes from the Information Schema tables. Metrics MyRocks cache MyRocks cache data bytes R/W MyRocks cache index hit rate MyRocks cache index MyRocks cache filter hit rate MyRocks cache filter MyRocks cache data bytes inserted MyRocks bloom filter MyRocks memtable MyRocks memtable size MyRocks number of keys MyRocks cache L0/L1 MyRocks number of DB ops MyRocks R/W MyRocks bytes read by iterations MyRocks write ops MyRocks WAL MyRocks number reseeks in iterations RocksDB row operations MyRocks file operations RocksDB stalls RocksDB stops/slowdowns","title":"MySQL MyRocks Details"},{"location":"details/dashboards/dashboard-mysql-performance-schema-details.html","text":"MySQL Performance Schema Details The MySQL Performance Schema dashboard helps determine the efficiency of communicating with Performance Schema. This dashboard contains the following metrics: Performance Schema file IO (events) Performance Schema file IO (load) Performance Schema file IO (Bytes) Performance Schema waits (events) Performance Schema waits (load) Index access operations (load) Table access operations (load) Performance Schema SQL and external locks (events) Performance Schema SQL and external locks (seconds)","title":"MySQL Performance Schema Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html","text":"MySQL Query Response Time Details Average Query Response Time The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA.QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries. Query Response Time Distribution Query response time counts (operations) are grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s Average Query Response Time Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time. Read Query Response Time Distribution Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s Write Query Response Time Distribution Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"MySQL Query Response Time Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time","text":"The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA.QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#query-response-time-distribution","text":"Query response time counts (operations) are grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time_1","text":"Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#read-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Read Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#write-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100 ms - 1 s 1 s - 10 s > 10 s","title":"Write Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html","text":"MySQL Replication Summary IO Thread Running This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host. Possible values Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns. SQL Thread Running This metric shows if the SQL thread is running or not. It only applies to a secondary host. Possible values Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host Replication Error No This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication to avoid data corruption. Read only This metric indicates whether the host is configured to be in Read Only mode or not. Possible values Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode. MySQL Replication Delay This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asynchronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point. Binlog Size This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog ) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog file is read by secondaries via their IO Thread process to replicate database changes modification on the data and on the table structures. There can be more than one binlog file depending on the binlog rotation policy (for example using the configuration variables max_binlog_size and expire_logs_days ) or because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data. Binlog Data Written Hourly This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion). Binlog Count This metric shows the overall count of binary log files, on both primary and secondary servers. Binlogs Created Hourly This metric shows the number of binlog files created hourly during the last 24 hours. Relay Log Space This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric. Relay Log Written Hourly This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"MySQL Replication Summary"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#io-thread-running","text":"This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host. Possible values Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns.","title":"IO Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#sql-thread-running","text":"This metric shows if the SQL thread is running or not. It only applies to a secondary host. Possible values Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host","title":"SQL Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#replication-error-no","text":"This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication to avoid data corruption.","title":"Replication Error No"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#read-only","text":"This metric indicates whether the host is configured to be in Read Only mode or not. Possible values Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode.","title":"Read only"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#mysql-replication-delay","text":"This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asynchronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point.","title":"MySQL Replication Delay"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-size","text":"This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog ) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog file is read by secondaries via their IO Thread process to replicate database changes modification on the data and on the table structures. There can be more than one binlog file depending on the binlog rotation policy (for example using the configuration variables max_binlog_size and expire_logs_days ) or because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data.","title":"Binlog Size"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-data-written-hourly","text":"This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion).","title":"Binlog Data Written Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-count","text":"This metric shows the overall count of binary log files, on both primary and secondary servers.","title":"Binlog Count"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlogs-created-hourly","text":"This metric shows the number of binlog files created hourly during the last 24 hours.","title":"Binlogs Created Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-space","text":"This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric.","title":"Relay Log Space"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-written-hourly","text":"This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"Relay Log Written Hourly"},{"location":"details/dashboards/dashboard-mysql-table-details.html","text":"MySQL Table Details Largest Tables Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables . Pie Total Database Size The total size of the database: as data + index size, so freeable one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size Table Activity The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on. Rows read The number of rows read from the table, shown for the top 5 tables. Rows Changed The number of rows changed in the table, shown for the top 5 tables. Auto Increment Usage The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"MySQL Table Details"},{"location":"details/dashboards/dashboard-mysql-table-details.html#largest-tables","text":"Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables .","title":"Largest Tables"},{"location":"details/dashboards/dashboard-mysql-table-details.html#pie","text":"Total Database Size The total size of the database: as data + index size, so freeable one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size","title":"Pie"},{"location":"details/dashboards/dashboard-mysql-table-details.html#table-activity","text":"The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on.","title":"Table Activity"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-read","text":"The number of rows read from the table, shown for the top 5 tables.","title":"Rows read"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-changed","text":"The number of rows changed in the table, shown for the top 5 tables.","title":"Rows Changed"},{"location":"details/dashboards/dashboard-mysql-table-details.html#auto-increment-usage","text":"The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"Auto Increment Usage"},{"location":"details/dashboards/dashboard-mysql-tokudb-details.html","text":"MySQL TokuDB Details No description","title":"MySQL TokuDB Details"},{"location":"details/dashboards/dashboard-mysql-user-details.html","text":"MySQL User Details Note This dashboard requires Percona Server for MySQL 5.1+ or MariaDB 10.1/10.2 with XtraDB. Also userstat should be enabled, for example with the SET GLOBAL userstat=1 statement. See Setting up MySQL . Data is displayed for the 5 top users. Top Users by Connections Created The number of times user\u2019s connections connected using SSL to the server. Top Users by Traffic The number of bytes sent to the user\u2019s connections. Top Users by Rows Fetched/Read The number of rows fetched by the user\u2019s connections. Top Users by Rows Updated The number of rows updated by the user\u2019s connections. Top Users by Busy Time The cumulative number of seconds there was activity on connections from the user. Top Users by CPU Time The cumulative CPU time elapsed, in seconds, while servicing connections of the user.","title":"MySQL User Details"},{"location":"details/dashboards/dashboard-mysql-wait-event-analyses-details.html","text":"MySQL Wait Event Analyses Details This dashboard helps to analyze Performance Schema wait events. It plots the following metrics for the chosen (one or more) wait events: Count - Performance Schema Waits Load - Performance Schema Waits Avg Wait Time - Performance Schema Waits","title":"MySQL Wait Event Analyses Details"},{"location":"details/dashboards/dashboard-network-details.html","text":"Network Details Last Hour Statistic This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate . Network Traffic This section contains the Network traffic and network utilization hourly metrics. Network Traffic Details This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicast Network Netstat TCP This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments Network Netstat UDP In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener ICMP This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests ICMP Errors InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received Messages/Redirects InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects Echos InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent Timestamps/Mask Requests InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"Network Details"},{"location":"details/dashboards/dashboard-network-details.html#last-hour-statistic","text":"This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate .","title":"Last Hour Statistic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic","text":"This section contains the Network traffic and network utilization hourly metrics.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic-details","text":"This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicast","title":"Network Traffic Details"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-tcp","text":"This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments","title":"Network Netstat TCP"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-udp","text":"In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener","title":"Network Netstat UDP"},{"location":"details/dashboards/dashboard-network-details.html#icmp","text":"This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests ICMP Errors InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received Messages/Redirects InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects Echos InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent Timestamps/Mask Requests InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"ICMP"},{"location":"details/dashboards/dashboard-node-summary.html","text":"Node Summary System Summary The output from pt-summary , one of the Percona Toolkit utilities . CPU Usage The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Interrupts and Context Switches Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system. Processes No description Memory Utilization No description Virtual Memory Utilization No description Swap Space No description Swap Activity Swap Activity is memory management that involves swapping sections of memory to and from physical storage. I/O Activity Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Global File Descriptors Usage No description Disk IO Latency Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk IO Load Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time. Network Utilization Hourly No description Local Network Errors Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero TCP Retransmission Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"Node Summary"},{"location":"details/dashboards/dashboard-node-summary.html#system-summary","text":"The output from pt-summary , one of the Percona Toolkit utilities .","title":"System Summary"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-node-summary.html#interrupts-and-context-switches","text":"Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system.","title":"Interrupts and Context Switches"},{"location":"details/dashboards/dashboard-node-summary.html#processes","text":"No description","title":"Processes"},{"location":"details/dashboards/dashboard-node-summary.html#memory-utilization","text":"No description","title":"Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#virtual-memory-utilization","text":"No description","title":"Virtual Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#swap-space","text":"No description","title":"Swap Space"},{"location":"details/dashboards/dashboard-node-summary.html#swap-activity","text":"Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Swap Activity"},{"location":"details/dashboards/dashboard-node-summary.html#io-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM.","title":"I/O Activity"},{"location":"details/dashboards/dashboard-node-summary.html#global-file-descriptors-usage","text":"No description","title":"Global File Descriptors Usage"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk IO Latency"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk IO Load"},{"location":"details/dashboards/dashboard-node-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-node-summary.html#network-utilization-hourly","text":"No description","title":"Network Utilization Hourly"},{"location":"details/dashboards/dashboard-node-summary.html#local-network-errors","text":"Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero","title":"Local Network Errors"},{"location":"details/dashboards/dashboard-node-summary.html#tcp-retransmission","text":"Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"TCP Retransmission"},{"location":"details/dashboards/dashboard-node-temperature-details.html","text":"Node Temperature Details The Node Temperature Details dashboard exposes hardware monitoring and sensor data obtained through the sysfs virtual file system of the node. Hardware monitoring devices attached to the CPU and/or other chips on the motherboard let you monitor the hardware health of a system. Most modern systems include several of such devices. The actual list can include temperature sensors, voltage sensors, fan speed sensors, and various additional features, such as the ability to control the rotation speed of the fans. CPU Cores Temperatures Presents data taken from the temperature sensors of the CPU Chips Temperatures Presents data taken from the temperature sensors connected to other system controllers Fan Rotation Speeds Fan rotation speeds reported in RPM (rotations per minute). Fan Power Usage Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Node Temperature Details"},{"location":"details/dashboards/dashboard-node-temperature-details.html#cpu-cores-temperatures","text":"Presents data taken from the temperature sensors of the CPU","title":"CPU Cores Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#chips-temperatures","text":"Presents data taken from the temperature sensors connected to other system controllers","title":"Chips Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-rotation-speeds","text":"Fan rotation speeds reported in RPM (rotations per minute).","title":"Fan Rotation Speeds"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-power-usage","text":"Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Fan Power Usage"},{"location":"details/dashboards/dashboard-nodes-compare.html","text":"Nodes Compare This dashboard lets you compare a wide range of parameters. Parameters of the same type are shown side by side for all servers, grouped into the following sections: System Information CPU Memory Disk Partitions Disk Performance Network The System Information section shows the System Info summary of each server, as well as System Uptime , CPU Cores , RAM , Saturation Metrics , and Load Average gauges. The CPU section offers the CPU Usage , Interrupts , and Context Switches metrics. In the Memory section, you can find the Memory Usage , Swap Usage , and Swap Activity metrics. The Disk Partitions section encapsulates two metrics, Mountpoint Usage and Free Space . The Disk Performance section contains the I/O Activity , Disk Operations , Disk Bandwidth , Disk IO Utilization , Disk Latency , and Disk Load metrics. Finally, Network section shows Network Traffic , and Network Utilization Hourly metrics.","title":"Nodes Compare"},{"location":"details/dashboards/dashboard-nodes-overview.html","text":"Nodes Overview The Nodes Overview dashboard provides details about the efficiency of work of the following components. Each component is represented as a section in the dashboard. CPU Memory & Swap Disk Network The CPU section offers the CPU Usage , CPU Saturation and Max Core Usage , Interrupts and Context Switches , and Processes metrics. In the Memory section, you can find the Memory Utilization , Virtual Memory Utilization , Swap Space , and Swap Activity metrics. The Disk section contains the I/O Activity , Global File Descriptors Usage , Disk IO Latency , and Disk IO Load metrics. In the Network section, you can find the Network Traffic , Network Utilization Hourly , Local Network Errors , and TCP Retransmission metrics.","title":"Nodes Overview"},{"location":"details/dashboards/dashboard-numa-details.html","text":"NUMA Details For each node, this dashboard shows metrics related to Non-uniform memory access (NUMA). Memory Usage Remotes over time the total, used, and free memory. Free Memory Percent Shows the free memory as the ratio to the total available memory. NUMA Memory Usage Types Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mapped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable. NUMA Allocation Hits Memory successfully allocated on this node as intended. NUMA Allocation Missed Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node. Anonymous Memory Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out. NUMA File (PageCache) Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact. Shared Memory Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM). HugePages Statistics Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages . Local Processes Memory allocated on a node while a process was running on it. Remote Processes Memory allocated on a node while a process was running on some other node. Slab Memory Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"NUMA Details"},{"location":"details/dashboards/dashboard-numa-details.html#memory-usage","text":"Remotes over time the total, used, and free memory.","title":"Memory Usage"},{"location":"details/dashboards/dashboard-numa-details.html#free-memory-percent","text":"Shows the free memory as the ratio to the total available memory.","title":"Free Memory Percent"},{"location":"details/dashboards/dashboard-numa-details.html#numa-memory-usage-types","text":"Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mapped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable.","title":"NUMA Memory Usage Types"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-hits","text":"Memory successfully allocated on this node as intended.","title":"NUMA Allocation Hits"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-missed","text":"Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node.","title":"NUMA Allocation Missed"},{"location":"details/dashboards/dashboard-numa-details.html#anonymous-memory","text":"Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out.","title":"Anonymous Memory"},{"location":"details/dashboards/dashboard-numa-details.html#numa-file-pagecache","text":"Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact.","title":"NUMA File (PageCache)"},{"location":"details/dashboards/dashboard-numa-details.html#shared-memory","text":"Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM).","title":"Shared Memory"},{"location":"details/dashboards/dashboard-numa-details.html#hugepages-statistics","text":"Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages .","title":"HugePages Statistics"},{"location":"details/dashboards/dashboard-numa-details.html#local-processes","text":"Memory allocated on a node while a process was running on it.","title":"Local Processes"},{"location":"details/dashboards/dashboard-numa-details.html#remote-processes","text":"Memory allocated on a node while a process was running on some other node.","title":"Remote Processes"},{"location":"details/dashboards/dashboard-numa-details.html#slab-memory","text":"Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"Slab Memory"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html","text":"PostgreSQL Instance Summary Number of Temp Files Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Size of Temp Files Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Temp Files Activity Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Temp Files Utilization Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Canceled Queries Based on pg_stat_database_conflicts view","title":"PostgreSQL Instance Summary"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#number-of-temp-files","text":"Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Number of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#size-of-temp-files","text":"Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Size of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-activity","text":"Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files Activity"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-utilization","text":"Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Temp Files Utilization"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#canceled-queries","text":"Based on pg_stat_database_conflicts view","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-compare.html","text":"PostgreSQL Instances Compare No description","title":"PostgreSQL Instances Compare"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html","text":"PostgreSQL Instances Overview Connected Reports whether PMM Server can connect to the PostgreSQL instance. Version The version of the PostgreSQL instance. Shared Buffers Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% . Disk-Page Buffers The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers. Memory Size for each Sort The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB . Disk Cache Size PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes. Autovacuum Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less. PostgreSQL Connections Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server. PostgreSQL Tuples Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5 min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones. PostgreSQL Transactions Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running. Temp Files Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. Note All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Conflicts and Locks Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL. Buffers and Blocks Operations Operations with Blocks The time spent reading and writing data file blocks by back ends, in milliseconds. Note Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL. Canceled Queries The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Note Data shown by this gauge are based on the pg_stat_database_conflicts view. Cache Hit Ratio The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. Note This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache. Checkpoint Stats The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds. PostgreSQL Settings The list of all settings of the PostgreSQL server. System Summary This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"PostgreSQL Dashboards"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#connected","text":"Reports whether PMM Server can connect to the PostgreSQL instance.","title":"Connected"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#version","text":"The version of the PostgreSQL instance.","title":"Version"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#shared-buffers","text":"Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% .","title":"Shared Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-page-buffers","text":"The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers.","title":"Disk-Page Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#memory-size-for-each-sort","text":"The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB .","title":"Memory Size for each Sort"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-cache-size","text":"PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes.","title":"Disk Cache Size"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#autovacuum","text":"Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less.","title":"Autovacuum"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-connections","text":"Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server.","title":"PostgreSQL Connections"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-tuples","text":"Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5 min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones.","title":"PostgreSQL Tuples"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-transactions","text":"Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running.","title":"PostgreSQL Transactions"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#temp-files","text":"Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. Note All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#conflicts-and-locks","text":"Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL.","title":"Conflicts and Locks"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#buffers-and-blocks-operations","text":"Operations with Blocks The time spent reading and writing data file blocks by back ends, in milliseconds. Note Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL.","title":"Buffers and Blocks Operations"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#canceled-queries","text":"The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Note Data shown by this gauge are based on the pg_stat_database_conflicts view.","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#cache-hit-ratio","text":"The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. Note This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache.","title":"Cache Hit Ratio"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#checkpoint-stats","text":"The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds.","title":"Checkpoint Stats"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-settings","text":"The list of all settings of the PostgreSQL server.","title":"PostgreSQL Settings"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#system-summary","text":"This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"System Summary"},{"location":"details/dashboards/dashboard-processes-details.html","text":"Processes Details The Processes Details dashboard displays Linux process information - PIDs, Threads, and Processes. The dashboard shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. The dashboard consists of two parts: the first section describes metrics for all hosts, and the second part provides charts for each host. Charts for all hosts, available in the first section, are the following ones: States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Runnable Processes Blocked Processes Waiting for I/O Sleeping Processes Running Processes Disk Sleep Processes Stopped Processes Zombie Processes Dead Processes The following charts are present in the second part, available for each host: Processes States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Number of PIDs No description Percentage of Max PIDs Limit No description Number of Threads No description Percentage of Max Threads Limit No description Runnable Processes Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Blocked Processes Waiting for I/O Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Sleeping Processes No description Running Processes No description Disk Sleep Processes No description Stopped Processes No description Zombie Processes No description Dead Processes No description","title":"Processes Details"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-pids","text":"No description","title":"Number of PIDs"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-pids-limit","text":"No description","title":"Percentage of Max PIDs Limit"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-threads","text":"No description","title":"Number of Threads"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-threads-limit","text":"No description","title":"Percentage of Max Threads Limit"},{"location":"details/dashboards/dashboard-processes-details.html#runnable-processes","text":"Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Runnable Processes"},{"location":"details/dashboards/dashboard-processes-details.html#blocked-processes-waiting-for-io","text":"Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Blocked Processes Waiting for I/O"},{"location":"details/dashboards/dashboard-processes-details.html#sleeping-processes","text":"No description","title":"Sleeping Processes"},{"location":"details/dashboards/dashboard-processes-details.html#running-processes","text":"No description","title":"Running Processes"},{"location":"details/dashboards/dashboard-processes-details.html#disk-sleep-processes","text":"No description","title":"Disk Sleep Processes"},{"location":"details/dashboards/dashboard-processes-details.html#stopped-processes","text":"No description","title":"Stopped Processes"},{"location":"details/dashboards/dashboard-processes-details.html#zombie-processes","text":"No description","title":"Zombie Processes"},{"location":"details/dashboards/dashboard-processes-details.html#dead-processes","text":"No description","title":"Dead Processes"},{"location":"details/dashboards/dashboard-prometheus-exporter-status.html","text":"Prometheus Exporter Status The Prometheus Exporter Status dashboard reports the consumption of resources by the Prometheus exporters used by PMM. For each exporter, this dashboard reveals the following information: CPU usage Memory usage File descriptors used Exporter uptime","title":"Prometheus Dashboards"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html","text":"Prometheus Exporters Overview Prometheus Exporters Summary This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. Note The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system. Prometheus Exporters Resource Usage by Node This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts). Prometheus Exporters Resource Usage by Type This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter. List of Hosts At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"Prometheus Exporters Overview"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-summary","text":"This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. Note The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system.","title":"Prometheus Exporters Summary"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-node","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts).","title":"Prometheus Exporters Resource Usage by Node"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-type","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter.","title":"Prometheus Exporters Resource Usage by Type"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#list-of-hosts","text":"At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"List of Hosts"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html","text":"ProxySQL Instance Summary Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time.","title":"ProxySQL Dashboards"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary.html","text":"PXC/Galera Cluster Summary No description","title":"PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html","text":"PXC/Galera Node Summary Galera Replication Latency Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster. Galera Replication Queues Shows the length of receive and send queues. Galera Cluster Size Shows the number of members currently connected to the cluster. Galera Flow Control Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem. Galera Parallelization Efficiency Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization). Galera Writing Conflicts Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes. Available Downtime before SST Required Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method. Galera Writeset Count Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node). Galera Writeset Size Shows the average transaction size received/replicated. Galera Writeset Traffic Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node). Galera Network Usage Hourly Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"HA Dashboards"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-latency","text":"Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster.","title":"Galera Replication Latency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-queues","text":"Shows the length of receive and send queues.","title":"Galera Replication Queues"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"Galera Cluster Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-flow-control","text":"Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem.","title":"Galera Flow Control"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-parallelization-efficiency","text":"Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization).","title":"Galera Parallelization Efficiency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writing-conflicts","text":"Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes.","title":"Galera Writing Conflicts"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#available-downtime-before-sst-required","text":"Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method.","title":"Available Downtime before SST Required"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-count","text":"Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Count"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-size","text":"Shows the average transaction size received/replicated.","title":"Galera Writeset Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-traffic","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-network-usage-hourly","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Network Usage Hourly"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html","text":"PXC/Galera Nodes Compare $cluster - Galera Cluster Size Shows the number of members currently connected to the cluster.","title":"PXC/Galera Nodes Compare"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html#cluster-galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"$cluster - Galera Cluster Size"},{"location":"details/dashboards/dashboard-victoriametrics-agents-overview.html","text":"VictoriaMetrics Agents Overview No description","title":"VictoriaMetrics Agents Overview"},{"location":"details/dashboards/dashboard-victoriametrics.html","text":"VictoriaMetrics No description","title":"VictoriaMetrics"},{"location":"how-to/index.html","text":"How to Configure via the PMM Settings page Upgrade PMM Server via the user interface Secure your PMM installation Optimize the performance of your PMM installation Annotate charts to mark significant events Render dashboard images to save or share Troubleshoot : Integrated Alerting","title":"How to"},{"location":"how-to/annotate.html","text":"Annotate Annotations mark a moment in time. They are useful for marking system changes or other significant application events. They can be set globally or for specific nodes or services. You create them on the command line with the pmm-admin annotate command. Annotations show as a vertical dashed line on a dashboard graph. Reveal the annotation text by mousing over the caret indicator below the line. You turn annotations on or off with the PMM Annotations switch in the second row menu bar.","title":"Annotate"},{"location":"how-to/configure.html","text":"Configure The PMM Settings page lets you configure a number of PMM options. Open the PMM Settings page with one of: the main menu: choose PMM\u2192PMM Settings search dashboards by name: type PMM Settings and click the search result On the left of the page is a set of sub-page selector tabs. (The Communication tab remains hidden until Integrated Alerting is activated.) Tip Click Apply changes after changing settings. Diagnostics Common to all sections is Diagnostics . PMM can generate a set of diagnostics data which can be examined and/or shared with Percona Support in case of some issue to solve it faster. You can get collected logs from PMM Server by clicking Download server diagnostics . Metrics resolution Metrics are collected at three intervals representing low, medium and high resolutions. Short time intervals are regarded as high resolution metrics, while those at longer time intervals are low resolution. The Metrics Resolution radio button lets you select one of four presets. Rare , Standard and Frequent are fixed presets. Custom is an editable preset. Each preset is a group of Low, Medium and High metrics resolution values. A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Values for the Custom preset can be entered as values, or changed with the arrows. Note If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server it is monitoring, scraping every second may not be possible when the network latency is greater than 1 second. Advanced Settings Data Retention Data retention specifies how long data is stored by PMM Server. Telemetry The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake of the various versions of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. Currently, only the following information is gathered: PMM Version, Installation Method (Docker, AMI, OVF), the Server Uptime. We do not gather anything that would make the system identifiable, but the following two things are to be mentioned: The Country Code is evaluated from the submitting IP address before it is discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow sufficient time to disable the service for those that do not wish to share any information. There is a landing page for this service, available at check.percona.com , which clearly explains what this service is, what it\u2019s collecting, and how you can turn it off. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Notes If the Security Threat Tool is enabled in PMM Settings, Telemetry is automatically enabled. Telemetry is sent immediately; the 24-hour grace period is not honored. Check for updates When active, PMM will automatically check for updates and put a notification in the Updates dashboard if any are available. Security Threat Tool The Security Threat Tool performs a range of security-related checks on a registered instance and reports the findings. It is disabled by default. It can be enabled in PMM\u2192PMM Settings\u2192Settings\u2192Advanced Settings\u2192Security Threat Tool . The checks are re-fetched and re-run every 24 hours. The results can be viewed in PMM\u2192PMM Database Checks . DBaaS A read-only setting that shows whether DBaaS features are activated on this server. Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See Setting up a development environment for DBaaS . Integrated Alerting Enables Integrated Alerting and reveals the Communication tab. Public Address Public address for accessing DBaaS features on this server. SSH Key This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key . Alertmanager integration Alertmanager manages alerts, de-duplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed. Percona Platform This panel is where you create, and log into and out of your Percona Platform account. Login If you have a Percona Platform account, enter your credentials and click Login . Click Sign out to log out of your Percona Platform account. Sign up To create a Percona Platform account: Click Sign up Enter a valid email address in the Email field Choose and enter a strong password in the Password field Select the check box acknowledging our terms of service and privacy policy Click Sign up A brief message will confirm the creation of your new account and you may now log in with these credentials. Note Your Percona Platform account is separate from your PMM User account. Communication Note This tab appears only when Advanced Settings \u2192 Integrated Alerting is on. Global communications settings for Integrated Alerting . Integrated Alerting uses a separate instance of Alertmanager run by pmm-managed . The descriptions for the settings here are reproduced from Prometheus Alertmanager configuration . Email Settings for the SMTP email server: Server Address : The default SMTP smarthost used for sending emails, including port number. From : The default SMTP From header field. Username : SMTP Auth using CRAM-MD5, LOGIN and PLAIN. Password : SMTP Auth using LOGIN and PLAIN. Hello : The default hostname to identify to the SMTP server. Identity : SMTP Auth using PLAIN. Secret : SMTP Auth using CRAM-MD5. Slack Settings for Slack notifications: URL : The Slack webhook URL to use for Slack notifications.","title":"Configure"},{"location":"how-to/configure.html#diagnostics","text":"Common to all sections is Diagnostics . PMM can generate a set of diagnostics data which can be examined and/or shared with Percona Support in case of some issue to solve it faster. You can get collected logs from PMM Server by clicking Download server diagnostics .","title":"Diagnostics"},{"location":"how-to/configure.html#metrics-resolution","text":"Metrics are collected at three intervals representing low, medium and high resolutions. Short time intervals are regarded as high resolution metrics, while those at longer time intervals are low resolution. The Metrics Resolution radio button lets you select one of four presets. Rare , Standard and Frequent are fixed presets. Custom is an editable preset. Each preset is a group of Low, Medium and High metrics resolution values. A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Values for the Custom preset can be entered as values, or changed with the arrows. Note If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server it is monitoring, scraping every second may not be possible when the network latency is greater than 1 second.","title":"Metrics resolution"},{"location":"how-to/configure.html#advanced-settings","text":"","title":"Advanced Settings"},{"location":"how-to/configure.html#data-retention","text":"Data retention specifies how long data is stored by PMM Server.","title":"Data Retention"},{"location":"how-to/configure.html#telemetry","text":"The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake of the various versions of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. Currently, only the following information is gathered: PMM Version, Installation Method (Docker, AMI, OVF), the Server Uptime. We do not gather anything that would make the system identifiable, but the following two things are to be mentioned: The Country Code is evaluated from the submitting IP address before it is discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow sufficient time to disable the service for those that do not wish to share any information. There is a landing page for this service, available at check.percona.com , which clearly explains what this service is, what it\u2019s collecting, and how you can turn it off. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Notes If the Security Threat Tool is enabled in PMM Settings, Telemetry is automatically enabled. Telemetry is sent immediately; the 24-hour grace period is not honored.","title":"Telemetry"},{"location":"how-to/configure.html#check-for-updates","text":"When active, PMM will automatically check for updates and put a notification in the Updates dashboard if any are available.","title":"Check for updates"},{"location":"how-to/configure.html#security-threat-tool","text":"The Security Threat Tool performs a range of security-related checks on a registered instance and reports the findings. It is disabled by default. It can be enabled in PMM\u2192PMM Settings\u2192Settings\u2192Advanced Settings\u2192Security Threat Tool . The checks are re-fetched and re-run every 24 hours. The results can be viewed in PMM\u2192PMM Database Checks .","title":"Security Threat Tool"},{"location":"how-to/configure.html#dbaas","text":"A read-only setting that shows whether DBaaS features are activated on this server. Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See Setting up a development environment for DBaaS .","title":"DBaaS"},{"location":"how-to/configure.html#integrated-alerting","text":"Enables Integrated Alerting and reveals the Communication tab.","title":"Integrated Alerting"},{"location":"how-to/configure.html#public-address","text":"Public address for accessing DBaaS features on this server.","title":"Public Address"},{"location":"how-to/configure.html#ssh-key","text":"This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key .","title":"SSH Key"},{"location":"how-to/configure.html#alertmanager-integration","text":"Alertmanager manages alerts, de-duplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed.","title":"Alertmanager integration"},{"location":"how-to/configure.html#percona-platform","text":"This panel is where you create, and log into and out of your Percona Platform account.","title":"Percona Platform"},{"location":"how-to/configure.html#login","text":"If you have a Percona Platform account, enter your credentials and click Login . Click Sign out to log out of your Percona Platform account.","title":"Login"},{"location":"how-to/configure.html#sign-up","text":"To create a Percona Platform account: Click Sign up Enter a valid email address in the Email field Choose and enter a strong password in the Password field Select the check box acknowledging our terms of service and privacy policy Click Sign up A brief message will confirm the creation of your new account and you may now log in with these credentials. Note Your Percona Platform account is separate from your PMM User account.","title":"Sign up"},{"location":"how-to/configure.html#communication","text":"Note This tab appears only when Advanced Settings \u2192 Integrated Alerting is on. Global communications settings for Integrated Alerting . Integrated Alerting uses a separate instance of Alertmanager run by pmm-managed . The descriptions for the settings here are reproduced from Prometheus Alertmanager configuration .","title":"Communication"},{"location":"how-to/configure.html#email","text":"Settings for the SMTP email server: Server Address : The default SMTP smarthost used for sending emails, including port number. From : The default SMTP From header field. Username : SMTP Auth using CRAM-MD5, LOGIN and PLAIN. Password : SMTP Auth using LOGIN and PLAIN. Hello : The default hostname to identify to the SMTP server. Identity : SMTP Auth using PLAIN. Secret : SMTP Auth using CRAM-MD5.","title":"Email"},{"location":"how-to/configure.html#slack","text":"Settings for Slack notifications: URL : The Slack webhook URL to use for Slack notifications.","title":"Slack"},{"location":"how-to/optimize.html","text":"Optimize Improving PMM Performance with Table Statistics Options If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats and --disable-tablestats-limit . Note These settings can only be used when adding an instance. To change them, you must remove and re-add the instances. You can only use one of these options when adding an instance. Disable per-table statistics for an instance When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance. USAGE sudo pmm-admin add mysql --disable-tablestats Change the number of tables beyond which per-table statistics is disabled When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled. USAGE sudo pmm-admin add mysql --disable-tablestats-limit = <LIMIT> EXAMPLE Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. sudo pmm-admin add mysql --disable-tablestats-limit = 2000","title":"Optimize"},{"location":"how-to/optimize.html#improving-pmm-performance-with-table-statistics-options","text":"If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats and --disable-tablestats-limit . Note These settings can only be used when adding an instance. To change them, you must remove and re-add the instances. You can only use one of these options when adding an instance.","title":"Improving PMM Performance with Table Statistics Options"},{"location":"how-to/optimize.html#disable-per-table-statistics-for-an-instance","text":"When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance.","title":"Disable per-table statistics for an instance"},{"location":"how-to/optimize.html#usage","text":"sudo pmm-admin add mysql --disable-tablestats","title":"USAGE"},{"location":"how-to/optimize.html#change-the-number-of-tables-beyond-which-per-table-statistics-is-disabled","text":"When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled.","title":"Change the number of tables beyond which per-table statistics is disabled"},{"location":"how-to/optimize.html#usage_1","text":"sudo pmm-admin add mysql --disable-tablestats-limit = <LIMIT>","title":"USAGE"},{"location":"how-to/optimize.html#example","text":"Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. sudo pmm-admin add mysql --disable-tablestats-limit = 2000","title":"EXAMPLE"},{"location":"how-to/render-dashboard-images.html","text":"Render dashboard images At the moment, PMM Server can\u2019t render dashboard images exported by Grafana without these steps. Part 1: Install dependencies Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plugins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib Part 2 - Share the image Navigate to the dashboard you want to share. Open the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . A new browser tab opens. Wait for the image to be rendered then use your browser\u2019s image save function to download the image. If the necessary plugins are not installed, a message in the Share Panel will say so.","title":"Render dashboard images"},{"location":"how-to/secure.html","text":"Secure You can improve the security of your PMM installation with: SSL encryption to secure traffic between client and server; Grafana HTTPS secure cookies To see which security features are enabled: pmm-admin status Tip You can gain an extra level of security by keeping PMM Server isolated from the internet, if possible. SSL encryption You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container. Mounting certificates For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 Note The certificates must be owned by root. You can do this with: sudo chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80. Copying certificates If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem Enabling SSL when connecting PMM Client to PMM Server pmm-admin config --server-url=https://<user>:<password>@<server IP> Grafana HTTPS secure cookies To enable: Start a shell within the Docker container: docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini Enable cookie_secure and set the value to true Restart Grafana: supervisorctl restart grafana","title":"Secure"},{"location":"how-to/secure.html#ssl-encryption","text":"You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container.","title":"SSL encryption"},{"location":"how-to/secure.html#mounting-certificates","text":"For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 Note The certificates must be owned by root. You can do this with: sudo chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80.","title":"Mounting certificates"},{"location":"how-to/secure.html#copying-certificates","text":"If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem","title":"Copying certificates"},{"location":"how-to/secure.html#enabling-ssl-when-connecting-pmm-client-to-pmm-server","text":"pmm-admin config --server-url=https://<user>:<password>@<server IP>","title":"Enabling SSL when connecting PMM Client to PMM Server"},{"location":"how-to/secure.html#grafana-https-secure-cookies","text":"To enable: Start a shell within the Docker container: docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini Enable cookie_secure and set the value to true Restart Grafana: supervisorctl restart grafana","title":"Grafana HTTPS secure cookies"},{"location":"how-to/troubleshoot.html","text":"Troubleshoot Integrated Alerting No Integrated Alerting icon You are not logged in as a privileged user. You need either Admin or Editor roles to work with Integrated Alerting. Integrated Alerting icon but no submenu Integrated Alerting isn\u2019t activated. Go to PMM \u2192 PMM Settings \u2192 Advanced Settings Enable Integrated Alerting Unreachable external IP addresses When I get an email or page from my system the IP is not reachable from outside my organization how do I fix this? You can configure your PMM Server\u2019s Public Address by navigating to PMM \u2192 PMM Settings \u2192 Advanced Settings, and supply an address to use in your alert notifications. What is \u2018Alertmanager integration\u2019? There\u2019s already an Alertmanager integration tab without me turning it on, I know because I was using your existing Alertmanager integration. This will continue to work but will be renamed External Alertmanager . Notification channels not working I tried to setup a Slack/Email channel but nothing happened Before you can use a notification channel you must provide your connection details. Go to PMM \u2192 PMM Settings\u2192 Communication Define your SMTP server or Slack incoming webhook URL For PagerDuty you can configure in the notification channel tab of Integrated Alerting by supplying your server/routing key. What\u2019s the difference: Username/Password vs Identity/Secret In configuring my email server I\u2019m being asked for a Username and Password as well as Identity and Secret. What is the difference between these and which do I use or do I need both? It depends on what kind of authentication your system uses: LOGIN: Use Username/Password PLAIN: Use either Username or Identity and Password CRAM-MD5: Use Username and Secret Alert Rule Templates is disabled Built-In alerts are not editable. However, you can copy them and edit the copies. (PMM >=2.14.0). If you create a custom alert rule template you will have access to edit. Creating rules I\u2019m ready to create my first rule! I\u2019ve chosen a template and given it a name\u2026what is the format of the fields? Threshold - float value, it has different meanings depending on what template is used Duration - The duration the condition must be satisfied in seconds Filters - A Key, Evaluator, and Value. E.g. service_name=ps5.7 Key must be an exact match. You can find a complete list of keys by using the Explore main menu item in PMM Evaluator can be any of: = , =~ Value is an exact match or when used with a \u2018fuzzy\u2019 evaluator (=~) can be a regular expression. E.g. service_name=~ps.* Variables in Templates The concept of \u201ctemplate\u201d implies things like variable substitutions\u2026where can I use these? Where can I find a complete list of them? Here is a guide to creating templates for Alertmanager: https://prometheus.io/docs/prometheus/latest/configuration/template_examples/","title":"Troubleshoot"},{"location":"how-to/troubleshoot.html#integrated-alerting","text":"","title":"Integrated Alerting"},{"location":"how-to/troubleshoot.html#no-integrated-alerting-icon","text":"You are not logged in as a privileged user. You need either Admin or Editor roles to work with Integrated Alerting.","title":"No  Integrated Alerting icon"},{"location":"how-to/troubleshoot.html#integrated-alerting-icon-but-no-submenu","text":"Integrated Alerting isn\u2019t activated. Go to PMM \u2192 PMM Settings \u2192 Advanced Settings Enable Integrated Alerting","title":" Integrated Alerting icon but no submenu"},{"location":"how-to/troubleshoot.html#unreachable-external-ip-addresses","text":"When I get an email or page from my system the IP is not reachable from outside my organization how do I fix this? You can configure your PMM Server\u2019s Public Address by navigating to PMM \u2192 PMM Settings \u2192 Advanced Settings, and supply an address to use in your alert notifications.","title":"Unreachable external IP addresses"},{"location":"how-to/troubleshoot.html#what-is-alertmanager-integration","text":"There\u2019s already an Alertmanager integration tab without me turning it on, I know because I was using your existing Alertmanager integration. This will continue to work but will be renamed External Alertmanager .","title":"What is 'Alertmanager integration'?"},{"location":"how-to/troubleshoot.html#notification-channels-not-working","text":"I tried to setup a Slack/Email channel but nothing happened Before you can use a notification channel you must provide your connection details. Go to PMM \u2192 PMM Settings\u2192 Communication Define your SMTP server or Slack incoming webhook URL For PagerDuty you can configure in the notification channel tab of Integrated Alerting by supplying your server/routing key.","title":"Notification channels not working"},{"location":"how-to/troubleshoot.html#whats-the-difference-usernamepassword-vs-identitysecret","text":"In configuring my email server I\u2019m being asked for a Username and Password as well as Identity and Secret. What is the difference between these and which do I use or do I need both? It depends on what kind of authentication your system uses: LOGIN: Use Username/Password PLAIN: Use either Username or Identity and Password CRAM-MD5: Use Username and Secret","title":"What's the difference: Username/Password vs Identity/Secret"},{"location":"how-to/troubleshoot.html#alert-rule-templates-is-disabled","text":"Built-In alerts are not editable. However, you can copy them and edit the copies. (PMM >=2.14.0). If you create a custom alert rule template you will have access to edit.","title":"Alert Rule Templates is disabled"},{"location":"how-to/troubleshoot.html#creating-rules","text":"I\u2019m ready to create my first rule! I\u2019ve chosen a template and given it a name\u2026what is the format of the fields? Threshold - float value, it has different meanings depending on what template is used Duration - The duration the condition must be satisfied in seconds Filters - A Key, Evaluator, and Value. E.g. service_name=ps5.7 Key must be an exact match. You can find a complete list of keys by using the Explore main menu item in PMM Evaluator can be any of: = , =~ Value is an exact match or when used with a \u2018fuzzy\u2019 evaluator (=~) can be a regular expression. E.g. service_name=~ps.*","title":"Creating rules"},{"location":"how-to/troubleshoot.html#variables-in-templates","text":"The concept of \u201ctemplate\u201d implies things like variable substitutions\u2026where can I use these? Where can I find a complete list of them? Here is a guide to creating templates for Alertmanager: https://prometheus.io/docs/prometheus/latest/configuration/template_examples/","title":"Variables in Templates"},{"location":"how-to/upgrade.html","text":"Upgrade Updating a Server Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated.","title":"Upgrade"},{"location":"how-to/upgrade.html#updating-a-server","text":"Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated.","title":"Updating a Server"},{"location":"release-notes/index.html","text":"Release Notes Percona Monitoring and Management 2.15.0 Percona Monitoring and Management 2.14.0 Percona Monitoring and Management 2.13.0 Percona Monitoring and Management 2.12.0 Percona Monitoring and Management 2.11.1 Percona Monitoring and Management 2.11.0 Percona Monitoring and Management 2.10.1 Percona Monitoring and Management 2.10.0 Percona Monitoring and Management 2.9.1 Percona Monitoring and Management 2.9.0 Percona Monitoring and Management 2.8.0 Percona Monitoring and Management 2.7.0 Percona Monitoring and Management 2.6.1 Percona Monitoring and Management 2.6.0 Percona Monitoring and Management 2.5.0 Percona Monitoring and Management 2.4.0 Percona Monitoring and Management 2.3.0 Percona Monitoring and Management 2.2.2 Percona Monitoring and Management 2.2.1 Percona Monitoring and Management 2.2.0 Percona Monitoring and Management 2.1.0 Percona Monitoring and Management 2.0.1 Percona Monitoring and Management 2.0.0","title":"Release Notes"},{"location":"release-notes/2.0.0.html","text":"Percona Monitoring and Management 2.0.0 Date: September 19, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. The new PMM2 introduces a number of enhancements and additional feature improvements, including: Detailed query analytics and filtering technologies which enable you to identify issues faster than ever before. A better user experience: Service-level dashboards give you immediate access to the data you need. The new addition of PostgreSQL query tuning. Enhanced security protocols to ensure your data is safe. Our new API allows you to extend and interact with third-party tools. More details about new and improved features available within the release can be found in the corresponding blog post . Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.0.0"},{"location":"release-notes/2.0.1.html","text":"Percona Monitoring and Management 2.0.1 Date: October 9, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package change log Fixed bugs PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.0.1"},{"location":"release-notes/2.0.1.html#improvements","text":"PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package change log","title":"Improvements"},{"location":"release-notes/2.0.1.html#fixed-bugs","text":"PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.1.0.html","text":"Percona Monitoring and Management 2.1.0 Date: November 11, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and configuration files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin Fixed bugs PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drill-downs from the Environment Overview dashboard doesn\u2019t show data for the preselected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.1.0"},{"location":"release-notes/2.1.0.html#improvements-and-new-features","text":"PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and configuration files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin","title":"Improvements and new features"},{"location":"release-notes/2.1.0.html#fixed-bugs","text":"PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drill-downs from the Environment Overview dashboard doesn\u2019t show data for the preselected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.10.0.html","text":"Percona Monitoring and Management 2.10.0 Date: September 15, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to 2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent interfaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration Improvements PMM-6647 : MongoDB dashboards: RocksDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers Bugs Fixed PMM-6336 : Suppress sensitive data: honor pmm-admin flag --disable-queryexamples when used in conjunction with --query-source=perfschema PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for DB uptime (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling Known Issues PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"Percona Monitoring and Management 2.10.0"},{"location":"release-notes/2.10.0.html#new-features","text":"PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to 2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent interfaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration","title":"New Features"},{"location":"release-notes/2.10.0.html#improvements","text":"PMM-6647 : MongoDB dashboards: RocksDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers","title":"Improvements"},{"location":"release-notes/2.10.0.html#bugs-fixed","text":"PMM-6336 : Suppress sensitive data: honor pmm-admin flag --disable-queryexamples when used in conjunction with --query-source=perfschema PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for DB uptime (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling","title":"Bugs Fixed"},{"location":"release-notes/2.10.0.html#known-issues","text":"PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"Known Issues"},{"location":"release-notes/2.10.1.html","text":"Percona Monitoring and Management 2.10.1 Date: September 22, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"Percona Monitoring and Management 2.10.1"},{"location":"release-notes/2.10.1.html#bugs-fixed","text":"PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"Bugs Fixed"},{"location":"release-notes/2.11.0.html","text":"Percona Monitoring and Management 2.11.0 Date: October 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-6567 : Technical preview of new PostgreSQL extension pg_stat_monitor PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window Improvements PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2 , grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to 2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnoDB Flushing by Type\u201d Bugs Fixed PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension pg_stat_monitor PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update ) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 ( postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite --disable-queryexamples option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs ( vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"Percona Monitoring and Management 2.11.0"},{"location":"release-notes/2.11.0.html#new-features","text":"PMM-6567 : Technical preview of new PostgreSQL extension pg_stat_monitor PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window","title":"New Features"},{"location":"release-notes/2.11.0.html#improvements","text":"PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2 , grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to 2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnoDB Flushing by Type\u201d","title":"Improvements"},{"location":"release-notes/2.11.0.html#bugs-fixed","text":"PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension pg_stat_monitor PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update ) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 ( postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite --disable-queryexamples option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs ( vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"Bugs Fixed"},{"location":"release-notes/2.11.1.html","text":"Percona Monitoring and Management 2.11.1 Date: October 19, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed PMM-6782 : High CPU usage after update to 2.11.0","title":"Percona Monitoring and Management 2.11.1"},{"location":"release-notes/2.11.1.html#bugs-fixed","text":"PMM-6782 : High CPU usage after update to 2.11.0","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html","text":"Percona Monitoring and Management 2.12.0 Date: December 1, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights VictoriaMetrics replaces Prometheus and is now the default data source. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better. New Features PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality Improvements PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : pmm-admin status --wait option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin : Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable Kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default data source name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables Bugs Fixed PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services Known Issues PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"Percona Monitoring and Management 2.12.0"},{"location":"release-notes/2.12.0.html#release-highlights","text":"VictoriaMetrics replaces Prometheus and is now the default data source. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better.","title":"Release Highlights"},{"location":"release-notes/2.12.0.html#new-features","text":"PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality","title":"New Features"},{"location":"release-notes/2.12.0.html#improvements","text":"PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : pmm-admin status --wait option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin : Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable Kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default data source name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables","title":"Improvements"},{"location":"release-notes/2.12.0.html#bugs-fixed","text":"PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html#known-issues","text":"PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"Known Issues"},{"location":"release-notes/2.13.0.html","text":"Percona Monitoring and Management 2.13.0 Date: December 29, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Note: We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .) Improvements PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in pmm-admin add command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/<end-point> to /victoriametrics/<end-point> but created aliases for users that still rely on the /prometheus/<end-point> in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to 1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags ) Bugs Fixed PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"Percona Monitoring and Management 2.13.0"},{"location":"release-notes/2.13.0.html#release-highlights","text":"Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Note: We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .)","title":"Release Highlights"},{"location":"release-notes/2.13.0.html#improvements","text":"PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in pmm-admin add command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/<end-point> to /victoriametrics/<end-point> but created aliases for users that still rely on the /prometheus/<end-point> in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to 1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags )","title":"Improvements"},{"location":"release-notes/2.13.0.html#bugs-fixed","text":"PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"Bugs Fixed"},{"location":"release-notes/2.14.0.html","text":"Percona Monitoring and Management 2.14.0 Date: January 28, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights Switch to push metrics by default In PMM 2.12.0, Percona replaced its metrics collection engine (formerly Prometheus) with VictoriaMetrics. Historically, PMM used a pull method with Prometheus while VictoriaMetrics can operate in either a pull or push method. When PMM 2.12.0 was released, Percona kept the default method as pull . Now with PMM 2.14.0, Percona is shifting the default to push for all newly-added instances. This blog post describes the two methods and why push benefits users. Also, here is a post by Peter Zaitzev of FAQs relating to the move to VictoriaMetrics and the push model. Documentation on the push method is here . Note : Installing the 2.14.0 or newer PMM server will change the default behavior on 2.12.0 and 2.13.0 clients from \u201cpull\u201d method to \u201cpush\u201d for any newly added services. Existing services will remain in whatever mode they were prior to upgrade. DBaaS Preview phase 1.0 (Technical Preview) In 2.13.0 we introduced Percona\u2019s Database as a Service (DBaaS) which enables non-DBAs (software architects, developers, site reliability engineers, etc.) to perform typical DBA tasks to manage an organization\u2019s database environment via user interfaces and automation orchestration. This release contains several enhancements and fixes, many directly from user feedback. Note : This capability is feature-flagged and turned off by default. Users require a variable to be passed to PMM to expose this functionality. External services presentation on node summary dashboard Improvements to the user experience for adding and viewing external services (any data that can be monitored by a Prometheus exporter such as: non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application) on the Node Summary dashboard of PMM. New Features PMM-5765 : Ability to monitor External Services for situations where PMM Client can\u2019t be installed \u2013 Uses a new command pmm-admin add external-serverless . (See pmm-admin .) (This is a Technical Preview feature) PMM-7015 : DBaaS Preview: Create DB cluster with randomly-generated password PMM-7007 : Integrated Alerting: Ability to copy (duplicate) alert rules PMM-7006 : Integrated Alerting: Ability to delete alert rules PMM-6941 : Integrated Alerting: Ability to delete alert rule templates Improvements PMM-6985 : DBaaS: Ability to force unregister Kubernetes cluster PMM-7145 : \u2018Push\u2019 metrics mode is default when adding services and nodes (All agents collecting data from Services and Nodes will now use PUSH model if not specified explicitly. You will still be able to use --metrics-mode flag to use Pull metrics if needed. All previously set up agents will keep their existing mode. To change these you need to remove and re-add them.) PMM-7282 : Integrated Alerting: Ability to create rule without channels and filters PMM-7226 : Integrated Alerting: Validate parameters during rule creation/update PMM-7082 : Integrated Alerting: Severity levels are color-coded PMM-7065 : Integrated Alerting: Show rule details for items in Alert Rules list PMM-7048 : DBaaS: Simplify Cluster creation by moving Create Cluster button to earlier steps PMM-6993 : Protect against possible problems with EXPLAIN of stored functions in MySQL \u2013 We are fixing possible problems caused by an attempt to analyze queries covered in https://bugs.mysql.com/bug.php?id=67632 . Bugs Fixed PMM-7312 : Error when accessing Metrics data on Dashboards for large installations PMM-7310 : VictoriaMetrics consuming 100\u2019s Gb\u2019s of disk in /tmp/searchResults in PMM 2.13.0 PMM-5137 : Swagger page redirect isn\u2019t working PMM-7144 : DBaaS: Creating DB cluster with same name (Thanks to Beata Handzelova for reporting this issue) PMM-7323 : DBaaS: \u2018Remove DB Cluster from Kubernetes Cluster\u2019 removes wrong one PMM-7251 : Integrated Alerting: Error Rule with ID \"mysql_version\" not found if both Security Threat Tool and Integrated Alerting enabled PMM-7247 : DBaaS: Disk size is always 0 for Percona XtraDB cluster PMM-7178 : pg_stat_monitor integration is broken with version 0.6.0 of the plugin PMM-7169 : Old data (from Prometheus) not deleted when Retention period expires PMM-7105 : Query Analytics: no \u2018Example\u2019 or \u2018Explain\u2019 data for MariaDB PMM-7239 : Integrated Alerting: Validate Slack channel names in Notification Channels PMM-7213 : MySQL InnoDB Details dashboard: remove color-coding on \u2018Data Buffer Pool Fit\u2019 element PMM-7167 : Some panels not visible when using long time intervals (e.g. 30 days) PMM-7133 : Incorrect descriptions for data links in dashboards PMM-7103 : VictoriaMetrics build logs not deleted from PMM Server Docker image PMM-6904 : pmm-admin annotate command crashes for non-generic node types PMM-6902 : No query Examples on PostgreSQL 12 with pg_stat_monitor PMM-6838 : ProxySQL Instance Summary dashboard: Incorrect \u201cHostgroup Size\u201d formula PMM-6490 : rds_exporter crashes when more than 100 AWS RDS instances added (Thanks to https://github.com/vlinevych for fixing this) PMM-6096 : pmm-agent connection checker does not check authentication for MongoDB PMM-7303 : Disk Details, Nodes Compare dashboards: \u2018Disk Utilization\u2019 description is confusing","title":"Percona Monitoring and Management 2.14.0"},{"location":"release-notes/2.14.0.html#release-highlights","text":"Switch to push metrics by default In PMM 2.12.0, Percona replaced its metrics collection engine (formerly Prometheus) with VictoriaMetrics. Historically, PMM used a pull method with Prometheus while VictoriaMetrics can operate in either a pull or push method. When PMM 2.12.0 was released, Percona kept the default method as pull . Now with PMM 2.14.0, Percona is shifting the default to push for all newly-added instances. This blog post describes the two methods and why push benefits users. Also, here is a post by Peter Zaitzev of FAQs relating to the move to VictoriaMetrics and the push model. Documentation on the push method is here . Note : Installing the 2.14.0 or newer PMM server will change the default behavior on 2.12.0 and 2.13.0 clients from \u201cpull\u201d method to \u201cpush\u201d for any newly added services. Existing services will remain in whatever mode they were prior to upgrade. DBaaS Preview phase 1.0 (Technical Preview) In 2.13.0 we introduced Percona\u2019s Database as a Service (DBaaS) which enables non-DBAs (software architects, developers, site reliability engineers, etc.) to perform typical DBA tasks to manage an organization\u2019s database environment via user interfaces and automation orchestration. This release contains several enhancements and fixes, many directly from user feedback. Note : This capability is feature-flagged and turned off by default. Users require a variable to be passed to PMM to expose this functionality. External services presentation on node summary dashboard Improvements to the user experience for adding and viewing external services (any data that can be monitored by a Prometheus exporter such as: non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application) on the Node Summary dashboard of PMM.","title":"Release Highlights"},{"location":"release-notes/2.14.0.html#new-features","text":"PMM-5765 : Ability to monitor External Services for situations where PMM Client can\u2019t be installed \u2013 Uses a new command pmm-admin add external-serverless . (See pmm-admin .) (This is a Technical Preview feature) PMM-7015 : DBaaS Preview: Create DB cluster with randomly-generated password PMM-7007 : Integrated Alerting: Ability to copy (duplicate) alert rules PMM-7006 : Integrated Alerting: Ability to delete alert rules PMM-6941 : Integrated Alerting: Ability to delete alert rule templates","title":"New Features"},{"location":"release-notes/2.14.0.html#improvements","text":"PMM-6985 : DBaaS: Ability to force unregister Kubernetes cluster PMM-7145 : \u2018Push\u2019 metrics mode is default when adding services and nodes (All agents collecting data from Services and Nodes will now use PUSH model if not specified explicitly. You will still be able to use --metrics-mode flag to use Pull metrics if needed. All previously set up agents will keep their existing mode. To change these you need to remove and re-add them.) PMM-7282 : Integrated Alerting: Ability to create rule without channels and filters PMM-7226 : Integrated Alerting: Validate parameters during rule creation/update PMM-7082 : Integrated Alerting: Severity levels are color-coded PMM-7065 : Integrated Alerting: Show rule details for items in Alert Rules list PMM-7048 : DBaaS: Simplify Cluster creation by moving Create Cluster button to earlier steps PMM-6993 : Protect against possible problems with EXPLAIN of stored functions in MySQL \u2013 We are fixing possible problems caused by an attempt to analyze queries covered in https://bugs.mysql.com/bug.php?id=67632 .","title":"Improvements"},{"location":"release-notes/2.14.0.html#bugs-fixed","text":"PMM-7312 : Error when accessing Metrics data on Dashboards for large installations PMM-7310 : VictoriaMetrics consuming 100\u2019s Gb\u2019s of disk in /tmp/searchResults in PMM 2.13.0 PMM-5137 : Swagger page redirect isn\u2019t working PMM-7144 : DBaaS: Creating DB cluster with same name (Thanks to Beata Handzelova for reporting this issue) PMM-7323 : DBaaS: \u2018Remove DB Cluster from Kubernetes Cluster\u2019 removes wrong one PMM-7251 : Integrated Alerting: Error Rule with ID \"mysql_version\" not found if both Security Threat Tool and Integrated Alerting enabled PMM-7247 : DBaaS: Disk size is always 0 for Percona XtraDB cluster PMM-7178 : pg_stat_monitor integration is broken with version 0.6.0 of the plugin PMM-7169 : Old data (from Prometheus) not deleted when Retention period expires PMM-7105 : Query Analytics: no \u2018Example\u2019 or \u2018Explain\u2019 data for MariaDB PMM-7239 : Integrated Alerting: Validate Slack channel names in Notification Channels PMM-7213 : MySQL InnoDB Details dashboard: remove color-coding on \u2018Data Buffer Pool Fit\u2019 element PMM-7167 : Some panels not visible when using long time intervals (e.g. 30 days) PMM-7133 : Incorrect descriptions for data links in dashboards PMM-7103 : VictoriaMetrics build logs not deleted from PMM Server Docker image PMM-6904 : pmm-admin annotate command crashes for non-generic node types PMM-6902 : No query Examples on PostgreSQL 12 with pg_stat_monitor PMM-6838 : ProxySQL Instance Summary dashboard: Incorrect \u201cHostgroup Size\u201d formula PMM-6490 : rds_exporter crashes when more than 100 AWS RDS instances added (Thanks to https://github.com/vlinevych for fixing this) PMM-6096 : pmm-agent connection checker does not check authentication for MongoDB PMM-7303 : Disk Details, Nodes Compare dashboards: \u2018Disk Utilization\u2019 description is confusing","title":"Bugs Fixed"},{"location":"release-notes/2.15.0.html","text":"Percona Monitoring and Management 2.15.0 Date: March 01, 2021 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights PMM 1 vs. 2 Parity Disable collectors during adding node/service to monitoring With this feature users can disable any collector used by PMM to get metrics. When metrics cannot be collected or are no longer needed, disabling the collector(s) prevents PMM from flooding logs and saves infrastructure resources. Our vision for PMM collectors is to provide \u201cstop from collecting\u201d functionality to prevent possible harm to the user environment. This \u201cdisable\u201d feature is an initial step towards the ideal functionality. The full and flexible management for \u201cWhat metrics to collect and in what resolution\u201d is slated for future releases. External services monitoring Since PMM 1.4.0, users had the ability to monitor external services Percona didn\u2019t currently support (e.g., Redis). This blog article from 2018 nicely described external services monitoring at that time. (At that time Percona was not natively supporting a PostgreSQL monitoring service and so this was listed as an external service. Today, PostgreSQL is natively supported by PMM.) Until now, PMM 2.x didn\u2019t support external services monitoring. With this release, any non-natively supported by PMM service will now become supported with external services monitoring. You can see the list of possible exporters to be used in https://prometheus.io/docs/instrumenting/exporters/ . Natively-supported services will continue to deliver an expanded set of metrics and insights. Provide summary information for systems (pt-*-summary actions) With the addition of pt-*-summary in PMM 2, users can now view summary information about services and nodes on PMM\u2019s dashboard. This summary information is in the industry common format of pt-*-summary tools output to simplify portability of this data. This format will also be preserved in the snapshot of the dashboard shared with Percona Support to simplify investigations of issues. Note: pt-*-summary includes formats for: pt-mysql-summary pt-mongodb-summary pt-pg-summary pt-summary HAProxy support by PMM Users are able to add HAProxy Services for monitoring in PMM2. The support level of them in PMM will be the same we have for ProxySQL, so they will be presented in Inventory and on Dashboard. This will allow users who use HAProxy in their HA configuration to also have this component monitored by PMM. In future releases PMM will start use HAProxy by default for the DBaaS feature and will also use this functionality to monitor HAProxy. DBaaS Preview improvements (Technical Preview) From now you will be able to see the progress of internal steps the system makes when executing some operations with DBaaS. The Progress Bar will not be time-related and will present only steps. The Progress Bar component will also reflect the K8s/Operator-related errors to the user, so in the case of errors, you will have the error text on the UI, and no need to use K8s tools to see the error. With the same UI, you will be able to see the latest logs from K8s so they will have even more information about why the error happened. Known Limitations: The progress bar will not provide valuable information for the Delete operation (will be in a later version when we\u2019ll change the API with Operators Team), Operation of DB Cluster Modification will have \u201cstrange\u201d behavior and will start changes from non-zero values of steps. (This will be modified after API changes.) New Features PMM-4172 , PMM-4306 , PMM-5784 , PMM-7177 : Services and Nodes Summary presentation. Present information about DB\u2019s and Node status using pt-mysql-summary , pt-mongodb-summary , pt-pg-summary outputs (in API and on Dashboards). PMM-7123 : Ability to add External Services via the UI in PMM server. PMM-6711 : Add external-group flag for pmm-admin inventory commands for simpler work with External services. PMM-7405 : Check connection response format when adding External Service to monitoring. PMM-6797 : HAProxy monitoring: Ability to add HAProxy services with pmm-admin [inventory] add [service] haproxy command. PMM-7487 : HAProxy monitoring: Check connection to HAProxy services when adding them for monitoring. PMM-7496 : HAProxy monitoring: New HAProxy PXC dashboards. PMM-6943 : HAProxy monitoring: Show HAProxy type services in PMM Inventory. PMM-6924 : Integrated Alerting: Show \u2018breadcrumbs\u2019 navigation aid on non-dashboard pages as well as Grafana dashboard pages. PMM-7294 : Integrated Alerting: Pagination for viewing large numbers of Alert Rules. PMM-7417 : Security Threat Tool: Show list of all available security checks. PMM-7418 : Security Threat Tool: Ability to disable specific security checks. PMM-7419 : DBaaS: Ability to see DB Cluster creation/modification logs. PMM-7266 : DBaaS: Cluster creation progress bar \u2013 You can now see the progress of DBaaS DB cluster creation. (The progress bar is based on the number of back-end technical steps, not the time required to perform the tasks.) Improvements PMM-4679 : Docker: :latest tag for pmm-server and pmm-client images has been moved from v1 latest release to v2 latest release. Note : use of the latest tag is not recommended in production environments, instead use :2 tag. PMM-7472 : Remove Prometheus data source \u2013 If you were using custom dashboards with a specified data source (not using empty to use default one) you may need to edit your dashboards to use the proper data source. PMM is no longer using Prometheus but uses compatible storage for metrics from VictoriaMetrics. We renamed the data source to be more technology-agnostic. PMM-6695 : Software update: Grafana 7.1.3 to 7.3.7 (See What\u2019s new in Grafana 7.2 and What\u2019s new in Grafana 7.3 .) PMM-7471 : Software update: VictoriaMetrics 1.52.0 to 1.53.1 (See VictoriaMetrics 1.53.0 and VictoriaMetrics 1.53.1 .) PMM-6693 : API keys usage \u2013 PMM users can now use API keys (generated in Grafana UI) for interaction with PMM server instead of username/password pairs. The API key should have the same level of access (Admin or Viewer) as is required for username/password pairs. PMM-7240 : DBaaS: Change from Dashboard to Grafana Page \u2013 We changed the DBaaS page from a Grafana Dashboard to a Grafana Page to be better aligned with the DBaaS enable/disable status and avoid confusion when DBaaS is disabled. PMM-7328 : Security Threat Tool: Download and run checks when activated, immediately, repeating every 24 hours thereafter (Previously, downloading and running new checks happened every 24 hours but the cycle didn\u2019t begin when STT was activated.) PMM-7329 : Security Threat Tool: Hide check results tab if STT is disabled. PMM-7331 : Security Threat Tool: Failed checks have \u2018Read more\u2019 links with helpful content. PMM-7422 : Security Threat Tool: View all active and silenced alerts. PMM-7257 , PMM-7433 : Integrated Alerting: Easier-to-read rule details in Alert Rules list (API and UI presentation). PMM-7259 : Integrated Alerting: Better UI error reporting for disabled Integrated Alerting. (Hint to users how to enable it.) PMM-5533 : Better indentation of columns in pmm-admin list output. PMM-5888 : Improve pmm-admin --help descriptions for external services. Bugs Fixed PMM-5837 : pmm-agent reports \u201cMalformed DSN\u201d error when adding PostgreSQL instance with a PMM user password containing = (equals sign) (Thanks to Alexandre Barth for reporting this issue). PMM-5969 : Removing Services or Nodes with pmm-admin ... --force mode does not stop running agents, VictoriaMetrics continues collecting data from exporters. PMM-6685 : In low screen resolutions Services submenu wraps, becomes obscured, and can\u2019t be accessed. PMM-6681 : Not all PMM admin users can download diagnostic logs, only those with Grafana admin rights. PMM-7227 : Table stats metrics not being collected in instances with millions of tables. PMM-7426 : vmagent continually restarts, blocking comms between pmm-agent & pmm-managed \u2013 Users running multiple services on the same PMM agent in \u2018push\u2019 mode could face this issue when restarting the agent after bulk-adding services. PMM-6636 : Dashboards: MySQL Replication Summary: \u2018Binlog Size\u2019, \u2018Binlog Data Written Hourly\u2019, \u2018Node\u2019 not being charted when the instance is RDS. PMM-7325 : Dashboards: MySQL User Details: user labels unreadable with high number (>20) of users (Thanks to Andrei Fedorov for reporting this issue). PMM-7416 : Dashboards: PostgreSQL Instance Summary: Some panels (e.g. Tuple) not using selected database. PMM-7235 : Integrated Alerting: Filtered out alerts are shown in the UI as firing. PMM-7324 : Integrated Alerting: Add Pager Duty Notification Channel: after user pastes copied key Add button is not enabled. PMM-7346 : Integrated Alerting: It is possible to create Alert Rule with negative duration time. PMM-7366 : Integrated Alerting: Entities (e.g. templates, channels, rules) are in inconsistent states. PMM-7467 : Integrated Alerting: < (less-than symbol) wrongly interpreted by Alert templates (as &lt; ). PMM-7591 : Integrated Alerting: User can not receive notifications on email after password update. PMM-7343 : Security Threat Tool: Check results show previously failed checks after STT reenabled. PMM-7250 : DBaaS: Confusing error \u201cCannot get PSMDB/PXC cluster\u201d appears after removing DB cluster. PMM-7193 : DBaaS: Number of Nodes can be set as float. PMM-7349 : DBaaS: Host and Password occasionally disappearing from Connection column.","title":"Percona Monitoring and Management 2.15.0"},{"location":"release-notes/2.15.0.html#release-highlights","text":"PMM 1 vs. 2 Parity Disable collectors during adding node/service to monitoring With this feature users can disable any collector used by PMM to get metrics. When metrics cannot be collected or are no longer needed, disabling the collector(s) prevents PMM from flooding logs and saves infrastructure resources. Our vision for PMM collectors is to provide \u201cstop from collecting\u201d functionality to prevent possible harm to the user environment. This \u201cdisable\u201d feature is an initial step towards the ideal functionality. The full and flexible management for \u201cWhat metrics to collect and in what resolution\u201d is slated for future releases. External services monitoring Since PMM 1.4.0, users had the ability to monitor external services Percona didn\u2019t currently support (e.g., Redis). This blog article from 2018 nicely described external services monitoring at that time. (At that time Percona was not natively supporting a PostgreSQL monitoring service and so this was listed as an external service. Today, PostgreSQL is natively supported by PMM.) Until now, PMM 2.x didn\u2019t support external services monitoring. With this release, any non-natively supported by PMM service will now become supported with external services monitoring. You can see the list of possible exporters to be used in https://prometheus.io/docs/instrumenting/exporters/ . Natively-supported services will continue to deliver an expanded set of metrics and insights. Provide summary information for systems (pt-*-summary actions) With the addition of pt-*-summary in PMM 2, users can now view summary information about services and nodes on PMM\u2019s dashboard. This summary information is in the industry common format of pt-*-summary tools output to simplify portability of this data. This format will also be preserved in the snapshot of the dashboard shared with Percona Support to simplify investigations of issues. Note: pt-*-summary includes formats for: pt-mysql-summary pt-mongodb-summary pt-pg-summary pt-summary HAProxy support by PMM Users are able to add HAProxy Services for monitoring in PMM2. The support level of them in PMM will be the same we have for ProxySQL, so they will be presented in Inventory and on Dashboard. This will allow users who use HAProxy in their HA configuration to also have this component monitored by PMM. In future releases PMM will start use HAProxy by default for the DBaaS feature and will also use this functionality to monitor HAProxy. DBaaS Preview improvements (Technical Preview) From now you will be able to see the progress of internal steps the system makes when executing some operations with DBaaS. The Progress Bar will not be time-related and will present only steps. The Progress Bar component will also reflect the K8s/Operator-related errors to the user, so in the case of errors, you will have the error text on the UI, and no need to use K8s tools to see the error. With the same UI, you will be able to see the latest logs from K8s so they will have even more information about why the error happened. Known Limitations: The progress bar will not provide valuable information for the Delete operation (will be in a later version when we\u2019ll change the API with Operators Team), Operation of DB Cluster Modification will have \u201cstrange\u201d behavior and will start changes from non-zero values of steps. (This will be modified after API changes.)","title":"Release Highlights"},{"location":"release-notes/2.15.0.html#new-features","text":"PMM-4172 , PMM-4306 , PMM-5784 , PMM-7177 : Services and Nodes Summary presentation. Present information about DB\u2019s and Node status using pt-mysql-summary , pt-mongodb-summary , pt-pg-summary outputs (in API and on Dashboards). PMM-7123 : Ability to add External Services via the UI in PMM server. PMM-6711 : Add external-group flag for pmm-admin inventory commands for simpler work with External services. PMM-7405 : Check connection response format when adding External Service to monitoring. PMM-6797 : HAProxy monitoring: Ability to add HAProxy services with pmm-admin [inventory] add [service] haproxy command. PMM-7487 : HAProxy monitoring: Check connection to HAProxy services when adding them for monitoring. PMM-7496 : HAProxy monitoring: New HAProxy PXC dashboards. PMM-6943 : HAProxy monitoring: Show HAProxy type services in PMM Inventory. PMM-6924 : Integrated Alerting: Show \u2018breadcrumbs\u2019 navigation aid on non-dashboard pages as well as Grafana dashboard pages. PMM-7294 : Integrated Alerting: Pagination for viewing large numbers of Alert Rules. PMM-7417 : Security Threat Tool: Show list of all available security checks. PMM-7418 : Security Threat Tool: Ability to disable specific security checks. PMM-7419 : DBaaS: Ability to see DB Cluster creation/modification logs. PMM-7266 : DBaaS: Cluster creation progress bar \u2013 You can now see the progress of DBaaS DB cluster creation. (The progress bar is based on the number of back-end technical steps, not the time required to perform the tasks.)","title":"New Features"},{"location":"release-notes/2.15.0.html#improvements","text":"PMM-4679 : Docker: :latest tag for pmm-server and pmm-client images has been moved from v1 latest release to v2 latest release. Note : use of the latest tag is not recommended in production environments, instead use :2 tag. PMM-7472 : Remove Prometheus data source \u2013 If you were using custom dashboards with a specified data source (not using empty to use default one) you may need to edit your dashboards to use the proper data source. PMM is no longer using Prometheus but uses compatible storage for metrics from VictoriaMetrics. We renamed the data source to be more technology-agnostic. PMM-6695 : Software update: Grafana 7.1.3 to 7.3.7 (See What\u2019s new in Grafana 7.2 and What\u2019s new in Grafana 7.3 .) PMM-7471 : Software update: VictoriaMetrics 1.52.0 to 1.53.1 (See VictoriaMetrics 1.53.0 and VictoriaMetrics 1.53.1 .) PMM-6693 : API keys usage \u2013 PMM users can now use API keys (generated in Grafana UI) for interaction with PMM server instead of username/password pairs. The API key should have the same level of access (Admin or Viewer) as is required for username/password pairs. PMM-7240 : DBaaS: Change from Dashboard to Grafana Page \u2013 We changed the DBaaS page from a Grafana Dashboard to a Grafana Page to be better aligned with the DBaaS enable/disable status and avoid confusion when DBaaS is disabled. PMM-7328 : Security Threat Tool: Download and run checks when activated, immediately, repeating every 24 hours thereafter (Previously, downloading and running new checks happened every 24 hours but the cycle didn\u2019t begin when STT was activated.) PMM-7329 : Security Threat Tool: Hide check results tab if STT is disabled. PMM-7331 : Security Threat Tool: Failed checks have \u2018Read more\u2019 links with helpful content. PMM-7422 : Security Threat Tool: View all active and silenced alerts. PMM-7257 , PMM-7433 : Integrated Alerting: Easier-to-read rule details in Alert Rules list (API and UI presentation). PMM-7259 : Integrated Alerting: Better UI error reporting for disabled Integrated Alerting. (Hint to users how to enable it.) PMM-5533 : Better indentation of columns in pmm-admin list output. PMM-5888 : Improve pmm-admin --help descriptions for external services.","title":"Improvements"},{"location":"release-notes/2.15.0.html#bugs-fixed","text":"PMM-5837 : pmm-agent reports \u201cMalformed DSN\u201d error when adding PostgreSQL instance with a PMM user password containing = (equals sign) (Thanks to Alexandre Barth for reporting this issue). PMM-5969 : Removing Services or Nodes with pmm-admin ... --force mode does not stop running agents, VictoriaMetrics continues collecting data from exporters. PMM-6685 : In low screen resolutions Services submenu wraps, becomes obscured, and can\u2019t be accessed. PMM-6681 : Not all PMM admin users can download diagnostic logs, only those with Grafana admin rights. PMM-7227 : Table stats metrics not being collected in instances with millions of tables. PMM-7426 : vmagent continually restarts, blocking comms between pmm-agent & pmm-managed \u2013 Users running multiple services on the same PMM agent in \u2018push\u2019 mode could face this issue when restarting the agent after bulk-adding services. PMM-6636 : Dashboards: MySQL Replication Summary: \u2018Binlog Size\u2019, \u2018Binlog Data Written Hourly\u2019, \u2018Node\u2019 not being charted when the instance is RDS. PMM-7325 : Dashboards: MySQL User Details: user labels unreadable with high number (>20) of users (Thanks to Andrei Fedorov for reporting this issue). PMM-7416 : Dashboards: PostgreSQL Instance Summary: Some panels (e.g. Tuple) not using selected database. PMM-7235 : Integrated Alerting: Filtered out alerts are shown in the UI as firing. PMM-7324 : Integrated Alerting: Add Pager Duty Notification Channel: after user pastes copied key Add button is not enabled. PMM-7346 : Integrated Alerting: It is possible to create Alert Rule with negative duration time. PMM-7366 : Integrated Alerting: Entities (e.g. templates, channels, rules) are in inconsistent states. PMM-7467 : Integrated Alerting: < (less-than symbol) wrongly interpreted by Alert templates (as &lt; ). PMM-7591 : Integrated Alerting: User can not receive notifications on email after password update. PMM-7343 : Security Threat Tool: Check results show previously failed checks after STT reenabled. PMM-7250 : DBaaS: Confusing error \u201cCannot get PSMDB/PXC cluster\u201d appears after removing DB cluster. PMM-7193 : DBaaS: Number of Nodes can be set as float. PMM-7349 : DBaaS: Host and Password occasionally disappearing from Connection column.","title":"Bugs Fixed"},{"location":"release-notes/2.2.0.html","text":"Percona Monitoring and Management 2.2.0 Date: December 24, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. Main improvements in this release are: Alternative installation methods available for PMM 1.x are re-implemented for PMM 2: now PMM Server can be installed as a virtual appliance, or run using AWS Marketplace AWS RDS and remote instances monitoring re-added in this release include AWS RDS MySQL / Aurora MySQL instances, and remote PostgreSQL, MySQL, MongoDB, and ProxySQL ones The new Settings dashboard allows configuring PMM Server via the graphical interface For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the predefined set of ten most informative ones, to reduce load on PMM Server at its first open Fixed bugs PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats , on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown file system type","title":"Percona Monitoring and Management 2.2.0"},{"location":"release-notes/2.2.0.html#improvements-and-new-features","text":"PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the predefined set of ten most informative ones, to reduce load on PMM Server at its first open","title":"Improvements and new features"},{"location":"release-notes/2.2.0.html#fixed-bugs","text":"PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats , on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown file system type","title":"Fixed bugs"},{"location":"release-notes/2.2.1.html","text":"Percona Monitoring and Management 2.2.1 Date: January 23, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. PMM Server version 2.2.0 suffered an unauthenticated denial of service vulnerability (CVE-2020-7920). Any other PMM versions do not carry the same code logic, and are thus unaffected by this issue. Users who have already deployed PMM Server 2.2.0 are advised to upgrade to version 2.2.1 which resolves this issue. Improvements and new features PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels Fixed bugs PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM 2.x OVF Image memory size was increased from 1 Gb to 4 Gb with the additional 1 Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : InnoDB Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"Percona Monitoring and Management 2.2.1"},{"location":"release-notes/2.2.1.html#improvements-and-new-features","text":"PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels","title":"Improvements and new features"},{"location":"release-notes/2.2.1.html#fixed-bugs","text":"PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM 2.x OVF Image memory size was increased from 1 Gb to 4 Gb with the additional 1 Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : InnoDB Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"Fixed bugs"},{"location":"release-notes/2.2.2.html","text":"Percona Monitoring and Management 2.2.2 Date: February 4, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30 to 40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server Fixed bugs PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Query Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"Percona Monitoring and Management 2.2.2"},{"location":"release-notes/2.2.2.html#improvements-and-new-features","text":"PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30 to 40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server","title":"Improvements and new features"},{"location":"release-notes/2.2.2.html#fixed-bugs","text":"PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Query Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"Fixed bugs"},{"location":"release-notes/2.3.0.html","text":"Percona Monitoring and Management 2.3.0 Date: February 19, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server Note This feature is for advanced users only at this point PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors Fixed bugs PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats . PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scroll bars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"Percona Monitoring and Management 2.3.0"},{"location":"release-notes/2.3.0.html#improvements-and-new-features","text":"PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server Note This feature is for advanced users only at this point PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors","title":"Improvements and new features"},{"location":"release-notes/2.3.0.html#fixed-bugs","text":"PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats . PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scroll bars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"Fixed bugs"},{"location":"release-notes/2.4.0.html","text":"Percona Monitoring and Management 2.4.0 Date: March 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013-pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon . The new dashboard is based on the hwmon collector data from the node_exporter . Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters. Improvements PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : ClickHouse query optimized for Query Analytics to improve its speed and reduce the load on the back-end PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A health check indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be used as follows: docker inspect -f {{ .State.Health.Status }} until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2 hours of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is a UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate client subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type Bugs Fixed PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.4.0"},{"location":"release-notes/2.4.0.html#new-features","text":"PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013-pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon . The new dashboard is based on the hwmon collector data from the node_exporter . Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters.","title":"New Features"},{"location":"release-notes/2.4.0.html#improvements","text":"PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : ClickHouse query optimized for Query Analytics to improve its speed and reduce the load on the back-end PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A health check indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be used as follows: docker inspect -f {{ .State.Health.Status }} until [ \"`docker inspect -f {{.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2 hours of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is a UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate client subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type","title":"Improvements"},{"location":"release-notes/2.4.0.html#bugs-fixed","text":"PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.5.0.html","text":"Percona Monitoring and Management 2.5.0 Date: April 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics. Improvements PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved. Bugs Fixed PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.5.0"},{"location":"release-notes/2.5.0.html#new-features","text":"PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics.","title":"New Features"},{"location":"release-notes/2.5.0.html#improvements","text":"PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved.","title":"Improvements"},{"location":"release-notes/2.5.0.html#bugs-fixed","text":"PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.0.html","text":"Percona Monitoring and Management 2.6.0 Date: May 11, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where PMM agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones. Improvements PMM-1962 : The PMM Server API (via /v1/readyz ) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, 6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : mongodb_exporter is now the latest version, 0.11.0. PMM-5734 : Temporary files activity and utilization charts ( rate & irate ) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013-socket option incorrectly. Bugs Fixed PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the connect-direct parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats , causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB . PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning NaN results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.6.0"},{"location":"release-notes/2.6.0.html#new-features","text":"PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where PMM agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones.","title":"New Features"},{"location":"release-notes/2.6.0.html#improvements","text":"PMM-1962 : The PMM Server API (via /v1/readyz ) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, 6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : mongodb_exporter is now the latest version, 0.11.0. PMM-5734 : Temporary files activity and utilization charts ( rate & irate ) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013-socket option incorrectly.","title":"Improvements"},{"location":"release-notes/2.6.0.html#bugs-fixed","text":"PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the connect-direct parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats , causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB . PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning NaN results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.1.html","text":"Percona Monitoring and Management 2.6.1 Date: May 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019 Bugs Fixed PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"Percona Monitoring and Management 2.6.1"},{"location":"release-notes/2.6.1.html#improvements","text":"PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019","title":"Improvements"},{"location":"release-notes/2.6.1.html#bugs-fixed","text":"PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"Bugs Fixed"},{"location":"release-notes/2.7.0.html","text":"Percona Monitoring and Management 2.7.0 Date: June 9, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. In this release, we have updated Grafana to version 6.7.4 to fix CVE-2020-13379 . We recommend updating to the latest version of PMM as soon as possible. New Features PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option ( \u2013-socket ) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances Improvements PMM-2244 : pmm-admin status command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated Bugs Fixed PMM-6004 : MySQL exporter reporting wrong values for cluster status ( wsrep_cluster_status ) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading Grafana to 6.7.4","title":"Percona Monitoring and Management 2.7.0"},{"location":"release-notes/2.7.0.html#new-features","text":"PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option ( \u2013-socket ) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances","title":"New Features"},{"location":"release-notes/2.7.0.html#improvements","text":"PMM-2244 : pmm-admin status command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated","title":"Improvements"},{"location":"release-notes/2.7.0.html#bugs-fixed","text":"PMM-6004 : MySQL exporter reporting wrong values for cluster status ( wsrep_cluster_status ) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading Grafana to 6.7.4","title":"Bugs Fixed"},{"location":"release-notes/2.8.0.html","text":"Percona Monitoring and Management 2.8.0 Date: June 25, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade Bugs Fixed PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"Percona Monitoring and Management 2.8.0"},{"location":"release-notes/2.8.0.html#improvements","text":"PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade","title":"Improvements"},{"location":"release-notes/2.8.0.html#bugs-fixed","text":"PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"Bugs Fixed"},{"location":"release-notes/2.9.0.html","text":"Percona Monitoring and Management 2.9.0 Date: July 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Highlights This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ). New Features PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details ( INFORMATION_SCHEMA.CLIENT_STATISTICS ) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.) Improvements PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions Bugs Fixed PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken ( relabel_configs un-marshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unit-less value \u2018Top I/O Load\u2019","title":"Percona Monitoring and Management 2.9.0"},{"location":"release-notes/2.9.0.html#highlights","text":"This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ).","title":"Highlights"},{"location":"release-notes/2.9.0.html#new-features","text":"PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details ( INFORMATION_SCHEMA.CLIENT_STATISTICS ) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.)","title":"New Features"},{"location":"release-notes/2.9.0.html#improvements","text":"PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions","title":"Improvements"},{"location":"release-notes/2.9.0.html#bugs-fixed","text":"PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken ( relabel_configs un-marshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unit-less value \u2018Top I/O Load\u2019","title":"Bugs Fixed"},{"location":"release-notes/2.9.1.html","text":"Percona Monitoring and Management 2.9.1 Date: August 4, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but un-actioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing Bugs Fixed PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: InvalidNamespace EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tool-tip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics drop-down list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"Percona Monitoring and Management 2.9.1"},{"location":"release-notes/2.9.1.html#improvements","text":"PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but un-actioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing","title":"Improvements"},{"location":"release-notes/2.9.1.html#bugs-fixed","text":"PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: InvalidNamespace EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tool-tip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics drop-down list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"Bugs Fixed"},{"location":"setting-up/index.html","text":"Setting up The PMM setting-up process can be broken into three key stages: Setting up at least one PMM Server Setting up one or more PMM Clients Configuring and adding services for monitoring 1. Setting up PMM Server You must set up at least one PMM Server. A server can run as: a Docker container a virtual appliance an Amazon AWS EC2 instance 2. Setting up PMM Client You must set up PMM Client on each node where there is a service to be monitored. You can do this: with a package manager ( apt , apt-get , dnf , yum ) by manually downloading and installing .deb or .rpm packages by manually downloading and unpacking a binary package ( .tar.gz ) with a Docker image 3. Configure and add services You must configure your services and add them to PMM Server\u2019s inventory of monitored systems. This is different for each type of service: MySQL and variants (Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External services HAProxy You do this on each node/service being monitored. If you have configured everything correctly, you\u2019ll see data in the PMM user interface, in one of the dashboards specific to the type of service.","title":"Setting up"},{"location":"setting-up/index.html#setting-up-pmm-server","text":"You must set up at least one PMM Server. A server can run as: a Docker container a virtual appliance an Amazon AWS EC2 instance","title":"1. Setting up PMM Server"},{"location":"setting-up/index.html#setting-up-pmm-client","text":"You must set up PMM Client on each node where there is a service to be monitored. You can do this: with a package manager ( apt , apt-get , dnf , yum ) by manually downloading and installing .deb or .rpm packages by manually downloading and unpacking a binary package ( .tar.gz ) with a Docker image","title":"2. Setting up PMM Client"},{"location":"setting-up/index.html#configure-add-services","text":"You must configure your services and add them to PMM Server\u2019s inventory of monitored systems. This is different for each type of service: MySQL and variants (Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External services HAProxy You do this on each node/service being monitored. If you have configured everything correctly, you\u2019ll see data in the PMM user interface, in one of the dashboards specific to the type of service.","title":"3. Configure and add services"},{"location":"setting-up/client/index.html","text":"Setting up PMM Client PMM Client is a collection of agents and exporters that run on the host being monitored. These sections cover the different ways to install PMM Client on a Linux node and register it with PMM Server. The options are: For Debian- or Red Hat-based distributions, install percona-release and use a Linux package manager ( apt / dnf ) to install PMM Client. For Debian- or Red Hat-based distributions, download .deb / .rpm PMM Client packages and install them . For other Linux distributions, download and unpack generic PMM Client Linux binaries . If you use Docker , run PMM Client as a Docker container . When you have installed PMM Client, you must: Register the node with PMM Server Configure and add services according to type Before you start PMM Server is installed and running with a known IP address accessible from the client node. You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. These Linux packages are installed: curl , gnupg , sudo , wget . Install PMM Client with a package manager Install on Debian-based distributions Configure repositories. wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb Install the PMM Client package. sudo apt update sudo apt install -y pmm2-client Install on Red Hat-based distributions Configure repositories. sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm Install the PMM Client package. sudo yum install -y pmm2-client Tip If you have used percona-release before, disable and reenable the repository like this: sudo percona-release disable all sudo percona-release enable original release Download and install PMM Client packages manually Visit the Percona Monitoring and Management 2 download page. Under Version: , select the one you want (usually the latest). Under Software: , select the item matching your software platform. Click to download the package file: For Debian, Ubuntu: .deb For Red Hat, CentOS, Oracle Linux: .rpm (Alternatively, copy the link and use wget to download it.) Here are the download page links for each supported platform. Debian 9 (\u201cStretch\u201d) Debian 10 (\u201cBuster\u201d) Red Hat/CentOS/Oracle 7 Red Hat/CentOS/Oracle 8 Ubuntu 16.04 (\u201cXenial Xerus\u201d) Ubuntu 18.04 (\u201cBionic Beaver\u201d) Ubuntu 20.04 (\u201cFocal Fossa\u201d) Install on Debian-based distributions sudo dpkg -i *.deb Install on Red Hat-based distributions sudo dnf localinstall *.rpm Download and unpack generic Linux binary package Download the PMM Client package: sudo wget https://downloads.percona.com/downloads/pmm2/2.15.0/binary/tarball/pmm2-client-2.15.0.tar.gz Download the PMM Client package checksum file: sudo wget https://downloads.percona.com/downloads/pmm2/2.15.0/binary/tarball/pmm2-client-2.15.0.tar.gz.sha256sum Verify the download. sha256sum -c pmm2-client-2.15.0.tar.gz.sha256sum Unpack the package and move into the directory. sudo tar xfz pmm2-client-2.15.0.tar.gz && cd pmm2-client-2.15.0 Run the installer. sudo ./install_tarball Change the path. PATH = $PATH :/usr/local/percona/pmm2/bin Set up the agent sudo pmm-agent setup --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin Open a new terminal and run the agent. PATH = $PATH :/usr/local/percona/pmm2/bin pmm-agent --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml In the first terminal, check. pmm-admin status Run PMM Client as a Docker container The PMM Client Docker image is a convenient way to run PMM Client as a preconfigured Docker container. Pull the PMM Client docker image. docker pull \\ percona/pmm-client:2 Use the image as a template to create a persistent data store that preserves local data when the image is updated. docker create \\ --volume /srv \\ --name pmm-client-data \\ percona/pmm-client:2 /bin/true Run the container to start PMM Agent in setup mode. Set X.X.X.X to the IP address of your PMM Server. (Do not use the docker --detach option as PMM agent only logs to the console.) PMM_SERVER = X.X.X.X:443 docker run \\ --rm \\ --name pmm-client \\ -e PMM_AGENT_SERVER_ADDRESS = ${ PMM_SERVER } \\ -e PMM_AGENT_SERVER_USERNAME = admin \\ -e PMM_AGENT_SERVER_PASSWORD = admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS = 1 \\ -e PMM_AGENT_SETUP = 1 \\ -e PMM_AGENT_CONFIG_FILE = pmm-agent.yml \\ --volumes-from pmm-client-data \\ percona/pmm-client:2 Check status. docker exec pmm-client \\ pmm-admin status In the PMM user interface you will also see an increase in the number of monitored nodes. You can now add services with pmm-admin by prefixing commands with docker exec pmm-client . Tips Adjust host firewall and routing rules to allow Docker communications. ( Read more in the FAQ. ) For help: docker run --rm percona/pmm-client:2 --help Register node with PMM Server Register your node ( X.X.X.X is the IP address of your PMM Server). pmm-admin config --server-insecure-tls --server-url = https://admin:admin@X.X.X.X:443 X.X.X.X is the address of your PMM Server. 443 is the default port number. admin / admin is the default PMM username and password. This is the same account you use to log into the PMM user interface, which you had the option to change when first logging in. Configure and add services You should continue by adding services according to the service type. MySQL and variants (Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External services HAProxy See also Percona release","title":"Client"},{"location":"setting-up/client/index.html#before-you-start","text":"PMM Server is installed and running with a known IP address accessible from the client node. You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. These Linux packages are installed: curl , gnupg , sudo , wget .","title":"Before you start"},{"location":"setting-up/client/index.html#package-manager","text":"","title":"Install PMM Client with a package manager"},{"location":"setting-up/client/index.html#install-on-debian-based-distributions","text":"Configure repositories. wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb Install the PMM Client package. sudo apt update sudo apt install -y pmm2-client","title":"Install on Debian-based distributions"},{"location":"setting-up/client/index.html#install-on-red-hat-based-distributions","text":"Configure repositories. sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm Install the PMM Client package. sudo yum install -y pmm2-client Tip If you have used percona-release before, disable and reenable the repository like this: sudo percona-release disable all sudo percona-release enable original release","title":"Install on Red Hat-based distributions"},{"location":"setting-up/client/index.html#manual-package","text":"Visit the Percona Monitoring and Management 2 download page. Under Version: , select the one you want (usually the latest). Under Software: , select the item matching your software platform. Click to download the package file: For Debian, Ubuntu: .deb For Red Hat, CentOS, Oracle Linux: .rpm (Alternatively, copy the link and use wget to download it.) Here are the download page links for each supported platform. Debian 9 (\u201cStretch\u201d) Debian 10 (\u201cBuster\u201d) Red Hat/CentOS/Oracle 7 Red Hat/CentOS/Oracle 8 Ubuntu 16.04 (\u201cXenial Xerus\u201d) Ubuntu 18.04 (\u201cBionic Beaver\u201d) Ubuntu 20.04 (\u201cFocal Fossa\u201d)","title":"Download and install PMM Client packages manually"},{"location":"setting-up/client/index.html#install-on-debian-based-distributions_1","text":"sudo dpkg -i *.deb","title":"Install on Debian-based distributions"},{"location":"setting-up/client/index.html#install-on-red-hat-based-distributions_1","text":"sudo dnf localinstall *.rpm","title":"Install on Red Hat-based distributions"},{"location":"setting-up/client/index.html#binary-package","text":"Download the PMM Client package: sudo wget https://downloads.percona.com/downloads/pmm2/2.15.0/binary/tarball/pmm2-client-2.15.0.tar.gz Download the PMM Client package checksum file: sudo wget https://downloads.percona.com/downloads/pmm2/2.15.0/binary/tarball/pmm2-client-2.15.0.tar.gz.sha256sum Verify the download. sha256sum -c pmm2-client-2.15.0.tar.gz.sha256sum Unpack the package and move into the directory. sudo tar xfz pmm2-client-2.15.0.tar.gz && cd pmm2-client-2.15.0 Run the installer. sudo ./install_tarball Change the path. PATH = $PATH :/usr/local/percona/pmm2/bin Set up the agent sudo pmm-agent setup --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml --server-address = 192 .168.1.123 --server-insecure-tls --server-username = admin --server-password = admin Open a new terminal and run the agent. PATH = $PATH :/usr/local/percona/pmm2/bin pmm-agent --config-file = /usr/local/percona/pmm2/config/pmm-agent.yaml In the first terminal, check. pmm-admin status","title":"Download and unpack generic Linux binary package"},{"location":"setting-up/client/index.html#docker","text":"The PMM Client Docker image is a convenient way to run PMM Client as a preconfigured Docker container. Pull the PMM Client docker image. docker pull \\ percona/pmm-client:2 Use the image as a template to create a persistent data store that preserves local data when the image is updated. docker create \\ --volume /srv \\ --name pmm-client-data \\ percona/pmm-client:2 /bin/true Run the container to start PMM Agent in setup mode. Set X.X.X.X to the IP address of your PMM Server. (Do not use the docker --detach option as PMM agent only logs to the console.) PMM_SERVER = X.X.X.X:443 docker run \\ --rm \\ --name pmm-client \\ -e PMM_AGENT_SERVER_ADDRESS = ${ PMM_SERVER } \\ -e PMM_AGENT_SERVER_USERNAME = admin \\ -e PMM_AGENT_SERVER_PASSWORD = admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS = 1 \\ -e PMM_AGENT_SETUP = 1 \\ -e PMM_AGENT_CONFIG_FILE = pmm-agent.yml \\ --volumes-from pmm-client-data \\ percona/pmm-client:2 Check status. docker exec pmm-client \\ pmm-admin status In the PMM user interface you will also see an increase in the number of monitored nodes. You can now add services with pmm-admin by prefixing commands with docker exec pmm-client . Tips Adjust host firewall and routing rules to allow Docker communications. ( Read more in the FAQ. ) For help: docker run --rm percona/pmm-client:2 --help","title":"Run PMM Client as a Docker container"},{"location":"setting-up/client/index.html#register","text":"Register your node ( X.X.X.X is the IP address of your PMM Server). pmm-admin config --server-insecure-tls --server-url = https://admin:admin@X.X.X.X:443 X.X.X.X is the address of your PMM Server. 443 is the default port number. admin / admin is the default PMM username and password. This is the same account you use to log into the PMM user interface, which you had the option to change when first logging in.","title":"Register node with PMM Server"},{"location":"setting-up/client/index.html#configure-add-services","text":"You should continue by adding services according to the service type. MySQL and variants (Percona Server for MySQL, Percona XtraDB Cluster, MariaDB) MongoDB PostgreSQL ProxySQL Amazon RDS Microsoft Azure Linux External services HAProxy See also Percona release","title":"Configure and add services"},{"location":"setting-up/client/aws.html","text":"Amazon RDS Required settings It is possible to use PMM for monitoring Amazon RDS (just like any remote MySQL instance). In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance. Creating an IAM user with permission to access Amazon RDS DB instances It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose. Creating a policy A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button. Creating an IAM user Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user . Creating an access key for an IAM user To discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. Note You may use an IAM role instead of IAM user provided your Amazon RDS DB instances are associated with the same AWS account as PMM. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances. Attaching a policy to an IAM user The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a check-box next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user. Setting up the Amazon RDS DB Instance Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Warning Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; If you have Amazon RDS with a MySQL version prior to 5.5, REPLICATION CLIENT privilege is not available there and has to be excluded from the above statement. Note General system metrics are monitored by using the rds_exporter exporter which replaces node_exporter . rds_exporter gives access to Amazon Cloudwatch metrics. node_exporter , used in versions of PMM prior to 1.8.0, was not able to monitor general system metrics remotely. Adding an Amazon RDS MySQL, Aurora MySQL or Remote Instance The preferred method of adding an Amazon RDS database instance to PMM is via the PMM \u2192 PMM Add Instance menu option. This method supports Amazon RDS database instances that use Amazon Aurora, MySQL, or MariaDB engines, as well as any remote PostgreSQL, ProxySQL, MySQL and MongoDB instances. The following steps are needed to add an Amazon RDS database instance to PMM: In the PMM web interface, go to PMM > PMM Add Instance . Select AWS RDS MySQL or Aurora MySQL \u2013 Add a remote instance . Enter the access key ID and the secret access key of your IAM user. Click the Discover button for PMM to retrieve the available Amazon RDS instances. For the instance that you would like to monitor, select the Start monitoring button. You will see a new page with the number of fields. The list is divided into the following groups: Main details , RDS database , Labels , and Additional options . Some already known data, such as already entered AWS access key , are filled in automatically, and some fields are optional. The Main details section allows you to specify the DNS hostname of your instance, the service name to use within PMM, the port your service is listening on, and the database user name and password. The Labels section allows you to specify labels for the environment, the AWS region and availability zone to be used, the Replication set and Cluster names and also it allows you to set the list of custom labels in a key:value format. The Additional options section contains specific flags which allow you to tune the RDS monitoring. They can allow you to skip connection check, to use TLS for the database connection, not to validate the TLS certificate and the hostname, as well as to disable basic and/or enhanced metrics collection for the RDS instance to reduce costs. Also this section contains a database-specific flag, which would allow Query Analytics for the selected remote database: when adding some remote MySQL, AWS RDS MySQL or Aurora MySQL instance, you will be able to choose using performance schema for the database monitoring; when adding a PostgreSQL instance, you will be able to activate using pg_stat_statements extension; when adding a MongoDB instance, you will be able to choose using Query Analytics MongoDB profiler. Finally press the Add service button to start monitoring your instance. Adding an Amazon RDS PostgreSQL instance For PostgreSQL, use the same method described above. In the PMM web interface, go to PMM > PMM Add Instance . Select AWS RDS MySQL or Aurora MySQL \u2013 Add a remote instance . At the moment of writing this guide, the Add button doesn\u2019t mention PostgreSQL but the discovery function already supports it. Follow steps 4 to 6 as in the previous section. Fill the form and remember to select PG Stat Statement to enable Query Analytics. To get queries for Query Analytics, you need to enable pg_stat_statements in your instance by running: CREATE EXTENSION pg_stat_statements SCHEMA public;","title":"Amazon RDS"},{"location":"setting-up/client/aws.html#required-settings","text":"It is possible to use PMM for monitoring Amazon RDS (just like any remote MySQL instance). In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance.","title":"Required settings"},{"location":"setting-up/client/aws.html#creating-an-iam-user-with-permission-to-access-amazon-rds-db-instances","text":"It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose.","title":"Creating an IAM user with permission to access Amazon RDS DB instances"},{"location":"setting-up/client/aws.html#creating-a-policy","text":"A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button.","title":"Creating a policy"},{"location":"setting-up/client/aws.html#creating-an-iam-user","text":"Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user .","title":"Creating an IAM user"},{"location":"setting-up/client/aws.html#creating-an-access-key-for-an-iam-user","text":"To discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. Note You may use an IAM role instead of IAM user provided your Amazon RDS DB instances are associated with the same AWS account as PMM. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances.","title":"Creating an access key for an IAM user"},{"location":"setting-up/client/aws.html#attaching-a-policy-to-an-iam-user","text":"The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a check-box next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user.","title":"Attaching a policy to an IAM user"},{"location":"setting-up/client/aws.html#setting-up-the-amazon-rds-db-instance","text":"Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Warning Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; If you have Amazon RDS with a MySQL version prior to 5.5, REPLICATION CLIENT privilege is not available there and has to be excluded from the above statement. Note General system metrics are monitored by using the rds_exporter exporter which replaces node_exporter . rds_exporter gives access to Amazon Cloudwatch metrics. node_exporter , used in versions of PMM prior to 1.8.0, was not able to monitor general system metrics remotely.","title":"Setting up the Amazon RDS DB Instance"},{"location":"setting-up/client/aws.html#adding-an-amazon-rds-mysql-aurora-mysql-or-remote-instance","text":"The preferred method of adding an Amazon RDS database instance to PMM is via the PMM \u2192 PMM Add Instance menu option. This method supports Amazon RDS database instances that use Amazon Aurora, MySQL, or MariaDB engines, as well as any remote PostgreSQL, ProxySQL, MySQL and MongoDB instances. The following steps are needed to add an Amazon RDS database instance to PMM: In the PMM web interface, go to PMM > PMM Add Instance . Select AWS RDS MySQL or Aurora MySQL \u2013 Add a remote instance . Enter the access key ID and the secret access key of your IAM user. Click the Discover button for PMM to retrieve the available Amazon RDS instances. For the instance that you would like to monitor, select the Start monitoring button. You will see a new page with the number of fields. The list is divided into the following groups: Main details , RDS database , Labels , and Additional options . Some already known data, such as already entered AWS access key , are filled in automatically, and some fields are optional. The Main details section allows you to specify the DNS hostname of your instance, the service name to use within PMM, the port your service is listening on, and the database user name and password. The Labels section allows you to specify labels for the environment, the AWS region and availability zone to be used, the Replication set and Cluster names and also it allows you to set the list of custom labels in a key:value format. The Additional options section contains specific flags which allow you to tune the RDS monitoring. They can allow you to skip connection check, to use TLS for the database connection, not to validate the TLS certificate and the hostname, as well as to disable basic and/or enhanced metrics collection for the RDS instance to reduce costs. Also this section contains a database-specific flag, which would allow Query Analytics for the selected remote database: when adding some remote MySQL, AWS RDS MySQL or Aurora MySQL instance, you will be able to choose using performance schema for the database monitoring; when adding a PostgreSQL instance, you will be able to activate using pg_stat_statements extension; when adding a MongoDB instance, you will be able to choose using Query Analytics MongoDB profiler. Finally press the Add service button to start monitoring your instance.","title":"Adding an Amazon RDS MySQL, Aurora MySQL or Remote Instance"},{"location":"setting-up/client/aws.html#adding-an-amazon-rds-postgresql-instance","text":"For PostgreSQL, use the same method described above. In the PMM web interface, go to PMM > PMM Add Instance . Select AWS RDS MySQL or Aurora MySQL \u2013 Add a remote instance . At the moment of writing this guide, the Add button doesn\u2019t mention PostgreSQL but the discovery function already supports it. Follow steps 4 to 6 as in the previous section. Fill the form and remember to select PG Stat Statement to enable Query Analytics. To get queries for Query Analytics, you need to enable pg_stat_statements in your instance by running: CREATE EXTENSION pg_stat_statements SCHEMA public;","title":"Adding an Amazon RDS PostgreSQL instance"},{"location":"setting-up/client/azure.html","text":"Microsoft Azure Required settings It is possible to use PMM for monitoring Azure database instances like other remote instances. In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Azure DB instance. Discovery is not yet implemented in PMM but it is possible to add known instances by providing the connection parameters. First of all, ensure that there is the minimal latency between PMM Server and the Azure instance. Second, add a firewall rule to enable access from PMM Client like this: Setting up a MySQL instance Query Analytics requires you to configure Performance Schema as the query source, because the slow query log is stored on the Azure side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. When adding a monitoring instance for Azure, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; Adding an Azure Instance Follow the instructions for remotes instances explained here . Example: and be sure to set Performance Schema as the query collection method for Query Analytics. MariaDB. MariaDB up to version 10.2 works out of the box but starting with MariaDB 10.3 instrumentation is disabled by default and cannot be enabled since there is no SUPER role in Azure-MariaDB. So, it is not possible to run the required queries to enable instrumentation. Monitoring will work but Query Analytics won\u2019t receive any query data. PostgreSQL For PostgreSQL follow the same methods used for MySQL and MariaDB and enable track_io_timing in the instance configuration to enable Query Analytics.","title":"Microsoft Azure"},{"location":"setting-up/client/azure.html#required-settings","text":"It is possible to use PMM for monitoring Azure database instances like other remote instances. In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Azure DB instance. Discovery is not yet implemented in PMM but it is possible to add known instances by providing the connection parameters. First of all, ensure that there is the minimal latency between PMM Server and the Azure instance. Second, add a firewall rule to enable access from PMM Client like this:","title":"Required settings"},{"location":"setting-up/client/azure.html#setting-up-a-mysql-instance","text":"Query Analytics requires you to configure Performance Schema as the query source, because the slow query log is stored on the Azure side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. When adding a monitoring instance for Azure, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ;","title":"Setting up a MySQL instance"},{"location":"setting-up/client/external.html","text":"External Services Adding general external services You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command. Usage pmm-admin add external --service-name = <service-name> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> pmm-admin add external-serverless --external-name = <external-service-name> --host = <hostname> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> Getting data from external exporters There two ways to get metrics from other exporters: external will collect metrics from the exporter that is run on the same host as PMM Client\u2019s connection to it by a port. (See more details with pmm-admin add external --help .) external-serverless is useful for collecting metrics from cloud services. You need a host and port number to add it to PMM Server. (See more details with pmm-admin add external-serverless --help .) Here are the differences between external and external-serverless types. Connection schema of external exporter: Connection schema of external serverless exporter: How I can add something not supported by PMM PMM can collect any metrics in Open metrics or Prometheus exposition format. You must specify the host and port of these metrics using the pmm-admin add external or pmm-admin add external-serverless commands. From this point, PMM will collect and store available metrics. To browse and visualize collected metrics as a first step, we can look at the Advanced Data Exploration dashboard and select informative services and metrics. Another way is to create a new Grafana Dashboard to PMM as needed . One more way is to search for an already created dashboard at https://grafana.com/grafana/dashboards for the added exporter and import it into PMM. Third-party exporters You can find more exporters on the official Prometheus page . Custom exporter You can write a custom external exporter or extend your application to expose metrics in Prometheus format. Please see more details here: https://prometheus.io/docs/instrumenting/writing_exporters/","title":"External Services"},{"location":"setting-up/client/external.html#adding-general-external-services","text":"You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command.","title":"Adding general external services"},{"location":"setting-up/client/external.html#usage","text":"pmm-admin add external --service-name = <service-name> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme> pmm-admin add external-serverless --external-name = <external-service-name> --host = <hostname> --listen-port = <listen-port> --metrics-path = <metrics-path> --scheme = <scheme>","title":"Usage"},{"location":"setting-up/client/external.html#getting-data-from-external-exporters","text":"There two ways to get metrics from other exporters: external will collect metrics from the exporter that is run on the same host as PMM Client\u2019s connection to it by a port. (See more details with pmm-admin add external --help .) external-serverless is useful for collecting metrics from cloud services. You need a host and port number to add it to PMM Server. (See more details with pmm-admin add external-serverless --help .) Here are the differences between external and external-serverless types. Connection schema of external exporter: Connection schema of external serverless exporter:","title":"Getting data from external exporters"},{"location":"setting-up/client/external.html#how-i-can-add-something-not-supported-by-pmm","text":"PMM can collect any metrics in Open metrics or Prometheus exposition format. You must specify the host and port of these metrics using the pmm-admin add external or pmm-admin add external-serverless commands. From this point, PMM will collect and store available metrics. To browse and visualize collected metrics as a first step, we can look at the Advanced Data Exploration dashboard and select informative services and metrics. Another way is to create a new Grafana Dashboard to PMM as needed . One more way is to search for an already created dashboard at https://grafana.com/grafana/dashboards for the added exporter and import it into PMM.","title":"How I can add something not supported by PMM"},{"location":"setting-up/client/external.html#third-party-exporters","text":"You can find more exporters on the official Prometheus page .","title":"Third-party exporters"},{"location":"setting-up/client/external.html#custom-exporter","text":"You can write a custom external exporter or extend your application to expose metrics in Prometheus format. Please see more details here: https://prometheus.io/docs/instrumenting/writing_exporters/","title":"Custom exporter"},{"location":"setting-up/client/haproxy.html","text":"HAProxy Adding HAProxy services You can collect metrics from HAProxy on a node when: there is already configured haproxy How to configure HAProxy https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint After HAProxy is running (default address http://localhost:8404/metrics) you can add it to PMM. Use the haproxy alias to enable HAProxy metrics monitoring. there is already a PMM Agent instance running this node has been configured using the pmm-admin config command. USAGE pmm-admin add haproxy --listen-port = 8404 where listen-port is the port number where HAProxy running. (This is the only required flag.) The output of this command should look as follows: HAProxy Service added. Service ID : /service_id/c481183f-70a2-443f-91e5-cae5cecd06a2 Service name: Ubuntu-haproxy Additionally, one positional argument can be appended to the command line flags: a service name to be used by PMM. If not specified, they are substituted automatically as <node>-haproxy . During adding here is connection check (can be skipped by flag --skip-connection-check ). If HAProxy doesn\u2019t run properly on the given port then you will see an error message: Connection check failed: Get \"http://127.0.0.1:8404/metrics\": dial tcp 127.0.0.1:8404: connect: connection refused. Beside positional argument shown above you can specify service name with the following flags: --username , --password , --metrics-path (path for scraping metrics, default: /metrics) and --scheme (http or https). Here are some examples: pmm-admin add haproxy --listen-port = 8404 --username = pmm --password = pmm new-haproxy pmm-admin add haproxy --listen-port = 8404 --metrics-path = /prom-metrics --scheme = https Here you can check list of all available flags: pmm-admin You can also add HAProxy by UI in Grafana. Add instance. HAProxy data is visible in the Advanced Data Exploration dashboard:","title":"HAProxy"},{"location":"setting-up/client/haproxy.html#adding-haproxy-services","text":"You can collect metrics from HAProxy on a node when: there is already configured haproxy How to configure HAProxy https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint After HAProxy is running (default address http://localhost:8404/metrics) you can add it to PMM. Use the haproxy alias to enable HAProxy metrics monitoring. there is already a PMM Agent instance running this node has been configured using the pmm-admin config command.","title":"Adding HAProxy services"},{"location":"setting-up/client/haproxy.html#usage","text":"pmm-admin add haproxy --listen-port = 8404 where listen-port is the port number where HAProxy running. (This is the only required flag.) The output of this command should look as follows: HAProxy Service added. Service ID : /service_id/c481183f-70a2-443f-91e5-cae5cecd06a2 Service name: Ubuntu-haproxy Additionally, one positional argument can be appended to the command line flags: a service name to be used by PMM. If not specified, they are substituted automatically as <node>-haproxy . During adding here is connection check (can be skipped by flag --skip-connection-check ). If HAProxy doesn\u2019t run properly on the given port then you will see an error message: Connection check failed: Get \"http://127.0.0.1:8404/metrics\": dial tcp 127.0.0.1:8404: connect: connection refused. Beside positional argument shown above you can specify service name with the following flags: --username , --password , --metrics-path (path for scraping metrics, default: /metrics) and --scheme (http or https). Here are some examples: pmm-admin add haproxy --listen-port = 8404 --username = pmm --password = pmm new-haproxy pmm-admin add haproxy --listen-port = 8404 --metrics-path = /prom-metrics --scheme = https Here you can check list of all available flags: pmm-admin You can also add HAProxy by UI in Grafana. Add instance. HAProxy data is visible in the Advanced Data Exploration dashboard:","title":"USAGE"},{"location":"setting-up/client/linux.html","text":"Linux Adding general system metrics service PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Linux"},{"location":"setting-up/client/linux.html#adding-general-system-metrics-service","text":"PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Adding general system metrics service"},{"location":"setting-up/client/mongodb.html","text":"MongoDB Configuring MongoDB for Monitoring in PMM Query Analytics In Query Analytics, you can monitor MongoDB metrics and queries. Run the pmm-admin add command to use these monitoring services. Supported versions of MongoDB Query Analytics supports MongoDB version 3.2 or higher. Setting Up the Required Permissions For MongoDB monitoring services to work in Query Analytics, you need to set up the mongodb_exporter user. Here is an example for the MongoDB shell that creates and assigns the appropriate roles to the user. db . createRole ({ role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collection : \"\" }, actions : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles : [] }) db . getSiblingDB ( \"admin\" ). createUser ({ user : \"mongodb_exporter\" , pwd : \"s3cR#tpa$$worD\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] }) Enabling Profiling For MongoDB to work correctly with Query Analytics, you need to enable profiling in your mongod configuration. When started without profiling enabled, Query Analytics displays the following warning: Note A warning message is displayed when profiling is not enabled It is required that profiling of the monitored MongoDB databases be enabled, however profiling is not enabled by default because it may reduce the performance of your MongoDB server. Enabling Profiling on Command Line You can enable profiling from command line when you start the mongod server. This command is useful if you start mongod manually. Run this command as root or by using the sudo command mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 Note that you need to specify a path to an existing directory that stores database files with the --dpbath . When the --profile option is set to 2, mongod collects the profiling data for all operations. To decrease the load, you may consider setting this option to 1 so that the profiling data are only collected for slow operations. The --slowms option sets the minimum time for a slow operation. In the given example, any operation which takes longer than 200 milliseconds is a slow operation. The --rateLimit option, which is available if you use PSMDB instead of MongoDB, refers to the number of queries that the MongoDB profiler collects. The lower the rate limit, the less impact on the performance. However, the accuracy of the collected information decreases as well. Enabling Profiling in the Configuration File If you run mongod as a service, you need to use the configuration file which by default is /etc/mongod.conf . In this file, you need to locate the operationProfiling: section and add the following settings: operationProfiling: slowOpThresholdMs: 200 mode: slowOp These settings affect mongod in the same way as the command line options. Note that the configuration file is in the YAML format. In this format the indentation of your lines is important as it defines levels of nesting. Restart the mongod service to enable the settings. Run this command as root or by using the sudo command service mongod restart Adding MongoDB Service Monitoring Add monitoring as follows: pmm-admin add mongodb --username = pmm --password = pmm where username and password are credentials for the monitored MongoDB access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mongodb and 127.0.0.1:27017 . The command line and the output of this command may look as follows: pmm-admin add mongodb --username = pmm --password = pmm mongo 127 .0.0.1:27017 MongoDB Service added. Service ID : /service_id/f1af8a88-5a95-4bf1-a646-0101f8a20791 Service name: mongo Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mongodb --username = pmm --password = pmm --service-name = mongo --host = 127 .0.0.1 --port = 27017 Note It is also possible to add a MongoDB instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock Passing SSL parameters to the MongoDB monitoring service SSL/TLS related parameters are passed to an SSL enabled MongoDB server as monitoring service parameters along with the pmm-admin add command when adding the MongoDB monitoring service. Run this command as root or by using the sudo command pmm-admin add mongodb --tls Supported SSL/TLS Parameters --tls Enable a TLS connection with mongo server --tls-skip-verify Skip TLS certificates validation --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file-password=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file.","title":"MongoDB"},{"location":"setting-up/client/mongodb.html#configuring-mongodb-for-monitoring-in-pmm-query-analytics","text":"In Query Analytics, you can monitor MongoDB metrics and queries. Run the pmm-admin add command to use these monitoring services. Supported versions of MongoDB Query Analytics supports MongoDB version 3.2 or higher.","title":"Configuring MongoDB for Monitoring in PMM Query Analytics"},{"location":"setting-up/client/mongodb.html#setting-up-the-required-permissions","text":"For MongoDB monitoring services to work in Query Analytics, you need to set up the mongodb_exporter user. Here is an example for the MongoDB shell that creates and assigns the appropriate roles to the user. db . createRole ({ role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collection : \"\" }, actions : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles : [] }) db . getSiblingDB ( \"admin\" ). createUser ({ user : \"mongodb_exporter\" , pwd : \"s3cR#tpa$$worD\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] })","title":"Setting Up the Required Permissions"},{"location":"setting-up/client/mongodb.html#enabling-profiling","text":"For MongoDB to work correctly with Query Analytics, you need to enable profiling in your mongod configuration. When started without profiling enabled, Query Analytics displays the following warning: Note A warning message is displayed when profiling is not enabled It is required that profiling of the monitored MongoDB databases be enabled, however profiling is not enabled by default because it may reduce the performance of your MongoDB server.","title":"Enabling Profiling"},{"location":"setting-up/client/mongodb.html#enabling-profiling-on-command-line","text":"You can enable profiling from command line when you start the mongod server. This command is useful if you start mongod manually. Run this command as root or by using the sudo command mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 Note that you need to specify a path to an existing directory that stores database files with the --dpbath . When the --profile option is set to 2, mongod collects the profiling data for all operations. To decrease the load, you may consider setting this option to 1 so that the profiling data are only collected for slow operations. The --slowms option sets the minimum time for a slow operation. In the given example, any operation which takes longer than 200 milliseconds is a slow operation. The --rateLimit option, which is available if you use PSMDB instead of MongoDB, refers to the number of queries that the MongoDB profiler collects. The lower the rate limit, the less impact on the performance. However, the accuracy of the collected information decreases as well.","title":"Enabling Profiling on Command Line"},{"location":"setting-up/client/mongodb.html#enabling-profiling-in-the-configuration-file","text":"If you run mongod as a service, you need to use the configuration file which by default is /etc/mongod.conf . In this file, you need to locate the operationProfiling: section and add the following settings: operationProfiling: slowOpThresholdMs: 200 mode: slowOp These settings affect mongod in the same way as the command line options. Note that the configuration file is in the YAML format. In this format the indentation of your lines is important as it defines levels of nesting. Restart the mongod service to enable the settings. Run this command as root or by using the sudo command service mongod restart","title":"Enabling Profiling in the Configuration File"},{"location":"setting-up/client/mongodb.html#adding-mongodb-service-monitoring","text":"Add monitoring as follows: pmm-admin add mongodb --username = pmm --password = pmm where username and password are credentials for the monitored MongoDB access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mongodb and 127.0.0.1:27017 . The command line and the output of this command may look as follows: pmm-admin add mongodb --username = pmm --password = pmm mongo 127 .0.0.1:27017 MongoDB Service added. Service ID : /service_id/f1af8a88-5a95-4bf1-a646-0101f8a20791 Service name: mongo Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mongodb --username = pmm --password = pmm --service-name = mongo --host = 127 .0.0.1 --port = 27017 Note It is also possible to add a MongoDB instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock","title":"Adding MongoDB Service Monitoring"},{"location":"setting-up/client/mongodb.html#passing-ssl-parameters-to-the-mongodb-monitoring-service","text":"SSL/TLS related parameters are passed to an SSL enabled MongoDB server as monitoring service parameters along with the pmm-admin add command when adding the MongoDB monitoring service. Run this command as root or by using the sudo command pmm-admin add mongodb --tls Supported SSL/TLS Parameters --tls Enable a TLS connection with mongo server --tls-skip-verify Skip TLS certificates validation --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file-password=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file.","title":"Passing SSL parameters to the MongoDB monitoring service"},{"location":"setting-up/client/mysql.html","text":"MySQL and variants PMM Client collects metrics from MySQL , Percona Server for MySQL , Percona XtraDB Cluster , and MariaDB . (Amazon RDS is also supported and explained in a separate section .) This page shows you how to set up PMM to monitor a MySQL or MySQL-based database instance. (You should read it completely before starting work.) Here is an overview of the steps involved. Before you start PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node registered with PMM Server . You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor. Create a database account for PMM It is good practice to use a non-superuser account to connect PMM Client to the monitored database instance. This example creates a database user with name pmm , password pass , and the necessary permissions. CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ; Choose and configure a source Decide which source of metrics to use, and configure your database server for it. The choices are Slow query log and Performance Schema . While you can use both at the same time we recommend using only one\u2013there is some overlap in the data reported, and each incurs a small performance penalty. The choice depends on the version and variant of your MySQL instance, and how much detail you want to see. Here are the benefits and drawbacks of Slow query log and Performance Schema metrics sources. Benefits Drawbacks Slow query log More detail. PMM Client must be on the same host as the database server or have access to the slow query log. Lower resource impact (with query sampling feature in Percona Server for MySQL). Log files grow and must be actively managed. Performance Schema Faster parsing. Less detail. Enabled by default on newer versions of MySQL. Data source recommendations Database server Versions Recommended source MySQL 5.1-5.5 Slow query log MySQL 5.6+ Performance Schema MariaDB 10.0+ Performance Schema Percona Server for MySQL 5.7, 8.0 Slow query log Percona XtraDB Cluster 5.6, 5.7, 8.0 Slow query log Slow query log This section covers how to configure a MySQL-based database server to use the slow query log as a source of metrics. Applicable versions Server Versions MySQL 5.1-5.5 MariaDB 10.1.2+ Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 The slow query log records the details of queries that take more than a certain amount of time to complete. With the database server configured to write this information to a file rather than a table, PMM Client parses the file and sends aggregated data to PMM Server via the Query Analytics part of PMM Agent. Settings Variable Value Description slow_query_log ON Enables the slow query log. log_output 'FILE' Ensures the log is sent to a file. (This is the default on MariaDB.) long_query_time 0 The slow query threshold in seconds. In heavily-loaded applications, many quick queries can affect performance more than a few slow ones. Setting this value to 0 ensures all queries are captured. log_slow_admin_statements ON Includes the logging of slow administrative statements. log_slow_slave_statements ON Enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Examples Configuration file slow_query_log = ON log_output = FILE long_query_time = 0 log_slow_admin_statements = ON log_slow_slave_statements = ON Session SET GLOBAL slow_query_log = 1 ; SET GLOBAL log_output = 'FILE' ; SET GLOBAL long_query_time = 0 ; SET GLOBAL log_slow_admin_statements = 1 ; SET GLOBAL log_slow_slave_statements = 1 ; Slow query log \u2013 extended Some MySQL-based database servers support extended slow query log variables. Applicable versions Server Versions Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.0 Settings Variable Value Description log_slow_rate_limit 100 Defines the rate of queries captured by the slow query log . A good rule of thumb is 100 queries logged per second. For example, if your Percona Server instance processes 10,000 queries per second, you should set log_slow_rate_limit to 100 and capture every 100 th query for the slow query log . Depending on the amount of traffic, logging could become aggressive and resource consuming. This variable throttles the level of intensity of the data capture without compromising information. log_slow_rate_type \u2018query\u2019 Set so that it applies to queries, rather than sessions. slow_query_log_always_write_time 1 Specifies which queries should ignore sampling. With query sampling this ensures that queries with longer execution time will always be captured by the slow query log, avoiding the possibility that infrequent slow queries might not get captured at all. log_slow_verbosity \u2018full\u2019 Ensures that all information about each captured query is stored in the slow query log. slow_query_log_use_global_control \u2018all\u2019 Configure the slow query log during runtime and apply these settings to existing connections. (By default, slow query log settings apply only to new sessions.) Examples Configuration file (Percona Server for MySQL, Percona XtraDB Cluster) log_slow_rate_limit = 100 log_slow_rate_type = 'query' slow_query_log_always_write_time = 1 log_slow_verbosity = 'full' slow_query_log_use_global_control = 'all' Configuration file (MariaDB) log_slow_rate_limit = 100 Session (Percona Server for MySQL, Percona XtraDB Cluster) SET GLOBAL log_slow_rate_limit = 100 ; SET GLOBAL log_slow_rate_type = 'query' ; SET GLOBAL slow_query_log_always_write_time = 1 ; SET GLOBAL log_slow_verbosity = 'full' ; SET GLOBAL slow_query_log_use_global_control = 'all' ; Slow query log rotation Slow query log files can grow quickly and must be managed. When adding a service with the command line use the pmm-admin option --size-slow-logs to set at what size the slow query log file is rotated. (The size is specified as a number with a suffix. See pmm-admin add mysql .) When the limit is reached, PMM Client will: remove the previous .old slow log file, rename the current file by adding the suffix .old , execute the MySQL command FLUSH LOGS . Only one .old file is kept. Older ones are deleted. You can manage log rotation yourself, for example, with logrotate . If you do, you can disable PMM Client\u2019s log rotation with the --slow-log-rotation=false option when adding a service with pmm-admin add . Performance Schema This section covers how to configure a MySQL-based database server to use Performance Schema as a source of metrics. Applicable versions Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.3+ PMM\u2019s MySQL Performance Schema Details dashboard charts the various performance_schema metrics. To use Performance Schema , set these variables. Variable Value Description performance_schema ON Enables Performance Schema metrics. This is the default in MySQL 5.6.6 and higher. performance-schema-instrument 'statement/%=ON' Configures Performance Schema instruments. performance-schema-consumer-statements-digest ON Configures the statements-digest consumer. innodb_monitor_enable all Enables InnoDB metrics counters. Examples Configuration file performance_schema = ON performance-schema-instrument = 'statement/%=ON' performance-schema-consumer-statements-digest = ON innodb_monitor_enable = all Session ( performance_schema cannot be set in a session and must be set at server start-up.) UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; SET GLOBAL innodb_monitor_enable = all ; MariaDB 10.5.7 or lower There is no Explain or Example data shown by default in Query Analytics when monitoring MariaDB instances version 10.5.7 or lower. A workaround is to set this variable. Variable Value Description performance_schema.setup_instruments 'statement/%' List of instrumented object classes. Session UPDATE performance_schema . setup_instruments SET ENABLED = 'YES' , TIMED = 'YES' WHERE NAME LIKE 'statement/%' ; UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; Query response time Query time distribution is a chart in the Details tab of Query Analytics showing the proportion of query time spent on various activities. It is enabled with the query_response_time_stats variable and associated plugins. Applicable versions Server Versions Percona Server for MySQL 5.7 ( not Percona Server for MySQL 8.0 .) MariaDB 10.0.4 Set this variable to see query time distribution charts. Variable Value Description query_response_time_stats ON Report query response time distributions . (Requires plugin installation. See below.) Configuration file query_response_time_stats = ON You must also install the plugins. Session Check that /usr/lib/mysql/plugin/query_response_time.so exists. Install the plugins and activate. For MariaDB 10.3 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; For Percona Server for MySQL 5.7 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; Tablestats Some table metrics are automatically disabled when the number of tables exceeds a default limit of 1000 tables. This prevents PMM Client from affecting the performance of your database server. The limit can be changed when adding a service on the command line with the two pmm-admin options: pmm-admin option Description --disable-tablestats Disables tablestats collection when the default limit is reached. --disable-tablestats-limit=N Sets the number of tables ( N ) for which tablestats collection is disabled. 0 means no limit. A negative number means tablestats is completely disabled (for any number of tables). User statistics Applicable versions User activity, individual table and index access details are shown on the MySQL User Details dashboard when the userstat variable is set. Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 5.2.0+ Examples Configuration file userstat = ON Session SET GLOBAL userstat = ON ; Add a service When you have configured your database server, you can add a MySQL service with the user interface or on the command line. When adding a service with the command line, you must use the pmm-admin --query-source=SOURCE option to match the source you\u2019ve chosen and configured the database server for. With the PMM user interface, you select Use performance schema , or deselect it to use slow query log . With the user interface Select PMM \u2192 PMM Add Instance . Select MySQL \u2013 Add a remote instance . Enter values for these fields. Section Field Required Description Default pmm-admin parameter Main details Hostname \u2611\ufe0f Hostname or IP address of the service --address Service name Service name --name Port Port for accessing the service 3306 port in --address=address[:port] Username MySQL user name --username Password MySQL user password --password Labels Environment --environment Region Availability zone Replication set --replication-set Cluster --cluster Custom labels --custom-labels Additional options Skip connection check --skip-connection-check Use TLS for database connections --tls Skip TLS certificate and hostname validation --tls-skip-verify Table statistics limit \u2192 Disabled --disable-tablestats \u2192 Default --disable-tablestats-limit \u2192 Custom --disable-tablestats-limit Use performance schema --perfschema if selected, --slowlog if not. Click Add service . On the command line Configure PMM Client (connect to example PMM Server at address 192.168.1.123 ). pmm-admin config --server-insecure-tls --server-url = https://192.168.1.123:443 Add the database server as a service using one of these example commands. If successful, PMM Client will print MySQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service that help you distinguish them. Examples \u2013 Slow query log Default query source ( slowlog ), service name ( {node name}-mysql ), and service address/port ( 127.0.0.1:3306 ), with database server account pmm and password pass . sudo pmm-admin add mysql --username = pmm --password = pass Slow query log source and log size limit (1 gigabyte), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). sudo pmm-admin add mysql --query-source = slowlog --size-slow-logs = 1GB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Slow query log source, disabled log management (use logrotate or some other log management tool), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). sudo pmm-admin add mysql --query-source = slowlog --size-slow-logs = false --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Default query source ( slowlog ), service name ( {node}-mysql ), connect via socket. sudo pmm-admin add mysql --username = pmm --password = pass --socket = /var/run/mysqld/mysqld.sock Examples \u2013 Performance Schema Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) sudo pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass MYSQL_NODE Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) specified with flags. sudo pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass --service-name = MYSQL_NODE --host = 127 .0.0.1 --port = 3306 Examples \u2013 Identifying services Default query source ( slowlog ), environment labelled test , custom labels setting source to slowlog . (This example uses positional parameters for service name and service address.) pmm-admin add mysql --environment = test --custom-labels = 'source=slowlog' --username = root --password = password --query-source = slowlog MySQLSlowLog localhost:3306 Check the service Check service - PMM user interface Go to PMM \u2192 PMM Inventory . Look in the Services tab for a matching Service Type (MySQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used. Check service - Command line Look for your service in the output of this command. pmm-admin inventory list services Check data Open the MySQL Instance Summary dashboard. Set the Service Name to the newly-added service. Percona Server for MySQL, MariaDB If query response time plugin was installed, check for data in the MySQL Query Response Time Details dashboard or select a query in PMM Query Analytics to see the Query time distribution bar. Percona XtraDB Cluster Open the PXC/Galera Cluster Summary dashboard . See also Percona Server for MySQL \u2013 Slow Query Log Extended Percona Server for MySQL \u2013 User Statistics MariaDB \u2013 Slow Query Log Overview MariaDB \u2013 Slow Query Log Extended Statistics MariaDB \u2013 User Statistics Percona Blog \u2013 PERFORMANCE_SCHEMA vs Slow Query Log Percona Blog \u2013 MySQL\u2019s INNODB_METRICS table Percona Blog \u2013 Rotating MySQL Slow Logs Safely Percona Blog \u2013 Impact of logging on MySQL\u2019s performance","title":"MySQL and variants"},{"location":"setting-up/client/mysql.html#before-you-start","text":"PMM Server is installed and running with a known IP address accessible from the client node. PMM Client is installed and the node registered with PMM Server . You have superuser (root) access on the client host. You have superuser access to any database servers that you want to monitor.","title":"Before you start"},{"location":"setting-up/client/mysql.html#create-a-database-account-for-pmm","text":"It is good practice to use a non-superuser account to connect PMM Client to the monitored database instance. This example creates a database user with name pmm , password pass , and the necessary permissions. CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ;","title":"Create a database account for PMM"},{"location":"setting-up/client/mysql.html#choose-and-configure-a-source","text":"Decide which source of metrics to use, and configure your database server for it. The choices are Slow query log and Performance Schema . While you can use both at the same time we recommend using only one\u2013there is some overlap in the data reported, and each incurs a small performance penalty. The choice depends on the version and variant of your MySQL instance, and how much detail you want to see. Here are the benefits and drawbacks of Slow query log and Performance Schema metrics sources. Benefits Drawbacks Slow query log More detail. PMM Client must be on the same host as the database server or have access to the slow query log. Lower resource impact (with query sampling feature in Percona Server for MySQL). Log files grow and must be actively managed. Performance Schema Faster parsing. Less detail. Enabled by default on newer versions of MySQL. Data source recommendations Database server Versions Recommended source MySQL 5.1-5.5 Slow query log MySQL 5.6+ Performance Schema MariaDB 10.0+ Performance Schema Percona Server for MySQL 5.7, 8.0 Slow query log Percona XtraDB Cluster 5.6, 5.7, 8.0 Slow query log","title":"Choose and configure a source"},{"location":"setting-up/client/mysql.html#slow-query-log","text":"This section covers how to configure a MySQL-based database server to use the slow query log as a source of metrics. Applicable versions Server Versions MySQL 5.1-5.5 MariaDB 10.1.2+ Percona Server for MySQL 5.7.10+, 8.0.12+ Percona XtraDB Cluster 5.6, 5.7, 8.0 The slow query log records the details of queries that take more than a certain amount of time to complete. With the database server configured to write this information to a file rather than a table, PMM Client parses the file and sends aggregated data to PMM Server via the Query Analytics part of PMM Agent. Settings Variable Value Description slow_query_log ON Enables the slow query log. log_output 'FILE' Ensures the log is sent to a file. (This is the default on MariaDB.) long_query_time 0 The slow query threshold in seconds. In heavily-loaded applications, many quick queries can affect performance more than a few slow ones. Setting this value to 0 ensures all queries are captured. log_slow_admin_statements ON Includes the logging of slow administrative statements. log_slow_slave_statements ON Enables logging for queries that have taken more than long_query_time seconds to execute on the replica. Examples Configuration file slow_query_log = ON log_output = FILE long_query_time = 0 log_slow_admin_statements = ON log_slow_slave_statements = ON Session SET GLOBAL slow_query_log = 1 ; SET GLOBAL log_output = 'FILE' ; SET GLOBAL long_query_time = 0 ; SET GLOBAL log_slow_admin_statements = 1 ; SET GLOBAL log_slow_slave_statements = 1 ;","title":"Slow query log"},{"location":"setting-up/client/mysql.html#performance-schema","text":"This section covers how to configure a MySQL-based database server to use Performance Schema as a source of metrics. Applicable versions Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 10.3+ PMM\u2019s MySQL Performance Schema Details dashboard charts the various performance_schema metrics. To use Performance Schema , set these variables. Variable Value Description performance_schema ON Enables Performance Schema metrics. This is the default in MySQL 5.6.6 and higher. performance-schema-instrument 'statement/%=ON' Configures Performance Schema instruments. performance-schema-consumer-statements-digest ON Configures the statements-digest consumer. innodb_monitor_enable all Enables InnoDB metrics counters. Examples Configuration file performance_schema = ON performance-schema-instrument = 'statement/%=ON' performance-schema-consumer-statements-digest = ON innodb_monitor_enable = all Session ( performance_schema cannot be set in a session and must be set at server start-up.) UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ; SET GLOBAL innodb_monitor_enable = all ; MariaDB 10.5.7 or lower There is no Explain or Example data shown by default in Query Analytics when monitoring MariaDB instances version 10.5.7 or lower. A workaround is to set this variable. Variable Value Description performance_schema.setup_instruments 'statement/%' List of instrumented object classes. Session UPDATE performance_schema . setup_instruments SET ENABLED = 'YES' , TIMED = 'YES' WHERE NAME LIKE 'statement/%' ; UPDATE performance_schema . setup_consumers SET ENABLED = 'YES' WHERE NAME LIKE '%statements%' ;","title":"Performance Schema"},{"location":"setting-up/client/mysql.html#query-response-time","text":"Query time distribution is a chart in the Details tab of Query Analytics showing the proportion of query time spent on various activities. It is enabled with the query_response_time_stats variable and associated plugins. Applicable versions Server Versions Percona Server for MySQL 5.7 ( not Percona Server for MySQL 8.0 .) MariaDB 10.0.4 Set this variable to see query time distribution charts. Variable Value Description query_response_time_stats ON Report query response time distributions . (Requires plugin installation. See below.) Configuration file query_response_time_stats = ON You must also install the plugins. Session Check that /usr/lib/mysql/plugin/query_response_time.so exists. Install the plugins and activate. For MariaDB 10.3 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ; For Percona Server for MySQL 5.7 : INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; SET GLOBAL query_response_time_stats = ON ;","title":"Query response time"},{"location":"setting-up/client/mysql.html#tablestats","text":"Some table metrics are automatically disabled when the number of tables exceeds a default limit of 1000 tables. This prevents PMM Client from affecting the performance of your database server. The limit can be changed when adding a service on the command line with the two pmm-admin options: pmm-admin option Description --disable-tablestats Disables tablestats collection when the default limit is reached. --disable-tablestats-limit=N Sets the number of tables ( N ) for which tablestats collection is disabled. 0 means no limit. A negative number means tablestats is completely disabled (for any number of tables).","title":"Tablestats"},{"location":"setting-up/client/mysql.html#user-statistics","text":"Applicable versions User activity, individual table and index access details are shown on the MySQL User Details dashboard when the userstat variable is set. Server Versions Percona Server for MySQL 5.6, 5.7, 8.0 Percona XtraDB Cluster 5.6, 5.7, 8.0 MariaDB 5.2.0+ Examples Configuration file userstat = ON Session SET GLOBAL userstat = ON ;","title":"User statistics"},{"location":"setting-up/client/mysql.html#add-a-service","text":"When you have configured your database server, you can add a MySQL service with the user interface or on the command line. When adding a service with the command line, you must use the pmm-admin --query-source=SOURCE option to match the source you\u2019ve chosen and configured the database server for. With the PMM user interface, you select Use performance schema , or deselect it to use slow query log .","title":"Add a service"},{"location":"setting-up/client/mysql.html#with-the-user-interface","text":"Select PMM \u2192 PMM Add Instance . Select MySQL \u2013 Add a remote instance . Enter values for these fields. Section Field Required Description Default pmm-admin parameter Main details Hostname \u2611\ufe0f Hostname or IP address of the service --address Service name Service name --name Port Port for accessing the service 3306 port in --address=address[:port] Username MySQL user name --username Password MySQL user password --password Labels Environment --environment Region Availability zone Replication set --replication-set Cluster --cluster Custom labels --custom-labels Additional options Skip connection check --skip-connection-check Use TLS for database connections --tls Skip TLS certificate and hostname validation --tls-skip-verify Table statistics limit \u2192 Disabled --disable-tablestats \u2192 Default --disable-tablestats-limit \u2192 Custom --disable-tablestats-limit Use performance schema --perfschema if selected, --slowlog if not. Click Add service .","title":"With the user interface"},{"location":"setting-up/client/mysql.html#on-the-command-line","text":"Configure PMM Client (connect to example PMM Server at address 192.168.1.123 ). pmm-admin config --server-insecure-tls --server-url = https://192.168.1.123:443 Add the database server as a service using one of these example commands. If successful, PMM Client will print MySQL Service added with the service\u2019s ID and name. Use the --environment and -custom-labels options to set tags for the service that help you distinguish them. Examples \u2013 Slow query log Default query source ( slowlog ), service name ( {node name}-mysql ), and service address/port ( 127.0.0.1:3306 ), with database server account pmm and password pass . sudo pmm-admin add mysql --username = pmm --password = pass Slow query log source and log size limit (1 gigabyte), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). sudo pmm-admin add mysql --query-source = slowlog --size-slow-logs = 1GB --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Slow query log source, disabled log management (use logrotate or some other log management tool), service name ( MYSQL_NODE ) and service address/port ( 191.168.1.123:3306 ). sudo pmm-admin add mysql --query-source = slowlog --size-slow-logs = false --username = pmm --password = pass MYSQL_NODE 192 .168.1.123:3306 Default query source ( slowlog ), service name ( {node}-mysql ), connect via socket. sudo pmm-admin add mysql --username = pmm --password = pass --socket = /var/run/mysqld/mysqld.sock Examples \u2013 Performance Schema Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) sudo pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass MYSQL_NODE Performance schema query source, service name ( MYSQL_NODE ) and default service address/port ( 127.0.0.1:3306 ) specified with flags. sudo pmm-admin add mysql --query-source = perfschema --username = pmm --password = pass --service-name = MYSQL_NODE --host = 127 .0.0.1 --port = 3306 Examples \u2013 Identifying services Default query source ( slowlog ), environment labelled test , custom labels setting source to slowlog . (This example uses positional parameters for service name and service address.) pmm-admin add mysql --environment = test --custom-labels = 'source=slowlog' --username = root --password = password --query-source = slowlog MySQLSlowLog localhost:3306","title":"On the command line"},{"location":"setting-up/client/mysql.html#check-the-service","text":"Check service - PMM user interface Go to PMM \u2192 PMM Inventory . Look in the Services tab for a matching Service Type (MySQL), Service name , Addresses , and any other details entered in the form. Look in the Agents tab to check the desired data source is being used. Check service - Command line Look for your service in the output of this command. pmm-admin inventory list services Check data Open the MySQL Instance Summary dashboard. Set the Service Name to the newly-added service. Percona Server for MySQL, MariaDB If query response time plugin was installed, check for data in the MySQL Query Response Time Details dashboard or select a query in PMM Query Analytics to see the Query time distribution bar. Percona XtraDB Cluster Open the PXC/Galera Cluster Summary dashboard . See also Percona Server for MySQL \u2013 Slow Query Log Extended Percona Server for MySQL \u2013 User Statistics MariaDB \u2013 Slow Query Log Overview MariaDB \u2013 Slow Query Log Extended Statistics MariaDB \u2013 User Statistics Percona Blog \u2013 PERFORMANCE_SCHEMA vs Slow Query Log Percona Blog \u2013 MySQL\u2019s INNODB_METRICS table Percona Blog \u2013 Rotating MySQL Slow Logs Safely Percona Blog \u2013 Impact of logging on MySQL\u2019s performance","title":"Check the service"},{"location":"setting-up/client/postgresql.html","text":"PostgreSQL PMM follows the postgresql.org EOL policy . For specific details on supported platforms and versions, see Percona\u2019s Software Platform Lifecycle page . To monitor PostgreSQL queries, you must install a database extension. There are two choices: pg_stat_monitor , a new extension created by Percona, based on pg_stat_statements and compatible with it. pg_stat_statements , the original extension created by PostgreSQL, part of the postgres-contrib package available on Linux. pg_stat_monitor provides all the features of pg_stat_statements , but extends it to provide bucket-based data aggregation, a feature missing from pg_stat_statements . ( pg_stat_statements accumulates data without providing aggregated statistics or histogram information.) Note pg_stat_monitor is the recommended option. Although nothing prevents you from installing and using both, we don\u2019t recommend this as you will get duplicate metrics. Caution pg_stat_monitor is beta software and currently unsupported. Prerequisites We recommend that you create a PostgreSQL user for SUPERUSER level access. This lets you gather the most data with the least fuss. This user must be able to connect to the postgres database where the extension was installed. The PostgreSQL user should have local password authentication enabled to access PMM. To do this, set ident to md5 for the user in the pg_hba.conf configuration file. To create a superuser: CREATE USER pmm_user WITH SUPERUSER ENCRYPTED PASSWORD '******' ; Or, if your database runs on Amazon RDS: CREATE USER pmm_user WITH rds_superuser ENCRYPTED PASSWORD '******' ; pg_stat_monitor pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded. Compatibility pg_stat_monitor has been tested with: PostgreSQL versions 11, 12, 13. Percona Distribution for PostgreSQL versions 11, 12, 13. Install This extension can be installed in two ways: For Percona Distribution for PostgreSQL: Using standard Linux package manager tools. For PostgreSQL or Percona Distribution for PostgreSQL: download and compile the source code . Install using Linux package manager The pg-stat-monitor extension is included in Percona Distribution for PostgreSQL . This can be installed via the percona-release package. This section reproduces parts of the following: Configuring Percona Repositories with percona-release Installing Percona Distribution for PostgreSQL Debian sudo apt-get install -y wget gnupg2 lsb-release wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb sudo percona-release setup ppg-12 # version 12 (others available) sudo apt install -y percona-postgresql-12 Red Hat sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm # If RHEL 8 sudo dnf module disable postgresql # If RHEL 7 sudo yum install -y epel-release sudo yum repolist sudo percona-release setup ppg-12 sudo yum install -y percona-postgresql12-server Install from source code Debian Install common packages sudo apt-get install -y curl git wget gnupg2 lsb-release sudo apt-get update -y Install PostgreSQL development packages With Percona Distribution for PostgreSQL (version 12): wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb sudo percona-release setup ppg-12 sudo apt install -y percona-postgresql-server-dev-all With PostgreSQL: wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list sudo apt install -y postgresql-server-dev-all Download, compile, and install extension git clone git://github.com/percona/pg_stat_monitor.git && cd pg_stat_monitor sudo make USE_PGXS = 1 sudo make USE_PGXS = 1 install Red Hat Install common packages sudo yum install -y centos-release-scl epel-release sudo yum update -y sudo yum install -y git gcc gcc-c++ llvm-toolset-7 Install PostgreSQL development packages With Percona Distribution for PostgreSQL (version 12): sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm sudo percona-release setup ppg-12 sudo yum install -y percona-postgresql12-devel With PostgreSQL version 12: sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql12-devel Download, compile, and install extension git clone git://github.com/percona/pg_stat_monitor.git && cd pg_stat_monitor sudo make PG_CONFIG = /usr/pgsql-12/bin/pg_config USE_PGXS = 1 sudo make PG_CONFIG = /usr/pgsql-12/bin/pg_config USE_PGXS = 1 install Configure Set or change the value for shared_preload_library in your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Set the value pg_stat_monitor.pgsm_normalized_query Start or restart your PostgreSQL instance. In a psql session: CREATE EXTENSION pg_stat_monitor ; Configuration Parameters Here are the configuration parameters, available values ranges, and default values. All require a restart of PostgreSQL except for pg_stat_monitor.pgsm_track_utility and pg_stat_monitor.pgsm_normalized_query . To make settings permanent, add them to your postgresql.conf file before starting your PostgreSQL instance. pg_stat_monitor.pgsm_max (5000-2147483647 bytes) Default: 5000 Defines the limit of shared memory. Memory is used by buckets in a circular manner and is divided between buckets equally when PostgreSQL starts. pg_stat_monitor.pgsm_query_max_len (1024-2147483647 bytes) Default: 1024 The maximum size of the query. Long queries are truncated to this length to avoid unnecessary usage of shared memory. This parameter must be set before PostgreSQL starts. pg_stat_monitor.pgsm_enable (0-1) Default: 1 (true). Enables or disables monitoring. A value of Disable means that pg_stat_monitor will not collect statistics for the entire cluster. pg_stat_monitor.pgsm_track_utility (0-1) Default: 1 (true) Controls whether utility commands (all except SELECT, INSERT, UPDATE and DELETE) are tracked. pg_stat_monitor.pgsm_normalized_query (0-1) Default: 0 (false) By default, a query shows the actual parameter instead of a placeholder. Set to 1 to change to showing value placeholders (as $n where n is an integer). pg_stat_monitor.pgsm_max_buckets (1-10) Default: 10 Sets the maximum number of available data buckets. pg_stat_monitor.pgsm_bucket_time (1-2147483647 seconds) Default: 60 Sets the lifetime of the bucket. The system switches between buckets on the basis of this value. pg_stat_monitor.pgsm_object_cache (50-2147483647) Default: 50 The maximum number of objects in the information cache. pg_stat_monitor.pgsm_respose_time_lower_bound (1-2147483647 milliseconds) Default: 1 Sets the lower bound of the execution time histogram. pg_stat_monitor.pgsm_respose_time_step (1-2147483647 milliseconds) Default: 1 Sets the time value of the steps for the histogram. pg_stat_monitor.pgsm_query_shared_buffer (500000-2147483647 bytes) Default: 500000 Sets the query shared buffer size. pg_stat_monitor.pgsm_track_planning (0-1) Default: 1 (true) Whether to track planning statistics. pg_stat_statements pg_stat_statements is included in the official PostgreSQL postgresql-contrib available from your Linux distribution package manager. Install Debian sudo apt-get install postgresql-contrib Red Hat sudo yum install -y postgresql-contrib Configure Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested Restart your PostgreSQL instance. Install the extension (run in the postgres database). CREATE EXTENSION pg_stat_statements SCHEMA public ; Adding PostgreSQL queries and metrics monitoring You add PostgreSQL metrics and queries monitoring with the following command: pmm-admin add postgresql --username = <user name> --password = <password> Where <user name> and <password> are the PostgreSQL user credentials. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-postgresql and 127.0.0.1:5432 . The command line and the output of this command may look as follows: pmm-admin add postgresql --username = pmm --password = pmm postgres 127 .0.0.1:5432 PostgreSQL Service added. Service ID : /service_id/28f1d93a-5c16-467f-841b-8c014bf81ca6 Service name: postgres If correct installed and set up, you should be able to see data in PostgreSQL Overview dashboard, and also Query Analytics should contain PostgreSQL queries. Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add postgresql --username = pmm --password = pmm --service-name = postgres --host = 127 .0.0.1 --port = 270175432 It is also possible to add a PostgreSQL instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add postgresql --socket = /var/run/postgresql Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf () ;","title":"PostgreSQL"},{"location":"setting-up/client/postgresql.html#prerequisites","text":"We recommend that you create a PostgreSQL user for SUPERUSER level access. This lets you gather the most data with the least fuss. This user must be able to connect to the postgres database where the extension was installed. The PostgreSQL user should have local password authentication enabled to access PMM. To do this, set ident to md5 for the user in the pg_hba.conf configuration file. To create a superuser: CREATE USER pmm_user WITH SUPERUSER ENCRYPTED PASSWORD '******' ; Or, if your database runs on Amazon RDS: CREATE USER pmm_user WITH rds_superuser ENCRYPTED PASSWORD '******' ;","title":"Prerequisites"},{"location":"setting-up/client/postgresql.html#pg_stat_monitor","text":"pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded.","title":"pg_stat_monitor"},{"location":"setting-up/client/postgresql.html#compatibility","text":"pg_stat_monitor has been tested with: PostgreSQL versions 11, 12, 13. Percona Distribution for PostgreSQL versions 11, 12, 13.","title":"Compatibility"},{"location":"setting-up/client/postgresql.html#install","text":"This extension can be installed in two ways: For Percona Distribution for PostgreSQL: Using standard Linux package manager tools. For PostgreSQL or Percona Distribution for PostgreSQL: download and compile the source code .","title":"Install"},{"location":"setting-up/client/postgresql.html#configure","text":"Set or change the value for shared_preload_library in your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Set the value pg_stat_monitor.pgsm_normalized_query Start or restart your PostgreSQL instance. In a psql session: CREATE EXTENSION pg_stat_monitor ;","title":"Configure"},{"location":"setting-up/client/postgresql.html#configuration-parameters","text":"Here are the configuration parameters, available values ranges, and default values. All require a restart of PostgreSQL except for pg_stat_monitor.pgsm_track_utility and pg_stat_monitor.pgsm_normalized_query . To make settings permanent, add them to your postgresql.conf file before starting your PostgreSQL instance. pg_stat_monitor.pgsm_max (5000-2147483647 bytes) Default: 5000 Defines the limit of shared memory. Memory is used by buckets in a circular manner and is divided between buckets equally when PostgreSQL starts. pg_stat_monitor.pgsm_query_max_len (1024-2147483647 bytes) Default: 1024 The maximum size of the query. Long queries are truncated to this length to avoid unnecessary usage of shared memory. This parameter must be set before PostgreSQL starts. pg_stat_monitor.pgsm_enable (0-1) Default: 1 (true). Enables or disables monitoring. A value of Disable means that pg_stat_monitor will not collect statistics for the entire cluster. pg_stat_monitor.pgsm_track_utility (0-1) Default: 1 (true) Controls whether utility commands (all except SELECT, INSERT, UPDATE and DELETE) are tracked. pg_stat_monitor.pgsm_normalized_query (0-1) Default: 0 (false) By default, a query shows the actual parameter instead of a placeholder. Set to 1 to change to showing value placeholders (as $n where n is an integer). pg_stat_monitor.pgsm_max_buckets (1-10) Default: 10 Sets the maximum number of available data buckets. pg_stat_monitor.pgsm_bucket_time (1-2147483647 seconds) Default: 60 Sets the lifetime of the bucket. The system switches between buckets on the basis of this value. pg_stat_monitor.pgsm_object_cache (50-2147483647) Default: 50 The maximum number of objects in the information cache. pg_stat_monitor.pgsm_respose_time_lower_bound (1-2147483647 milliseconds) Default: 1 Sets the lower bound of the execution time histogram. pg_stat_monitor.pgsm_respose_time_step (1-2147483647 milliseconds) Default: 1 Sets the time value of the steps for the histogram. pg_stat_monitor.pgsm_query_shared_buffer (500000-2147483647 bytes) Default: 500000 Sets the query shared buffer size. pg_stat_monitor.pgsm_track_planning (0-1) Default: 1 (true) Whether to track planning statistics.","title":"Configuration Parameters"},{"location":"setting-up/client/postgresql.html#pg_stat_statements","text":"pg_stat_statements is included in the official PostgreSQL postgresql-contrib available from your Linux distribution package manager.","title":"pg_stat_statements"},{"location":"setting-up/client/postgresql.html#install_1","text":"","title":"Install"},{"location":"setting-up/client/postgresql.html#configure_1","text":"Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested Restart your PostgreSQL instance. Install the extension (run in the postgres database). CREATE EXTENSION pg_stat_statements SCHEMA public ;","title":"Configure"},{"location":"setting-up/client/postgresql.html#adding-postgresql-queries-and-metrics-monitoring","text":"You add PostgreSQL metrics and queries monitoring with the following command: pmm-admin add postgresql --username = <user name> --password = <password> Where <user name> and <password> are the PostgreSQL user credentials. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-postgresql and 127.0.0.1:5432 . The command line and the output of this command may look as follows: pmm-admin add postgresql --username = pmm --password = pmm postgres 127 .0.0.1:5432 PostgreSQL Service added. Service ID : /service_id/28f1d93a-5c16-467f-841b-8c014bf81ca6 Service name: postgres If correct installed and set up, you should be able to see data in PostgreSQL Overview dashboard, and also Query Analytics should contain PostgreSQL queries. Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add postgresql --username = pmm --password = pmm --service-name = postgres --host = 127 .0.0.1 --port = 270175432 It is also possible to add a PostgreSQL instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add postgresql --socket = /var/run/postgresql Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf () ;","title":"Adding PostgreSQL queries and metrics monitoring"},{"location":"setting-up/client/proxysql.html","text":"ProxySQL Use the proxysql alias to enable ProxySQL performance metrics monitoring. USAGE pmm-admin add proxysql --username = admin --password = admin where username and password are credentials for the administration interface of the monitored ProxySQL instance. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = admin --password = admin ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"ProxySQL"},{"location":"setting-up/client/proxysql.html#usage","text":"pmm-admin add proxysql --username = admin --password = admin where username and password are credentials for the administration interface of the monitored ProxySQL instance. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = admin --password = admin ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"USAGE"},{"location":"setting-up/server/index.html","text":"Setting up PMM Server You can run PMM Server: as a Docker container as a virtual appliance on an Amazon AWS instance When PMM Server is running, set up PMM Client for each node or service.","title":"Server"},{"location":"setting-up/server/aws.html","text":"AWS Marketplace You can run an instance of PMM Server hosted at AWS Marketplace. Assuming that you have an AWS (Amazon Web Services) account, locate Percona Monitoring and Management Server in AWS Marketplace (or use this link ). Selecting a region and instance type in the Pricing Information section will give you an estimate of the costs involved. This is only an indication of costs. You will choose regions and instance types in later steps. Percona Monitoring and Management Server is provided at no cost, but you may need to pay for infrastructure costs. Note Disk space consumed by PMM Server depends on the number of hosts being monitored. Although each environment will be unique, you can consider the data consumption figures for the PMM Demo web site which consumes approximately 230MB/host/day, or ~6.9GB/host at the default 30 day retention period. For more information, see our blog post How much disk space should I allocate for Percona Monitoring and Management? . Click Continue to Subscribe . Subscribe to this software : Check the terms and conditions and click Continue to Configuration . Configure this software : Select a value for Software Version . (The latest is 2.15.0) Select a region. (You can change this in the next step.) Click Continue to Launch . Launch this software : Choose Action : Select a launch option. Launch from Website is a quick way to make your instance ready. For more control, choose Launch through EC2 . EC2 Instance Type : Select an instance type. VPC Settings : Choose or create a VPC (virtual private cloud). Subnet Settings : Choose or create a subnet. Security Group Settings : Choose a security group or click *Create New Based On Seller Settings Key Pair Settings : Choose or create a key pair. Click Launch . Limiting Access to the instance: security group and a key pair In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Note It is important that the security group allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance. Applying settings Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. Note The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click . Adjusting instance settings in the EC2 Console Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console. Running the instance After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. Note When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - the credentials to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials when configuring PMM Client for the first time on a server, for example: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 Note For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin . Resizing the EBS Volume Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions. Upgrading PMM Server on AWS Upgrading EC2 instance class Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resizing of the EC2 instance without impacting the EBS volume. Expanding the PMM Data EBS Volume The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume. Expand existing EBS volume To expand the existing EBS volume for increased capacity, follow these steps. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS file system to reflect the new size using xfs_growfs , and confirm the file system is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv","title":"AWS Marketplace"},{"location":"setting-up/server/aws.html#limiting-access-to-the-instance-security-group-and-a-key-pair","text":"In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Note It is important that the security group allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance.","title":"Limiting Access to the instance: security group and a key pair"},{"location":"setting-up/server/aws.html#applying-settings","text":"Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. Note The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click .","title":"Applying settings"},{"location":"setting-up/server/aws.html#adjusting-instance-settings-in-the-ec2-console","text":"Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console.","title":"Adjusting instance settings in the EC2 Console"},{"location":"setting-up/server/aws.html#running-the-instance","text":"After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. Note When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - the credentials to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials when configuring PMM Client for the first time on a server, for example: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 Note For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin .","title":"Running the instance"},{"location":"setting-up/server/aws.html#resizing-the-ebs-volume","text":"Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions.","title":"Resizing the EBS Volume"},{"location":"setting-up/server/aws.html#upgrading-pmm-server-on-aws","text":"","title":"Upgrading PMM Server on AWS"},{"location":"setting-up/server/aws.html#upgrading-ec2-instance-class","text":"Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resizing of the EC2 instance without impacting the EBS volume.","title":"Upgrading EC2 instance class"},{"location":"setting-up/server/aws.html#expanding-the-pmm-data-ebs-volume","text":"The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume.","title":"Expanding the PMM Data EBS Volume"},{"location":"setting-up/server/aws.html#expand-existing-ebs-volume","text":"To expand the existing EBS volume for increased capacity, follow these steps. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS file system to reflect the new size using xfs_growfs , and confirm the file system is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv","title":"Expand existing EBS volume"},{"location":"setting-up/server/dbaas.html","text":"Setting up a development environment for DBaaS Caution DBaaS functionality is Alpha. The information on this page is subject to change and may be inaccurate. Software prerequisites Docker minikube Start PMM server with DBaaS activated Install Percona operators in minikube Installing Percona operators in AWS EKS (Kubernetes) Install Operators on GKE Deleting clusters Run PMM Server as a Docker container for DBaaS Exposing PSMDB and XtraDB clusters for access by external clients Software prerequisites Docker Red Hat, CentOS yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce usermod -a -G docker centos systemctl enable docker systemctl start docker Debian, Ubuntu apt-add-repository https://download.docker.com/linux/centos/docker-ce.repo systemctl enable docker systemctl start docker minikube Please install minikube 1.16.0 Red Hat, CentOS yum -y install curl curl -Lo /usr/local/sbin/minikube https://github.com/kubernetes/minikube/releases/download/v1.16.0/minikube-linux-amd64 chmod +x /usr/local/sbin/minikube ln -s /usr/local/sbin/minikube /usr/sbin/minikube alias kubectl = 'minikube kubectl --' Start PMM server with DBaaS activated Notes To start a fully-working 3 node XtraDB cluster, consisting of sets of 3x ProxySQL, 3x PXC and 6x PMM Client containers, you will need at least 9 vCPU available for minikube. (1x vCPU for ProxySQL and PXC and 0.5vCPU for each pmm-client containers). DBaaS does not depend on PMM Client. Setting the environment variable PERCONA_TEST_DBAAS=1 enables DBaaS functionality. Add the option --network minikube if you run PMM Server and minikube in the same Docker instance. (This will share a single network and the kubeconfig will work.) Add the options --env PMM_DEBUG=1 and/or --env PMM_TRACE=1 if you need extended debug details Start PMM server: docker run --detach --publish 80 :80 --publish 443 :443 --name pmm-server --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 Change the default administrator credentials from CLI: (This step is optional, because the same can be done from the web interface of PMM on first login.) docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Install Percona operators in minikube Configure and start minikube: minikube config set cpus 16 minikube config set memory 32768 minikube config set kubernetes-version 1 .16.15 minikube start Deploy the Percona operators configuration for PXC and PSMDB in minikube: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*/pmmserver: ${ PMM_PASS } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - Check the operators are deployed: minikube kubectl -- get nodes minikube kubectl -- get pods minikube kubectl -- wait --for = condition = Available deployment percona-xtradb-cluster-operator minikube kubectl -- wait --for = condition = Available deployment percona-server-mongodb-operator Get your kubeconfig details from minikube (to register your Kubernetes cluster with PMM Server): minikube kubectl -- config view --flatten --minify Note You will need to copy this output to your clipboard and continue with add a Kubernetes cluster to PMM . Installing Percona operators in AWS EKS (Kubernetes) Create your cluster via eksctl or the Amazon AWS interface. For example: eksctl create cluster --write-kubeconfig --name = your-cluster-name --zones = us-west-2a,us-west-2b --kubeconfig <PATH_TO_KUBECONFIG> When your EKS cluster is running, install the PXC and PSMDB operators: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - # Validate that the operators are running kubectl get pods Modify your kubeconfig file, if it\u2019s not utilizing the aws-iam-authenticator or client-certificate method for authentication with Kubernetes. Here are two examples that you can use as templates to modify a copy of your existing kubeconfig: For the aws-iam-authenticator method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_USER >> user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 command: aws-iam-authenticator args: - \"token\" - \"-i\" - \"<< K8S_CLUSTER_NAME >>\" - --region - << AWS_REGION >> env: - name: AWS_ACCESS_KEY_ID value: \"<< AWS_ACCESS_KEY_ID >>\" - name: AWS_SECRET_ACCESS_KEY value: \"<< AWS_SECRET_ACCESS_KEY >>\" For the client-certificate method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_NAME >> user: client-certificate-data: << CLIENT_CERT_DATA >> client-key-data: << CLIENT_KEY_DATA >> Follow the instructions for Add a Kubernetes cluster . Note If possible, the connection details will show the cluster\u2019s external IP (not possible with minikube). Install Operators on GKE Caution These instructions are still in development. Prerequisites You should have an account on GCP https://cloud.google.com/ . Login into google cloud platform console https://console.cloud.google.com/ Navigate to Menu \u2192 Kubernetes Engine \u2192 Clusters Click button Create cluster You can specify cluster option in form or simply click on \u201cMy first cluster\u201d and button Create Wait until cluster created Click on button Connect in a the cluster\u2019s row Click button Run in Cloud shell Click Authorize Set up PXC and PSMDB operators: curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - Check if it was set up successfully kubectl api-resources --api-group='psmdb.percona.com' kubectl api-resources --api-group='pxc.percona.com' Check versions kubectl api-versions | grep percona.com Create Service Account, copy and store kubeconfig - output of the following command cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: percona-dbaas-cluster-operator --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-xtradb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-xtradb-cluster-operator apiGroup: rbac.authorization.k8s.io --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-psmdb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-server-mongodb-operator apiGroup: rbac.authorization.k8s.io EOF name=`kubectl get serviceAccounts percona-dbaas-cluster-operator -o json | jq -r .secrets[].name` certificate=`kubectl get secret $name -o json | jq -r '.data.\"ca.crt\"'` token=`kubectl get secret $name -o json | jq -r '.data.token' | base64 -d` server=`kubectl cluster-info | grep 'Kubernetes master' | cut -d ' ' -f 6` echo \" apiVersion: v1 kind: Config users: - name: percona-dbaas-cluster-operator user: token: $token clusters: - cluster: certificate-authority-data: $certificate server: $server name: self-hosted-cluster contexts: - context: cluster: self-hosted-cluster user: percona-dbaas-cluster-operator name: svcs-acct-context current-context: svcs-acct-context \" Start PMM Server on you local machine or other VM instance: docker run --detach --name pmm-server --publish 80:80 --publish 443:443 \\ --env PERCONA_TEST_DBAAS=1 perconalab/pmm-server-fb:PR-1240-07bef94; Login into PMM and navigate to DBaaS Register your GKE using kubeconfig from step 12. Important Please make sure there are no stray new lines in the kubeconfig, especially in long lines like certificate or token. Deleting clusters You should delete all installation operators as the operators own resources. If you only run eksctl delete cluster without cleaning up the cluster first, there will be a lot of orphaned resources as Cloud Formations, Load Balancers, EC2 instances, Network interfaces, etc. In the pmm-managed repository, in the deploy directory there are 2 example bash scripts to install and delete the operators from the EKS cluster. The install script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Install the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } apply -f - # Install the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } apply -f - The delete script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Delete the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } delete -f - # Delete the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } delete -f - (Both scripts are similar except the install script command is apply while in the delete script it is delete .) After deleting everything in the EKS cluster, run this command (using your own configuration path) and wait until the output only shows service/kubernetes before deleting the cluster with the eksclt delete command. kubectl --kubeconfig ~/.kube/config_eks get all Example output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 4d5h If you don\u2019t need the cluster anymore, you can uninstall everything in it and destroy it: # Delete all volumes created by the operators: kubectl [ --kubeconfig <config file> ] delete pvc --all # Delete the cluster eksctl delete cluster --name = your-cluster-name Run PMM Server as a Docker container for DBaaS Start PMM server from a feature branch: docker run --detach --name pmm-server --publish 80 :80 --publish 443 :443 --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 ; Important Use --network minikube if running PMM Server and minikube in the same Docker instance. This way they will share single network and the kubeconfig will work. Use Docker variables --env PMM_DEBUG=1 --env PMM_TRACE=1 to see extended debug details. Change the default administrator credentials: Note This step is optional, because the same can be done from the web interface of PMM on the first login. docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Set the public address for PMM Server in PMM settings UI Follow the steps for Add a Kubernetes cluster . Follow the steps for Add a DB Cluster . Get the IP address to connect your app/service: minikube kubectl get services Exposing PSMDB and XtraDB clusters for access by external clients To make services visible externally, you create a LoadBalancer service or manually run commands to expose ports: kubectl expose deployment hello-world --type = NodePort. See also DBaaS Dashboard Install minikube Setting up a Standalone MYSQL Instance on Kubernetes & exposing it using Nginx Ingress Controller Use a Service to Access an Application in a Cluster. Exposing applications using services.","title":"Setting up a development environment for DBaaS"},{"location":"setting-up/server/dbaas.html#software-prerequisites","text":"","title":"Software prerequisites"},{"location":"setting-up/server/dbaas.html#docker","text":"Red Hat, CentOS yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce usermod -a -G docker centos systemctl enable docker systemctl start docker Debian, Ubuntu apt-add-repository https://download.docker.com/linux/centos/docker-ce.repo systemctl enable docker systemctl start docker","title":"Docker"},{"location":"setting-up/server/dbaas.html#minikube","text":"Please install minikube 1.16.0 Red Hat, CentOS yum -y install curl curl -Lo /usr/local/sbin/minikube https://github.com/kubernetes/minikube/releases/download/v1.16.0/minikube-linux-amd64 chmod +x /usr/local/sbin/minikube ln -s /usr/local/sbin/minikube /usr/sbin/minikube alias kubectl = 'minikube kubectl --'","title":"minikube"},{"location":"setting-up/server/dbaas.html#start-pmm-server-with-dbaas-activated","text":"Notes To start a fully-working 3 node XtraDB cluster, consisting of sets of 3x ProxySQL, 3x PXC and 6x PMM Client containers, you will need at least 9 vCPU available for minikube. (1x vCPU for ProxySQL and PXC and 0.5vCPU for each pmm-client containers). DBaaS does not depend on PMM Client. Setting the environment variable PERCONA_TEST_DBAAS=1 enables DBaaS functionality. Add the option --network minikube if you run PMM Server and minikube in the same Docker instance. (This will share a single network and the kubeconfig will work.) Add the options --env PMM_DEBUG=1 and/or --env PMM_TRACE=1 if you need extended debug details Start PMM server: docker run --detach --publish 80 :80 --publish 443 :443 --name pmm-server --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 Change the default administrator credentials from CLI: (This step is optional, because the same can be done from the web interface of PMM on first login.) docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>'","title":"Start PMM server with DBaaS activated"},{"location":"setting-up/server/dbaas.html#install-percona-operators-in-minikube","text":"Configure and start minikube: minikube config set cpus 16 minikube config set memory 32768 minikube config set kubernetes-version 1 .16.15 minikube start Deploy the Percona operators configuration for PXC and PSMDB in minikube: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*/pmmserver: ${ PMM_PASS } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - Check the operators are deployed: minikube kubectl -- get nodes minikube kubectl -- get pods minikube kubectl -- wait --for = condition = Available deployment percona-xtradb-cluster-operator minikube kubectl -- wait --for = condition = Available deployment percona-server-mongodb-operator Get your kubeconfig details from minikube (to register your Kubernetes cluster with PMM Server): minikube kubectl -- config view --flatten --minify Note You will need to copy this output to your clipboard and continue with add a Kubernetes cluster to PMM .","title":"Install Percona operators in minikube"},{"location":"setting-up/server/dbaas.html#installing-percona-operators-in-aws-eks-kubernetes","text":"Create your cluster via eksctl or the Amazon AWS interface. For example: eksctl create cluster --write-kubeconfig --name = your-cluster-name --zones = us-west-2a,us-west-2b --kubeconfig <PATH_TO_KUBECONFIG> When your EKS cluster is running, install the PXC and PSMDB operators: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/v1.7.0/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - # Validate that the operators are running kubectl get pods Modify your kubeconfig file, if it\u2019s not utilizing the aws-iam-authenticator or client-certificate method for authentication with Kubernetes. Here are two examples that you can use as templates to modify a copy of your existing kubeconfig: For the aws-iam-authenticator method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_USER >> user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 command: aws-iam-authenticator args: - \"token\" - \"-i\" - \"<< K8S_CLUSTER_NAME >>\" - --region - << AWS_REGION >> env: - name: AWS_ACCESS_KEY_ID value: \"<< AWS_ACCESS_KEY_ID >>\" - name: AWS_SECRET_ACCESS_KEY value: \"<< AWS_SECRET_ACCESS_KEY >>\" For the client-certificate method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_NAME >> user: client-certificate-data: << CLIENT_CERT_DATA >> client-key-data: << CLIENT_KEY_DATA >> Follow the instructions for Add a Kubernetes cluster . Note If possible, the connection details will show the cluster\u2019s external IP (not possible with minikube).","title":"Installing Percona operators in AWS EKS (Kubernetes)"},{"location":"setting-up/server/dbaas.html#install-operators-on-gke","text":"Caution These instructions are still in development. Prerequisites You should have an account on GCP https://cloud.google.com/ . Login into google cloud platform console https://console.cloud.google.com/ Navigate to Menu \u2192 Kubernetes Engine \u2192 Clusters Click button Create cluster You can specify cluster option in form or simply click on \u201cMy first cluster\u201d and button Create Wait until cluster created Click on button Connect in a the cluster\u2019s row Click button Run in Cloud shell Click Authorize Set up PXC and PSMDB operators: curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - Check if it was set up successfully kubectl api-resources --api-group='psmdb.percona.com' kubectl api-resources --api-group='pxc.percona.com' Check versions kubectl api-versions | grep percona.com Create Service Account, copy and store kubeconfig - output of the following command cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: percona-dbaas-cluster-operator --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-xtradb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-xtradb-cluster-operator apiGroup: rbac.authorization.k8s.io --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-psmdb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-server-mongodb-operator apiGroup: rbac.authorization.k8s.io EOF name=`kubectl get serviceAccounts percona-dbaas-cluster-operator -o json | jq -r .secrets[].name` certificate=`kubectl get secret $name -o json | jq -r '.data.\"ca.crt\"'` token=`kubectl get secret $name -o json | jq -r '.data.token' | base64 -d` server=`kubectl cluster-info | grep 'Kubernetes master' | cut -d ' ' -f 6` echo \" apiVersion: v1 kind: Config users: - name: percona-dbaas-cluster-operator user: token: $token clusters: - cluster: certificate-authority-data: $certificate server: $server name: self-hosted-cluster contexts: - context: cluster: self-hosted-cluster user: percona-dbaas-cluster-operator name: svcs-acct-context current-context: svcs-acct-context \" Start PMM Server on you local machine or other VM instance: docker run --detach --name pmm-server --publish 80:80 --publish 443:443 \\ --env PERCONA_TEST_DBAAS=1 perconalab/pmm-server-fb:PR-1240-07bef94; Login into PMM and navigate to DBaaS Register your GKE using kubeconfig from step 12. Important Please make sure there are no stray new lines in the kubeconfig, especially in long lines like certificate or token.","title":"Install Operators on GKE"},{"location":"setting-up/server/dbaas.html#deleting-clusters","text":"You should delete all installation operators as the operators own resources. If you only run eksctl delete cluster without cleaning up the cluster first, there will be a lot of orphaned resources as Cloud Formations, Load Balancers, EC2 instances, Network interfaces, etc. In the pmm-managed repository, in the deploy directory there are 2 example bash scripts to install and delete the operators from the EKS cluster. The install script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Install the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } apply -f - # Install the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } apply -f - The delete script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Delete the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } delete -f - # Delete the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } delete -f - (Both scripts are similar except the install script command is apply while in the delete script it is delete .) After deleting everything in the EKS cluster, run this command (using your own configuration path) and wait until the output only shows service/kubernetes before deleting the cluster with the eksclt delete command. kubectl --kubeconfig ~/.kube/config_eks get all Example output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 4d5h If you don\u2019t need the cluster anymore, you can uninstall everything in it and destroy it: # Delete all volumes created by the operators: kubectl [ --kubeconfig <config file> ] delete pvc --all # Delete the cluster eksctl delete cluster --name = your-cluster-name","title":"Deleting clusters"},{"location":"setting-up/server/dbaas.html#run-pmm-server-as-a-docker-container-for-dbaas","text":"Start PMM server from a feature branch: docker run --detach --name pmm-server --publish 80 :80 --publish 443 :443 --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 ; Important Use --network minikube if running PMM Server and minikube in the same Docker instance. This way they will share single network and the kubeconfig will work. Use Docker variables --env PMM_DEBUG=1 --env PMM_TRACE=1 to see extended debug details. Change the default administrator credentials: Note This step is optional, because the same can be done from the web interface of PMM on the first login. docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Set the public address for PMM Server in PMM settings UI Follow the steps for Add a Kubernetes cluster . Follow the steps for Add a DB Cluster . Get the IP address to connect your app/service: minikube kubectl get services","title":"Run PMM Server as a Docker container for DBaaS"},{"location":"setting-up/server/dbaas.html#exposing-psmdb-and-xtradb-clusters-for-access-by-external-clients","text":"To make services visible externally, you create a LoadBalancer service or manually run commands to expose ports: kubectl expose deployment hello-world --type = NodePort. See also DBaaS Dashboard Install minikube Setting up a Standalone MYSQL Instance on Kubernetes & exposing it using Nginx Ingress Controller Use a Service to Access an Application in a Cluster. Exposing applications using services.","title":"Exposing PSMDB and XtraDB clusters for access by external clients"},{"location":"setting-up/server/docker.html","text":"Docker Percona maintains a Docker image for PMM Server . This section shows how to run PMM Server as a Docker container. (The tags used here are for the latest version of PMM 2 (2.15.0). Other tags are available .) Before you start You need Docker 1.12.6 or higher. Your system needs approximately 1GB of storage for each monitored database node with data retention set to one week. (By default, data retention is 30 days.) To reduce the size of the VictoriaMetrics database, you can consider disabling table statistics. The minimum amount of memory is 2 GB for one monitored database node. (Memory usage does not grow in proportion to the number of nodes. For example, 16GB is adequate for 20 nodes.) Running PMM Client as a Docker container Pull the image. docker pull percona/pmm-server:2 Create a persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2 /bin/true PMM Server expects the data volume (specified with --volume ) to be /srv . Using any other value will result in data loss when upgrading. Run the image to start PMM Server. docker run --detach --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data --name pmm-server \\ percona/pmm-server:2 You can disable manual updates via the Home Dashboard PMM Upgrade panel by adding -e DISABLE_UPDATES=true to the docker run command. In a web browser, visit server hostname :80 or server hostname :443 to see the PMM user interface. Backup and upgrade You can test a new release of the PMM Server Docker image by making backups of your current pmm-server and pmm-data containers which you can restore if you need to. Find out which release you have now. docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version Tip Use jq to extract the quoted string value. sudo apt install jq # Example for Debian, Ubuntu docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version | jq .version Check the container mount points are the same ( /srv ). docker inspect pmm-data | grep Destination docker inspect pmm-server | grep Destination With jq : docker inspect pmm-data | jq '.[].Mounts[].Destination' docker inspect pmm-server | jq '.[].Mounts[].Destination' Stop the container and create backups. docker stop pmm-server docker rename pmm-server pmm-server-backup mkdir pmm-data-backup && cd $_ docker cp pmm-data:/srv . Pull and run the latest image. docker pull percona/pmm-server:2 docker run \\ --detach \\ --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 (Optional) Repeat step 1 to confirm the version, or check the PMM Upgrade panel on the Home Dashboard . Restore Stop and remove the running version. docker stop pmm-server docker rm pmm-server Restore backups. docker rename pmm-server-backup pmm-server # cd to wherever you saved the backup docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start (don\u2019t run) the image. docker start pmm-server","title":"Docker"},{"location":"setting-up/server/docker.html#before-you-start","text":"You need Docker 1.12.6 or higher. Your system needs approximately 1GB of storage for each monitored database node with data retention set to one week. (By default, data retention is 30 days.) To reduce the size of the VictoriaMetrics database, you can consider disabling table statistics. The minimum amount of memory is 2 GB for one monitored database node. (Memory usage does not grow in proportion to the number of nodes. For example, 16GB is adequate for 20 nodes.)","title":"Before you start"},{"location":"setting-up/server/docker.html#running-pmm-client-as-a-docker-container","text":"Pull the image. docker pull percona/pmm-server:2 Create a persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2 /bin/true PMM Server expects the data volume (specified with --volume ) to be /srv . Using any other value will result in data loss when upgrading. Run the image to start PMM Server. docker run --detach --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data --name pmm-server \\ percona/pmm-server:2 You can disable manual updates via the Home Dashboard PMM Upgrade panel by adding -e DISABLE_UPDATES=true to the docker run command. In a web browser, visit server hostname :80 or server hostname :443 to see the PMM user interface.","title":"Running PMM Client as a Docker container"},{"location":"setting-up/server/docker.html#backup-and-upgrade","text":"You can test a new release of the PMM Server Docker image by making backups of your current pmm-server and pmm-data containers which you can restore if you need to. Find out which release you have now. docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version Tip Use jq to extract the quoted string value. sudo apt install jq # Example for Debian, Ubuntu docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version | jq .version Check the container mount points are the same ( /srv ). docker inspect pmm-data | grep Destination docker inspect pmm-server | grep Destination With jq : docker inspect pmm-data | jq '.[].Mounts[].Destination' docker inspect pmm-server | jq '.[].Mounts[].Destination' Stop the container and create backups. docker stop pmm-server docker rename pmm-server pmm-server-backup mkdir pmm-data-backup && cd $_ docker cp pmm-data:/srv . Pull and run the latest image. docker pull percona/pmm-server:2 docker run \\ --detach \\ --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 (Optional) Repeat step 1 to confirm the version, or check the PMM Upgrade panel on the Home Dashboard .","title":"Backup and upgrade"},{"location":"setting-up/server/docker.html#restore","text":"Stop and remove the running version. docker stop pmm-server docker rm pmm-server Restore backups. docker rename pmm-server-backup pmm-server # cd to wherever you saved the backup docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start (don\u2019t run) the image. docker start pmm-server","title":"Restore"},{"location":"setting-up/server/virtual-appliance.html","text":"Virtual Appliance Run PMM Server as a virtual machine by downloading and importing the PMM 2.15.0 Open Virtual Appliance (OVA) file into any virtualization software supporting the OVF standard . This page shows how to set up PMM Server as a virtual machine in VMware Workstation Player and Oracle VM VirtualBox . Most steps can be done with either a user interface or on the command line, but some steps can only be done in one or the other. Sections are marked with UI for user interface or CLI for command line instructions. Terminology Host is the desktop or server machine running the hypervisor. Hypervisor is software (e.g. VirtualBox, VMware) that runs the guest OS as a virtual machine. Guest is the CentOS virtual machine that runs PMM Server. OVA file details Download page: https://www.percona.com/downloads/pmm2/2.15.0/ova File name: pmm-server-2.15.0.ova VM name: PMM2-Server-2021-03-01-N ( N =build number) VM specifications: CentOS 7.9 (64-bit) CPU: 1 Base memory: 4096 MB Disks: LVM, 2 physical volumes Disk 1 (sda): VMDK (SCSI, 40 GB) Disk 2 (sdb): VMDK (SCSI, 400 GB) Username/default password: root / percona admin / admin 1. Download UI Open a web browser. Visit the PMM Server download page . Choose a Version or use the default (the latest). Click the link for pmm-server-2.15.0.ova to download it. Note where your browser saves it. Right click the link for pmm-server-2.15.0.sha256sum and save it in the same place as the .ova file. (Optional) Verify . CLI Download the latest PMM Server OVA and checksum files wget https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.ova wget https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.sha256sum 2. Verify CLI Verify the checksum of the downloaded .ova file shasum -ca 256 pmm-server-2.15.0.sha256sum 3. VMware Workstation Player 3.1. Import UI Select File \u2192 Import . Click Choose file\u2026 . Navigate to the downloaded .ova file and select it. Click Open . Click Continue . In the Save as dialog: a. (Optional) Change the directory or file name. b. Click Save . Choose one of: (Optional) Click Finish . This starts the virtual machine. (Recommended) Click Customize Settings . This opens the VM\u2019s settings page without starting the machine. CLI Install ovftool . (You need to register.) Import and convert the OVA file. ( ovftool can\u2019t change CPU or memory settings during import but it can set the default interface.) Choose one of: - Download and import the OVA file. ovftool --name = \"PMM Server\" --net:NAT = Wi-Fi \\ https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.ova \\ pmm-server-2.15.0.vmx - Import an already-downloaded OVA file. ovftool --name = \"PMM Server\" --net:NAT = WiFi \\ pmm-server-2.15.0.ova \\ pmm-server.vmx 3.2. Reconfigure interface Notes When using the command line, the interface is remapped during import. UI If started, shut down the virtual machine. In the VMware main window, select the imported virtual machine. Click Virtual Machine \u2192 Settings\u2026 Click Network Adapter . In the Bridged Networking section, select Autodetect . Close the settings window. 3.3. Start guest and get IP address UI In the VMware main window, select the imported virtual machine. Click the play button or select Virtual Machine \u2192 Start Up . When the instance has booted, note the IP address in the guest console. CLI/UI Start the virtual machine in GUI mode. (There\u2019s no way to redirect a VMware VM\u2019s console to the host.) vmrun -gu root -gp percona start \\ pmm-server.vmx gui When the instance has booted, note the IP address in the guest console. (Optional) Stop and restart the instance in headless mode. vmrun stop pmm-server.vmx vmrun -gu root -gp percona start \\ pmm-server.vmx nogui 4. Oracle VM VirtualBox 4.1. Import UI Select File \u2192 Import appliance\u2026 . In the File field, type the path to the downloaded .ova file, or click the folder icon to navigate and open it. Click Continue . On the Appliance settings page, review the settings and click Import . Click Start . When the guest has booted, note the IP address in the guest console. CLI Open a terminal and change directory to where the downloaded .ova file is. (Optional) Do a \u2018dry run\u2019 import to see what values will be used. VBoxManage import pmm-server-2.15.0.ova --dry-run Import the image. Choose one of: With the default settings. VBoxManage import pmm-server-2.15.0.ova With custom settings (in this example, Name: \u201cPMM Server\u201d, CPUs: 2, RAM: 8192 MB). VBoxManage import --vsys 0 --vmname \"PMM Server\" \\ --cpus 2 --memory 8192 pmm-server-2.15.0.ova 4.2. Reconfigure interface UI Click Settings . Click Network . In the Adapter 1 field, click Attached to and change to Bridged Adapter . In the Name field, select your host\u2019s active network interface (e.g. en0: Wi-Fi (Wireless) ). Click OK . CLI Show the list of available bridge interfaces. VBoxManage list bridgedifs Find the name of the active interface you want to bridge to (one with Status: Up and a valid IP address). Example: en0: Wi-Fi (Wireless) Bridge the virtual machine\u2019s first interface ( nic1 ) to the host\u2019s en0 ethernet adapter. VBoxManage modifyvm 'PMM Server' \\ --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (Wireless)' Redirect the console output into a host file. VBoxManage modifyvm 'PMM Server' \\ --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log 4.3. Start guest and get IP address UI Select the PMM Server virtual machine in the list. Click Start . When the guest has booted, note the IP address in the guest console. CLI Start the guest. VBoxManage startvm --type headless 'PMM Server' (Optional) Watch the log file. tail -f /tmp/pmm-server-console.log Wait for one minute for the server to boot up. Choose one of: Read the IP address from the tailed log file. Extract the IP address from the log file. grep -e \"^IP:\" /tmp/pmm-server-console.log | cut -f2 -d ' ' (Optional) Stop the guest: VBoxManage controlvm \"PMM Server\" poweroff 5. Log into PMM user interface UI Open a web browser and visit the guest IP address. The PMM login screen appears. Enter the default username and password in the relevant fields and click Log in . username: admin password: admin (Recommended) Follow the prompts to change the default password. The PMM Home Dashboard appears. 6. (Optional) Change root password UI Start the virtual machine in GUI mode. Log in with the default superuser credentials: Username: root Password: percona Follow the prompts to change the password. 7. (Optional) Set up SSH UI/CLI Create a key pair for the admin user. ssh-keygen -f admin Log into the PMM user interface . Select PMM \u2192 PMM Settings \u2192 SSH Key . Copy and paste the contents of the admin.pub file into the SSH Key field. Click Apply SSH Key . (This copies the public key to /home/admin/.ssh/authorized_keys in the guest). Log in via SSH ( N.N.N.N is the guest IP address). ssh -i admin admin@N.N.N.N 8. (Optional) Set up static IP When the guest OS starts, it will get an IP address from the hypervisor\u2019s DHCP server. This IP can change each time the guest OS is restarted. Setting a static IP for the guest OS avoids having to check the IP address whenever the guest is restarted. CLI Start the virtual machine in non-headless (GUI) mode. Log in as root . Edit /etc/sysconfig/network-scripts/ifcfg-eth0 Change the value of BOOTPROTO : BOOTPROTO = none Add these values: IPADDR = 192.168.1.123 # Example NETMASK = 255.255.255.0 GATEWAY = 192.168.1.1 Restart the interface. ifdown eth0 && ifup eth0 Check the IP. ip addr show eth0","title":"Virtual Appliance"},{"location":"setting-up/server/virtual-appliance.html#1-download","text":"UI Open a web browser. Visit the PMM Server download page . Choose a Version or use the default (the latest). Click the link for pmm-server-2.15.0.ova to download it. Note where your browser saves it. Right click the link for pmm-server-2.15.0.sha256sum and save it in the same place as the .ova file. (Optional) Verify . CLI Download the latest PMM Server OVA and checksum files wget https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.ova wget https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.sha256sum","title":"1. Download"},{"location":"setting-up/server/virtual-appliance.html#2-verify","text":"CLI Verify the checksum of the downloaded .ova file shasum -ca 256 pmm-server-2.15.0.sha256sum","title":"2. Verify"},{"location":"setting-up/server/virtual-appliance.html#3-vmware-workstation-player","text":"","title":"3. VMware Workstation Player"},{"location":"setting-up/server/virtual-appliance.html#31-import","text":"UI Select File \u2192 Import . Click Choose file\u2026 . Navigate to the downloaded .ova file and select it. Click Open . Click Continue . In the Save as dialog: a. (Optional) Change the directory or file name. b. Click Save . Choose one of: (Optional) Click Finish . This starts the virtual machine. (Recommended) Click Customize Settings . This opens the VM\u2019s settings page without starting the machine. CLI Install ovftool . (You need to register.) Import and convert the OVA file. ( ovftool can\u2019t change CPU or memory settings during import but it can set the default interface.) Choose one of: - Download and import the OVA file. ovftool --name = \"PMM Server\" --net:NAT = Wi-Fi \\ https://www.percona.com/downloads/pmm2/2.15.0/ova/pmm-server-2.15.0.ova \\ pmm-server-2.15.0.vmx - Import an already-downloaded OVA file. ovftool --name = \"PMM Server\" --net:NAT = WiFi \\ pmm-server-2.15.0.ova \\ pmm-server.vmx","title":"3.1. Import"},{"location":"setting-up/server/virtual-appliance.html#32-reconfigure-interface","text":"Notes When using the command line, the interface is remapped during import. UI If started, shut down the virtual machine. In the VMware main window, select the imported virtual machine. Click Virtual Machine \u2192 Settings\u2026 Click Network Adapter . In the Bridged Networking section, select Autodetect . Close the settings window.","title":"3.2. Reconfigure interface"},{"location":"setting-up/server/virtual-appliance.html#33-start-guest-and-get-ip-address","text":"UI In the VMware main window, select the imported virtual machine. Click the play button or select Virtual Machine \u2192 Start Up . When the instance has booted, note the IP address in the guest console. CLI/UI Start the virtual machine in GUI mode. (There\u2019s no way to redirect a VMware VM\u2019s console to the host.) vmrun -gu root -gp percona start \\ pmm-server.vmx gui When the instance has booted, note the IP address in the guest console. (Optional) Stop and restart the instance in headless mode. vmrun stop pmm-server.vmx vmrun -gu root -gp percona start \\ pmm-server.vmx nogui","title":"3.3. Start guest and get IP address"},{"location":"setting-up/server/virtual-appliance.html#4-oracle-vm-virtualbox","text":"","title":"4. Oracle VM VirtualBox"},{"location":"setting-up/server/virtual-appliance.html#41-import","text":"UI Select File \u2192 Import appliance\u2026 . In the File field, type the path to the downloaded .ova file, or click the folder icon to navigate and open it. Click Continue . On the Appliance settings page, review the settings and click Import . Click Start . When the guest has booted, note the IP address in the guest console. CLI Open a terminal and change directory to where the downloaded .ova file is. (Optional) Do a \u2018dry run\u2019 import to see what values will be used. VBoxManage import pmm-server-2.15.0.ova --dry-run Import the image. Choose one of: With the default settings. VBoxManage import pmm-server-2.15.0.ova With custom settings (in this example, Name: \u201cPMM Server\u201d, CPUs: 2, RAM: 8192 MB). VBoxManage import --vsys 0 --vmname \"PMM Server\" \\ --cpus 2 --memory 8192 pmm-server-2.15.0.ova","title":"4.1. Import"},{"location":"setting-up/server/virtual-appliance.html#42-reconfigure-interface","text":"UI Click Settings . Click Network . In the Adapter 1 field, click Attached to and change to Bridged Adapter . In the Name field, select your host\u2019s active network interface (e.g. en0: Wi-Fi (Wireless) ). Click OK . CLI Show the list of available bridge interfaces. VBoxManage list bridgedifs Find the name of the active interface you want to bridge to (one with Status: Up and a valid IP address). Example: en0: Wi-Fi (Wireless) Bridge the virtual machine\u2019s first interface ( nic1 ) to the host\u2019s en0 ethernet adapter. VBoxManage modifyvm 'PMM Server' \\ --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (Wireless)' Redirect the console output into a host file. VBoxManage modifyvm 'PMM Server' \\ --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log","title":"4.2. Reconfigure interface"},{"location":"setting-up/server/virtual-appliance.html#43-start-guest-and-get-ip-address","text":"UI Select the PMM Server virtual machine in the list. Click Start . When the guest has booted, note the IP address in the guest console. CLI Start the guest. VBoxManage startvm --type headless 'PMM Server' (Optional) Watch the log file. tail -f /tmp/pmm-server-console.log Wait for one minute for the server to boot up. Choose one of: Read the IP address from the tailed log file. Extract the IP address from the log file. grep -e \"^IP:\" /tmp/pmm-server-console.log | cut -f2 -d ' ' (Optional) Stop the guest: VBoxManage controlvm \"PMM Server\" poweroff","title":"4.3. Start guest and get IP address"},{"location":"setting-up/server/virtual-appliance.html#5-log-into-pmm-user-interface","text":"UI Open a web browser and visit the guest IP address. The PMM login screen appears. Enter the default username and password in the relevant fields and click Log in . username: admin password: admin (Recommended) Follow the prompts to change the default password. The PMM Home Dashboard appears.","title":"5. Log into PMM user interface"},{"location":"setting-up/server/virtual-appliance.html#6-optional-change-root-password","text":"UI Start the virtual machine in GUI mode. Log in with the default superuser credentials: Username: root Password: percona Follow the prompts to change the password.","title":"6. (Optional) Change root password"},{"location":"setting-up/server/virtual-appliance.html#7-optional-set-up-ssh","text":"UI/CLI Create a key pair for the admin user. ssh-keygen -f admin Log into the PMM user interface . Select PMM \u2192 PMM Settings \u2192 SSH Key . Copy and paste the contents of the admin.pub file into the SSH Key field. Click Apply SSH Key . (This copies the public key to /home/admin/.ssh/authorized_keys in the guest). Log in via SSH ( N.N.N.N is the guest IP address). ssh -i admin admin@N.N.N.N","title":"7. (Optional) Set up SSH"},{"location":"setting-up/server/virtual-appliance.html#8-optional-set-up-static-ip","text":"When the guest OS starts, it will get an IP address from the hypervisor\u2019s DHCP server. This IP can change each time the guest OS is restarted. Setting a static IP for the guest OS avoids having to check the IP address whenever the guest is restarted. CLI Start the virtual machine in non-headless (GUI) mode. Log in as root . Edit /etc/sysconfig/network-scripts/ifcfg-eth0 Change the value of BOOTPROTO : BOOTPROTO = none Add these values: IPADDR = 192.168.1.123 # Example NETMASK = 255.255.255.0 GATEWAY = 192.168.1.1 Restart the interface. ifdown eth0 && ifup eth0 Check the IP. ip addr show eth0","title":"8. (Optional) Set up static IP"},{"location":"using/index.html","text":"Using User Interface Using the web-based user interface Finding dashboards Rendering dashboard images Viewing graph details Annotating events Integrated alerting Query Analytics , a specialized dashboard for detailed query analysis Percona Enterprise Platform Security Threat Tool : Enabling and seeing the results of database security checks","title":"Using"},{"location":"using/alerting.html","text":"Integrated Alerting Integrated Alerting lets you know when certain system events occur. Warning Integrated Alerting is a technical preview and is subject to change. To activate Integrated Alerting , select PMM\u2192PMM Settings\u2192Advanced Settings , turn on Integrated Alerting and click Apply changes . Definitions Alerts are generated when their criteria ( alert rules ) are met; an alert is the result of an alert rule expression evaluating to true . Alert rules are based on alert rule templates . We provide a default set of templates. You can also create your own. Note PMM\u2019s Integrated Alerting is a customized and separate instance of the Prometheus Alertmanager, and distinct from Grafana\u2019s alerting functionality. Prerequisites Set up a communication channel: When the Communication tab appears, select it. Enter details for Email or Slack . ( Read more ) Open the Integrated Alerting page From the left menu, select Alerting , Integrated Alerting Note The Alerting menu also lists Alert Rules and Notification Channels . These are for Grafana\u2019s alerting functionality. This page has four tabs. Alerts : Shows alerts (if any). Alert Rules : Shows rule definitions. Alert Rule Templates : Lists rule templates. Notification Channels : Lists notification channels. Add a Notification Channel On the Integrated Alerting page, go to the Notification Channels tab. Click Add . Fill in the details: Name Type Email: Addresses Pager Duty Routing key Service key Slack Channel Click Add to add the notification channel, or Cancel to abort the operation. Add an Alert Rule On the Integrated Alerting page, go to the Alert Rules tab. Click Add . Fill in the details Template Name Threshold Duration(s) Severity Filters Channels Activate Click Add to add the alert rule, or Cancel to abort the operation. Add an Alert Rule Template On the Integrated Alerting page, go to the Alert Rule Templates tab. Click Add . Enter a template in the Alert Rule Template text box. --- templates: - name: mysql_too_many_connections version: 1 summary: MySQL connections in use tiers: [anonymous, registered] expr: |- max_over_time(mysql_global_status_threads_connected[5m]) / ignoring (job) mysql_global_variables_max_connections * 100 > [[ .threshold ]] params: - name: threshold summary: A percentage from configured maximum unit: '%' type: float range: [0, 100] value: 80 for: 5m severity: warning labels: foo: bar annotations: description: |- More than [[ .threshold ]]% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }} LABELS: {{ $labels }} summary: MySQL too many connections (instance {{ $labels.instance }}) Click Add to add the alert rule template, or Cancel to abort the operation.","title":"Integrated Alerting"},{"location":"using/alerting.html#definitions","text":"Alerts are generated when their criteria ( alert rules ) are met; an alert is the result of an alert rule expression evaluating to true . Alert rules are based on alert rule templates . We provide a default set of templates. You can also create your own. Note PMM\u2019s Integrated Alerting is a customized and separate instance of the Prometheus Alertmanager, and distinct from Grafana\u2019s alerting functionality.","title":"Definitions"},{"location":"using/alerting.html#prerequisites","text":"Set up a communication channel: When the Communication tab appears, select it. Enter details for Email or Slack . ( Read more )","title":"Prerequisites"},{"location":"using/alerting.html#open-the-integrated-alerting-page","text":"From the left menu, select Alerting , Integrated Alerting Note The Alerting menu also lists Alert Rules and Notification Channels . These are for Grafana\u2019s alerting functionality. This page has four tabs. Alerts : Shows alerts (if any). Alert Rules : Shows rule definitions. Alert Rule Templates : Lists rule templates. Notification Channels : Lists notification channels.","title":"Open the Integrated Alerting page"},{"location":"using/alerting.html#add-a-notification-channel","text":"On the Integrated Alerting page, go to the Notification Channels tab. Click Add . Fill in the details: Name Type Email: Addresses Pager Duty Routing key Service key Slack Channel Click Add to add the notification channel, or Cancel to abort the operation.","title":"Add a Notification Channel"},{"location":"using/alerting.html#add-an-alert-rule","text":"On the Integrated Alerting page, go to the Alert Rules tab. Click Add . Fill in the details Template Name Threshold Duration(s) Severity Filters Channels Activate Click Add to add the alert rule, or Cancel to abort the operation.","title":"Add an Alert Rule"},{"location":"using/alerting.html#add-an-alert-rule-template","text":"On the Integrated Alerting page, go to the Alert Rule Templates tab. Click Add . Enter a template in the Alert Rule Template text box. --- templates: - name: mysql_too_many_connections version: 1 summary: MySQL connections in use tiers: [anonymous, registered] expr: |- max_over_time(mysql_global_status_threads_connected[5m]) / ignoring (job) mysql_global_variables_max_connections * 100 > [[ .threshold ]] params: - name: threshold summary: A percentage from configured maximum unit: '%' type: float range: [0, 100] value: 80 for: 5m severity: warning labels: foo: bar annotations: description: |- More than [[ .threshold ]]% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }} LABELS: {{ $labels }} summary: MySQL too many connections (instance {{ $labels.instance }}) Click Add to add the alert rule template, or Cancel to abort the operation.","title":"Add an Alert Rule Template"},{"location":"using/interface.html","text":"User Interface PMM\u2019s user interface is a browser application based on Grafana . This page explains how to log in, how the user interface is laid out, and what the controls do. Logging in Dashboards Controls Navigation Panels Logging in Start a web browser and in the address bar enter the server name or IP address of the PMM server host. The page loads showing the PMM login screen. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Enter a new password in both fields and click Submit . You can click Skip to carry on with the default password. The PMM Home dashboard loads. (This is an example taken from https://pmmdemo.percona.com/ . Yours may be different.) Dashboards The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, by renaming them or creating new ones. The area inside dashboards is populated by panels . Some are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set. Controls These menus and controls appear on all dashboards: Main menu (also Grafana menu , side menu ) Navigation bar View controls View selectors (dynamic contents) Shortcut menu (dynamic contents) See also User interface components Navigation Search for a dashboard by name There are two ways to open the dashboard search page. (Each takes you to the same search screen.) Click the icon in the main menu. Click the dashboard name in the navigation bar (top row, to the right of the icon). (To search within the current folder, click the folder name instead of the dashboard name.) Click Search dashboards by name and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d). Click one of the search results to go to that dashboard. Change the search text to refine the list. To abandon the search, click the icon at the end of the search bar. Open a dashboard with the menu The shortcut menu has links to commonly-used dashboards. Panels Charts, graphs and set-based panels reveal extra information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information. Panel menu At the top of each panel and to the right of the panel name is the panel menu . Tip The menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode Share Render the panel\u2019s image for sharing Explore Run PromQL queries Inspect See the panel\u2019s data or definition More (Only charts and graphs) Additional options View The View menu items opens panels in full-window mode. This is useful for graphs with several metrics. Exit a panel\u2019s full window mode by pressing Escape or clicking the left arrow next to the dashboard name. See also How to render dashboard images How to annotate special events","title":"User Interface"},{"location":"using/interface.html#logging-in","text":"Start a web browser and in the address bar enter the server name or IP address of the PMM server host. The page loads showing the PMM login screen. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Enter a new password in both fields and click Submit . You can click Skip to carry on with the default password. The PMM Home dashboard loads. (This is an example taken from https://pmmdemo.percona.com/ . Yours may be different.)","title":"Logging in"},{"location":"using/interface.html#dashboards","text":"The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, by renaming them or creating new ones. The area inside dashboards is populated by panels . Some are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set.","title":"Dashboards"},{"location":"using/interface.html#controls","text":"These menus and controls appear on all dashboards: Main menu (also Grafana menu , side menu ) Navigation bar View controls View selectors (dynamic contents) Shortcut menu (dynamic contents) See also User interface components","title":"Controls"},{"location":"using/interface.html#navigation","text":"Search for a dashboard by name There are two ways to open the dashboard search page. (Each takes you to the same search screen.) Click the icon in the main menu. Click the dashboard name in the navigation bar (top row, to the right of the icon). (To search within the current folder, click the folder name instead of the dashboard name.) Click Search dashboards by name and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d). Click one of the search results to go to that dashboard. Change the search text to refine the list. To abandon the search, click the icon at the end of the search bar. Open a dashboard with the menu The shortcut menu has links to commonly-used dashboards.","title":"Navigation"},{"location":"using/interface.html#panels","text":"Charts, graphs and set-based panels reveal extra information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information.","title":"Panels"},{"location":"using/interface.html#panel-menu","text":"At the top of each panel and to the right of the panel name is the panel menu . Tip The menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode Share Render the panel\u2019s image for sharing Explore Run PromQL queries Inspect See the panel\u2019s data or definition More (Only charts and graphs) Additional options","title":"Panel menu"},{"location":"using/query-analytics.html","text":"Query Analytics The Query Analytics dashboard shows how queries are executed and where they spend their time. It helps you analyze database queries over time, optimize database performance, and find and remedy the source of problems. Note Query Analytics supports MySQL, MongoDB and PostgreSQL. The minimum requirements for MySQL are: MySQL 5.1 or later (if using the slow query log) MySQL 5.6.9 or later (if using Performance Schema) Query Analytics data retrieval is not instantaneous and can be delayed due to network conditions. In such situations no data is reported and a gap appears in the sparkline. Query Analytics displays metrics in both visual and numeric form. Performance-related characteristics appear as plotted graphics with summaries. The dashboard contains three panels: Filters Panel Overview Panel Details Panel Filters Panel The Filter panel occupies the left side of the dashboard. It lists filters, grouped by category. Selecting one reduces the Overview list to those items matching the filter. The first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol ( ) to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the predefined Relative time ranges . Overview Panel To the right of the Filters panel and occupying the upper part of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query Service Name Database Schema User Name Client Host At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again. Columns The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips. Tool-tips For the Query dimension, hovering over the information icon ( ) reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value. Adding and removing columns Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric . Sorting The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values. Pagination The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items. Details Panel Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab . Details Tab The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on one of the follow named activities: query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with these headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over. Rate/Second : A sparkline chart of real-time values per unit time. Sum : A summation of the metric for the selected query, and the percentage of the total. Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension. Examples Tab (For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element. Explain Tab (For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats: MySQL: Classic and JSON MongoDB: JSON only PostgreSQL: Not supported Tables Tab (For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query. Query Analytics for MongoDB MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking tables. In contrast, MongoDB uses the concept of a document where all essential information for a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools.","title":"Query Analytics"},{"location":"using/query-analytics.html#filters-panel","text":"The Filter panel occupies the left side of the dashboard. It lists filters, grouped by category. Selecting one reduces the Overview list to those items matching the filter. The first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol ( ) to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the predefined Relative time ranges .","title":"Filters Panel"},{"location":"using/query-analytics.html#overview-panel","text":"To the right of the Filters panel and occupying the upper part of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query Service Name Database Schema User Name Client Host At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again. Columns The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips. Tool-tips For the Query dimension, hovering over the information icon ( ) reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value. Adding and removing columns Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric . Sorting The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values. Pagination The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items.","title":"Overview Panel"},{"location":"using/query-analytics.html#details-panel","text":"Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab .","title":"Details Panel"},{"location":"using/query-analytics.html#details-tab","text":"The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on one of the follow named activities: query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with these headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over. Rate/Second : A sparkline chart of real-time values per unit time. Sum : A summation of the metric for the selected query, and the percentage of the total. Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension.","title":"Details Tab"},{"location":"using/query-analytics.html#examples-tab","text":"(For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element.","title":"Examples Tab"},{"location":"using/query-analytics.html#explain-tab","text":"(For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats: MySQL: Classic and JSON MongoDB: JSON only PostgreSQL: Not supported","title":"Explain Tab"},{"location":"using/query-analytics.html#tables-tab","text":"(For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query.","title":"Tables Tab"},{"location":"using/query-analytics.html#query-analytics-for-mongodb","text":"MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking tables. In contrast, MongoDB uses the concept of a document where all essential information for a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools.","title":"Query Analytics for MongoDB"},{"location":"using/platform/index.html","text":"About Percona Enterprise Platform Percona Enterprise Platform provides value-added services to PMM. The services comprise: Security Threat Tool","title":"Percona Enterprise Platform"},{"location":"using/platform/dbaas.html","text":"DBaaS Dashboard Caution DBaaS functionality is Alpha. The information on this page is subject to change and may be inaccurate. Note You must run PMM Server with a DBaaS feature flag to activate the features described here. Kubernetes clusters Add a Kubernetes cluster Unregister a Kubernetes cluster View a Kubernetes cluster\u2019s configuration DB clusters Add a DB Cluster Delete a DB Cluster Edit a DB Cluster Restart a DB Cluster Suspend or resume a DB Cluster The DBaaS dashboard is where you add, remove, and operate on Kubernetes and database clusters. To open the DBaaS dashboard: From the main menu, select PMM \u2192 PMM DBaaS ; Or, from the left menu, select DBaaS . Kubernetes clusters Add a Kubernetes cluster Click Register new Kubernetes Cluster Enter values for the Kubernetes Cluster Name and Kubeconfig file in the corresponding fields. Click Register . A message will momentarily display telling you whether the registration was successful or not. Unregister a Kubernetes cluster Note You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel . View a Kubernetes cluster\u2019s configuration Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration . DB clusters Add a DB Cluster You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first. Tip Resource consumption in Kubernetes can cause problems. Use this formula to ensure your nodes have enough resources to start the requested configuration: ( 2 * # of nodes in DB cluster * CPU per node ) + (.5 * # of nodes in db cluster) = total # of CPUs that must be free for cluster to start The first part of the equation is resources for the cluster. It is doubled because each DB cluster member must also have a proxy started with it. The second part is to start the container(s) that automatically monitor each member of the DB cluster. (You can also specify CPU in decimal tenths, e.g. .1 CPUs or 1.5 CPUs.) Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name that complies with domain naming rules. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name Database type : The cluster database type Connection : Host : The hostname Port : The port number Username : The connection username Password : The connection password (click the eye icon to reveal) DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name CPU : The number of CPUs allocated to the cluster Memory : The amount of memory allocated to the cluster Disk : The amount of disk space allocated to the cluster Cluster Status : PENDING : The cluster is being created ACTIVE : The cluster is active FAILED : The cluster could not be created DELETING : The cluster is being deleted Delete a DB Cluster Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel . Edit a DB Cluster Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited. Restart a DB Cluster Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart . Suspend or resume a DB Cluster Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume . See also Setting up a development environment for DBaaS","title":"DBaaS Dashboard"},{"location":"using/platform/dbaas.html#kubernetes-clusters","text":"","title":"Kubernetes clusters"},{"location":"using/platform/dbaas.html#add-a-kubernetes-cluster","text":"Click Register new Kubernetes Cluster Enter values for the Kubernetes Cluster Name and Kubeconfig file in the corresponding fields. Click Register . A message will momentarily display telling you whether the registration was successful or not.","title":"Add a Kubernetes cluster"},{"location":"using/platform/dbaas.html#unregister-a-kubernetes-cluster","text":"Note You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel .","title":"Unregister a Kubernetes cluster"},{"location":"using/platform/dbaas.html#view-a-kubernetes-clusters-configuration","text":"Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration .","title":"View a Kubernetes cluster's configuration"},{"location":"using/platform/dbaas.html#db-clusters","text":"","title":"DB clusters"},{"location":"using/platform/dbaas.html#add-a-db-cluster","text":"You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first. Tip Resource consumption in Kubernetes can cause problems. Use this formula to ensure your nodes have enough resources to start the requested configuration: ( 2 * # of nodes in DB cluster * CPU per node ) + (.5 * # of nodes in db cluster) = total # of CPUs that must be free for cluster to start The first part of the equation is resources for the cluster. It is doubled because each DB cluster member must also have a proxy started with it. The second part is to start the container(s) that automatically monitor each member of the DB cluster. (You can also specify CPU in decimal tenths, e.g. .1 CPUs or 1.5 CPUs.) Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name that complies with domain naming rules. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name Database type : The cluster database type Connection : Host : The hostname Port : The port number Username : The connection username Password : The connection password (click the eye icon to reveal) DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name CPU : The number of CPUs allocated to the cluster Memory : The amount of memory allocated to the cluster Disk : The amount of disk space allocated to the cluster Cluster Status : PENDING : The cluster is being created ACTIVE : The cluster is active FAILED : The cluster could not be created DELETING : The cluster is being deleted","title":"Add a DB Cluster"},{"location":"using/platform/dbaas.html#delete-a-db-cluster","text":"Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel .","title":"Delete a DB Cluster"},{"location":"using/platform/dbaas.html#edit-a-db-cluster","text":"Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited.","title":"Edit a DB Cluster"},{"location":"using/platform/dbaas.html#restart-a-db-cluster","text":"Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart .","title":"Restart a DB Cluster"},{"location":"using/platform/dbaas.html#suspend-or-resume-a-db-cluster","text":"Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume . See also Setting up a development environment for DBaaS","title":"Suspend or resume a DB Cluster"},{"location":"using/platform/security-threat-tool.html","text":"Security Threat Tool The Security Threat Tool runs regular checks against connected databases, alerting you if any servers pose a potential security threat. All checks run on the PMM Client side. Results are sent to PMM Server where a summary count is shown on the Home Dashboard , with details in the PMM Database Checks dashboard. Checks are automatically downloaded from Percona Enterprise Platform and run every 24 hours. (This period is not configurable.) Check results data always remains on the PMM Server. It is not related to anonymous data sent for Telemetry purposes. The Failed security checks panel on the Home Dashboard shows the number of failed checks classed as critical (red), major (amber), and trivial (blue). Key Critical / Major / Trivial Details are in the PMM Database Checks dashboard (select PMM\u2192PMM Database Checks ). How to enable The Security Threat Tool (STT) is disabled by default. Enable it in PMM Settings\u2192Advanced Settings . Enabling STT in the settings also causes the PMM server to download STT checks from Percona Platform and run them once. This operation runs in the background so even though the settings update finishes instantly it might take some time for the checks to complete download and execution and the results (if any) to be visible in the PMM Database Checks dashboard. List of checks made Check ID Description mongodb_auth MongoDB authentication is disabled mongodb_version MongoDB/PSMDB version is not the latest mysql_anonymous_users There are accounts with no username mysql_empty_password There are users without passwords mysql_version MySQL/PS/MariaDB version is not the latest postgresql_super_role PostgreSQL has users (besides postgres , rdsadmin , and pmm_user ) with the role \u2018SUPER\u2019 postgresql_version PostgreSQL version is not the latest","title":"Security Threat Tool"},{"location":"using/platform/security-threat-tool.html#how-to-enable","text":"The Security Threat Tool (STT) is disabled by default. Enable it in PMM Settings\u2192Advanced Settings . Enabling STT in the settings also causes the PMM server to download STT checks from Percona Platform and run them once. This operation runs in the background so even though the settings update finishes instantly it might take some time for the checks to complete download and execution and the results (if any) to be visible in the PMM Database Checks dashboard.","title":"How to enable"},{"location":"using/platform/security-threat-tool.html#list-of-checks-made","text":"Check ID Description mongodb_auth MongoDB authentication is disabled mongodb_version MongoDB/PSMDB version is not the latest mysql_anonymous_users There are accounts with no username mysql_empty_password There are users without passwords mysql_version MySQL/PS/MariaDB version is not the latest postgresql_super_role PostgreSQL has users (besides postgres , rdsadmin , and pmm_user ) with the role \u2018SUPER\u2019 postgresql_version PostgreSQL version is not the latest","title":"List of checks made"}]}